['datasets/drexel_1/g/g_06.txt', 'datasets/drexel_1/g/g_04.txt', 'datasets/drexel_1/g/g_08.txt', 'datasets/drexel_1/g/g_07.txt', 'datasets/drexel_1/g/g_02.txt', 'datasets/drexel_1/g/g_05.txt', 'datasets/drexel_1/g/g_01.txt', 'datasets/drexel_1/g/g_03.txt']
g
g
g
g
g
g
g
g
['datasets/drexel_1/p/p_01.txt', 'datasets/drexel_1/p/p_10.txt', 'datasets/drexel_1/p/p_08.txt', 'datasets/drexel_1/p/p_05.txt', 'datasets/drexel_1/p/p_04.txt', 'datasets/drexel_1/p/p_02.txt', 'datasets/drexel_1/p/p_07.txt', 'datasets/drexel_1/p/p_06.txt', 'datasets/drexel_1/p/p_03.txt', 'datasets/drexel_1/p/p_09.txt']
p
p
p
p
p
p
p
p
p
p
['datasets/drexel_1/e/e_07.txt', 'datasets/drexel_1/e/e_05.txt', 'datasets/drexel_1/e/e_06.txt', 'datasets/drexel_1/e/e_08.txt', 'datasets/drexel_1/e/e_09.txt', 'datasets/drexel_1/e/e_03.txt', 'datasets/drexel_1/e/e_04.txt', 'datasets/drexel_1/e/e_02.txt', 'datasets/drexel_1/e/e_01.txt']
e
e
e
e
e
e
e
e
e
['datasets/drexel_1/h/h_07.txt', 'datasets/drexel_1/h/h_05.txt', 'datasets/drexel_1/h/h_06.txt', 'datasets/drexel_1/h/h_03.txt', 'datasets/drexel_1/h/h_02.txt', 'datasets/drexel_1/h/h_04.txt', 'datasets/drexel_1/h/h_01.txt', 'datasets/drexel_1/h/h_08.txt']
h
h
h
h
h
h
h
h
['datasets/drexel_1/cm/cm_06.txt', 'datasets/drexel_1/cm/cm_01.txt', 'datasets/drexel_1/cm/cm_10.txt', 'datasets/drexel_1/cm/cm_07.txt', 'datasets/drexel_1/cm/cm_04.txt', 'datasets/drexel_1/cm/cm_08.txt', 'datasets/drexel_1/cm/cm_03.txt', 'datasets/drexel_1/cm/cm_05.txt', 'datasets/drexel_1/cm/cm_02.txt', 'datasets/drexel_1/cm/cm_09.txt', 'datasets/drexel_1/cm/cm_11.txt']
cm
cm
cm
cm
cm
cm
cm
cm
cm
cm
cm
['datasets/drexel_1/b/b_07.txt', 'datasets/drexel_1/b/b_05.txt', 'datasets/drexel_1/b/b_02.txt', 'datasets/drexel_1/b/b_04.txt', 'datasets/drexel_1/b/b_03.txt', 'datasets/drexel_1/b/b_01.txt', 'datasets/drexel_1/b/b_06.txt']
b
b
b
b
b
b
b
['datasets/drexel_1/m/m_08.txt', 'datasets/drexel_1/m/m_05.txt', 'datasets/drexel_1/m/m_06.txt', 'datasets/drexel_1/m/m_02.txt', 'datasets/drexel_1/m/m_01.txt', 'datasets/drexel_1/m/m_07.txt', 'datasets/drexel_1/m/m_04.txt', 'datasets/drexel_1/m/m_03.txt']
m
m
m
m
m
m
m
m
[('datasets/drexel_1/g/g_06.txt', 'g'), ('datasets/drexel_1/g/g_04.txt', 'g'), ('datasets/drexel_1/g/g_08.txt', 'g'), ('datasets/drexel_1/g/g_07.txt', 'g'), ('datasets/drexel_1/g/g_02.txt', 'g'), ('datasets/drexel_1/g/g_05.txt', 'g'), ('datasets/drexel_1/g/g_01.txt', 'g'), ('datasets/drexel_1/g/g_03.txt', 'g'), ('datasets/drexel_1/p/p_01.txt', 'p'), ('datasets/drexel_1/p/p_10.txt', 'p'), ('datasets/drexel_1/p/p_08.txt', 'p'), ('datasets/drexel_1/p/p_05.txt', 'p'), ('datasets/drexel_1/p/p_04.txt', 'p'), ('datasets/drexel_1/p/p_02.txt', 'p'), ('datasets/drexel_1/p/p_07.txt', 'p'), ('datasets/drexel_1/p/p_06.txt', 'p'), ('datasets/drexel_1/p/p_03.txt', 'p'), ('datasets/drexel_1/p/p_09.txt', 'p'), ('datasets/drexel_1/e/e_07.txt', 'e'), ('datasets/drexel_1/e/e_05.txt', 'e'), ('datasets/drexel_1/e/e_06.txt', 'e'), ('datasets/drexel_1/e/e_08.txt', 'e'), ('datasets/drexel_1/e/e_09.txt', 'e'), ('datasets/drexel_1/e/e_03.txt', 'e'), ('datasets/drexel_1/e/e_04.txt', 'e'), ('datasets/drexel_1/e/e_02.txt', 'e'), ('datasets/drexel_1/e/e_01.txt', 'e'), ('datasets/drexel_1/h/h_07.txt', 'h'), ('datasets/drexel_1/h/h_05.txt', 'h'), ('datasets/drexel_1/h/h_06.txt', 'h'), ('datasets/drexel_1/h/h_03.txt', 'h'), ('datasets/drexel_1/h/h_02.txt', 'h'), ('datasets/drexel_1/h/h_04.txt', 'h'), ('datasets/drexel_1/h/h_01.txt', 'h'), ('datasets/drexel_1/h/h_08.txt', 'h'), ('datasets/drexel_1/cm/cm_06.txt', 'cm'), ('datasets/drexel_1/cm/cm_01.txt', 'cm'), ('datasets/drexel_1/cm/cm_10.txt', 'cm'), ('datasets/drexel_1/cm/cm_07.txt', 'cm'), ('datasets/drexel_1/cm/cm_04.txt', 'cm'), ('datasets/drexel_1/cm/cm_08.txt', 'cm'), ('datasets/drexel_1/cm/cm_03.txt', 'cm'), ('datasets/drexel_1/cm/cm_05.txt', 'cm'), ('datasets/drexel_1/cm/cm_02.txt', 'cm'), ('datasets/drexel_1/cm/cm_09.txt', 'cm'), ('datasets/drexel_1/cm/cm_11.txt', 'cm'), ('datasets/drexel_1/b/b_07.txt', 'b'), ('datasets/drexel_1/b/b_05.txt', 'b'), ('datasets/drexel_1/b/b_02.txt', 'b'), ('datasets/drexel_1/b/b_04.txt', 'b'), ('datasets/drexel_1/b/b_03.txt', 'b'), ('datasets/drexel_1/b/b_01.txt', 'b'), ('datasets/drexel_1/b/b_06.txt', 'b'), ('datasets/drexel_1/m/m_08.txt', 'm'), ('datasets/drexel_1/m/m_05.txt', 'm'), ('datasets/drexel_1/m/m_06.txt', 'm'), ('datasets/drexel_1/m/m_02.txt', 'm'), ('datasets/drexel_1/m/m_01.txt', 'm'), ('datasets/drexel_1/m/m_07.txt', 'm'), ('datasets/drexel_1/m/m_04.txt', 'm'), ('datasets/drexel_1/m/m_03.txt', 'm')]
User doc features:  [[  5.16000000e+03   1.54298372e+01   1.52701422e+00   5.54000000e+02
    4.60000000e+01   6.43700000e+03   2.29347826e+01   5.25118483e-01
    5.43707928e+01]]
User other features:  [[  1.82100000e+03   1.90592031e+01   1.77259475e+00   2.23000000e+02
    1.50000000e+01   2.23600000e+03   2.28666667e+01   6.50145773e-01
    3.36638173e+01]
 [  1.85200000e+03   1.46950651e+01   1.56130790e+00   2.15000000e+02
    1.80000000e+01   2.32900000e+03   2.03888889e+01   5.85831063e-01
    5.40536293e+01]
 [  3.04500000e+03   1.53653462e+01   1.53462158e+00   3.28000000e+02
    3.00000000e+01   3.82600000e+03   2.07000000e+01   5.28180354e-01
    5.59955145e+01]
 [  1.72500000e+03   1.48524268e+01   1.52124646e+00   2.17000000e+02
    1.50000000e+01   2.17100000e+03   2.35333333e+01   6.14730878e-01
    5.42512162e+01]
 [  3.38000000e+03   1.49090304e+01   1.51432665e+00   3.66000000e+02
    3.10000000e+01   4.24500000e+03   2.25161290e+01   5.24355301e-01
    5.58690946e+01]
 [  2.29700000e+03   1.70120879e+01   1.63516484e+00   2.66000000e+02
    2.00000000e+01   2.86000000e+03   2.27500000e+01   5.84615385e-01
    4.54088049e+01]]
Other author features:  [[  2.16000000e+03   1.29423966e+01   1.33268859e+00   2.77000000e+02
    2.10000000e+01   2.73500000e+03   2.46190476e+01   5.35783366e-01
    6.91012121e+01]
 [  2.34100000e+03   1.14887597e+01   1.45736434e+00   2.66000000e+02
    3.20000000e+01   2.91200000e+03   1.61250000e+01   5.15503876e-01
    6.71751017e+01]
 [  2.40500000e+03   1.46379767e+01   1.43774319e+00   2.46000000e+02
    2.00000000e+01   2.97000000e+03   2.57000000e+01   4.78599222e-01
    5.91164261e+01]
 [  2.28400000e+03   1.28059667e+01   1.45972495e+00   2.82000000e+02
    2.70000000e+01   2.85900000e+03   1.88518519e+01   5.54027505e-01
    6.42076395e+01]
 [  2.35500000e+03   1.39250656e+01   1.47104247e+00   2.85000000e+02
    2.50000000e+01   2.92600000e+03   2.07200000e+01   5.50193050e-01
    6.13540069e+01]
 [  2.15900000e+03   1.00441699e+01   1.32629559e+00   2.76000000e+02
    3.20000000e+01   2.75600000e+03   1.62812500e+01   5.29750480e-01
    7.81049247e+01]
 [  2.22200000e+03   9.49707661e+00   1.36132812e+00   2.95000000e+02
    3.10000000e+01   2.81200000e+03   1.65161290e+01   5.76171875e-01
    7.49027697e+01]
 [  2.56300000e+03   1.66535458e+01   1.62151394e+00   2.19000000e+02
    2.00000000e+01   3.09900000e+03   2.51000000e+01   4.36254980e-01
    4.41784203e+01]
 [  2.72200000e+03   2.23440476e+01   1.78476190e+00   2.87000000e+02
    1.60000000e+01   3.31000000e+03   3.28125000e+01   5.46666667e-01
    2.25394554e+01]
 [  2.78600000e+03   2.23375758e+01   1.70075758e+00   2.74000000e+02
    1.50000000e+01   3.39500000e+03   3.52000000e+01   5.18939394e-01
    2.72229091e+01]
 [  2.67000000e+03   2.13287508e+01   1.64606742e+00   2.87000000e+02
    1.70000000e+01   3.28200000e+03   3.14117647e+01   5.37453184e-01
    3.56947555e+01]
 [  2.58300000e+03   1.91265024e+01   1.67315175e+00   2.68000000e+02
    1.80000000e+01   3.17100000e+03   2.85555556e+01   5.21400778e-01
    3.63024730e+01]
 [  2.73800000e+03   2.06424544e+01   1.73507463e+00   3.04000000e+02
    1.80000000e+01   3.33800000e+03   2.97777778e+01   5.67164179e-01
    2.98232421e+01]
 [  2.66700000e+03   2.44817043e+01   1.82066277e+00   2.71000000e+02
    1.40000000e+01   3.25900000e+03   3.66428571e+01   5.28265107e-01
    1.56144298e+01]
 [  2.62200000e+03   2.30117647e+01   1.72352941e+00   2.73000000e+02
    1.50000000e+01   3.19300000e+03   3.40000000e+01   5.35294118e-01
    2.65144118e+01]
 [  2.62300000e+03   2.04432359e+01   1.72340426e+00   2.84000000e+02
    1.70000000e+01   3.20300000e+03   3.04117647e+01   5.49323017e-01
    3.01670588e+01]
 [  2.66400000e+03   2.12374382e+01   1.79303675e+00   2.70000000e+02
    1.80000000e+01   3.25000000e+03   2.87222222e+01   5.22243714e-01
    2.59910354e+01]
 [  2.61100000e+03   1.84561011e+01   1.56797020e+00   2.80000000e+02
    1.90000000e+01   3.21400000e+03   2.82631579e+01   5.21415270e-01
    4.54976154e+01]
 [  2.27100000e+03   1.43510078e+01   1.34883721e+00   2.18000000e+02
    2.00000000e+01   2.85100000e+03   2.58000000e+01   4.22480620e-01
    6.65363721e+01]
 [  2.17700000e+03   1.24545455e+01   1.32500000e+00   2.26000000e+02
    2.20000000e+01   2.75800000e+03   2.36363636e+01   4.34615385e-01
    7.07490909e+01]
 [  2.29000000e+03   1.27027990e+01   1.35305344e+00   2.33000000e+02
    2.40000000e+01   2.88000000e+03   2.18333333e+01   4.44656489e-01
    7.02058461e+01]
 [  2.33100000e+03   1.46531360e+01   1.44767442e+00   2.42000000e+02
    2.20000000e+01   2.90000000e+03   2.34545455e+01   4.68992248e-01
    6.05553805e+01]
 [  2.34100000e+03   1.65730276e+01   1.46641075e+00   2.34000000e+02
    1.90000000e+01   2.91100000e+03   2.74210526e+01   4.49136276e-01
    5.49442823e+01]
 [  2.24100000e+03   1.50404118e+01   1.43050193e+00   2.35000000e+02
    2.10000000e+01   2.81100000e+03   2.46666667e+01   4.53667954e-01
    6.07778700e+01]
 [  2.31100000e+03   1.45837524e+01   1.43907157e+00   2.28000000e+02
    2.20000000e+01   2.88700000e+03   2.35000000e+01   4.41005803e-01
    6.12370455e+01]
 [  2.38100000e+03   1.51112092e+01   1.49514563e+00   2.29000000e+02
    2.20000000e+01   2.96300000e+03   2.34090909e+01   4.44660194e-01
    5.65854523e+01]
 [  2.59600000e+03   1.83428571e+01   1.58458647e+00   2.21000000e+02
    1.90000000e+01   3.17400000e+03   2.80000000e+01   4.15413534e-01
    4.43589850e+01]
 [  2.50100000e+03   1.52687008e+01   1.56063618e+00   2.48000000e+02
    2.30000000e+01   3.05500000e+03   2.18695652e+01   4.93041750e-01
    5.26075702e+01]
 [  2.64500000e+03   1.30111623e+01   1.54166667e+00   2.47000000e+02
    3.10000000e+01   3.24000000e+03   1.62580645e+01   4.90079365e-01
    5.99080645e+01]
 [  2.64300000e+03   1.46125401e+01   1.58464567e+00   2.40000000e+02
    2.70000000e+01   3.20500000e+03   1.88148148e+01   4.72440945e-01
    5.36769393e+01]
 [  2.66300000e+03   1.54961361e+01   1.56332703e+00   2.46000000e+02
    2.50000000e+01   3.25900000e+03   2.11600000e+01   4.65028355e-01
    5.31001331e+01]
 [  2.61200000e+03   1.64218231e+01   1.55019305e+00   2.46000000e+02
    2.30000000e+01   3.17500000e+03   2.25217391e+01   4.74903475e-01
    5.28291027e+01]
 [  2.56800000e+03   1.46730646e+01   1.55772994e+00   2.62000000e+02
    2.50000000e+01   3.14100000e+03   2.04400000e+01   5.12720157e-01
    5.43044470e+01]
 [  2.77100000e+03   1.55067713e+01   1.60115607e+00   2.63000000e+02
    2.80000000e+01   3.37900000e+03   1.85357143e+01   5.06743738e-01
    5.25634465e+01]
 [  2.51100000e+03   1.37822398e+01   1.53557312e+00   2.43000000e+02
    3.00000000e+01   3.06300000e+03   1.68666667e+01   4.80237154e-01
    5.98058472e+01]
 [  2.20000000e+03   6.31713555e+00   1.19411765e+00   2.85000000e+02
    4.60000000e+01   2.77600000e+03   1.10869565e+01   5.58823529e-01
    9.45593862e+01]
 [  2.04100000e+03   6.45140861e+00   1.17647059e+00   2.70000000e+02
    3.60000000e+01   2.58800000e+03   1.36944444e+01   5.47667343e-01
    9.34057271e+01]
 [  2.03000000e+03   4.38559441e+00   1.03409091e+00   2.84000000e+02
    6.50000000e+01   2.70400000e+03   8.12307692e+00   5.37878788e-01
    1.11105986e+02]
 [  2.07400000e+03   5.51392045e+00   1.15039062e+00   2.82000000e+02
    4.40000000e+01   2.65500000e+03   1.16363636e+01   5.50781250e-01
    9.77010440e+01]
 [  1.94400000e+03   4.01625544e+00   1.04905660e+00   2.56000000e+02
    6.50000000e+01   2.60400000e+03   8.15384615e+00   4.83018868e-01
    1.09808657e+02]
 [  1.97400000e+03   5.31029906e+00   1.05893910e+00   2.78000000e+02
    4.50000000e+01   2.57400000e+03   1.13111111e+01   5.46168959e-01
    1.05767975e+02]
 [  2.03100000e+03   5.44336346e+00   1.14619883e+00   2.60000000e+02
    4.40000000e+01   2.61200000e+03   1.16590909e+01   5.06822612e-01
    9.80326017e+01]
 [  2.05800000e+03   6.06079545e+00   1.18359375e+00   2.69000000e+02
    4.40000000e+01   2.64000000e+03   1.16363636e+01   5.25390625e-01
    9.48920597e+01]
 [  2.05600000e+03   5.03967251e+00   1.19883041e+00   2.58000000e+02
    5.00000000e+01   2.63900000e+03   1.02600000e+01   5.02923977e-01
    9.50000474e+01]
 [  2.19900000e+03   5.86054159e+00   1.20309478e+00   2.71000000e+02
    4.40000000e+01   2.78400000e+03   1.17500000e+01   5.24177950e-01
    9.31269318e+01]
 [  2.10500000e+03   4.31122690e+00   1.14836224e+00   2.83000000e+02
    7.10000000e+01   2.75300000e+03   7.30985915e+00   5.45279383e-01
    1.02264048e+02]
 [  2.40900000e+03   1.68033539e+01   1.47792706e+00   2.83000000e+02
    1.90000000e+01   3.02900000e+03   2.74210526e+01   5.43186180e-01
    5.39700020e+01]
 [  2.47600000e+03   1.69838783e+01   1.51711027e+00   3.00000000e+02
    2.00000000e+01   3.08400000e+03   2.63000000e+01   5.70342205e-01
    5.17929715e+01]
 [  2.52000000e+03   1.37476190e+01   1.50285714e+00   2.95000000e+02
    2.80000000e+01   3.11300000e+03   1.87500000e+01   5.61904762e-01
    6.06620357e+01]
 [  2.65600000e+03   1.47126688e+01   1.57559199e+00   3.04000000e+02
    2.90000000e+01   3.28400000e+03   1.89310345e+01   5.53734062e-01
    5.43249180e+01]
 [  2.50700000e+03   1.48300277e+01   1.51310861e+00   2.75000000e+02
    2.30000000e+01   3.11500000e+03   2.32173913e+01   5.14981273e-01
    5.52603591e+01]
 [  2.48400000e+03   1.46264240e+01   1.50954198e+00   2.68000000e+02
    2.60000000e+01   3.08200000e+03   2.01538462e+01   5.11450382e-01
    5.86715942e+01]
 [  2.48700000e+03   1.59204356e+01   1.51423150e+00   2.70000000e+02
    2.30000000e+01   3.07800000e+03   2.29130435e+01   5.12333966e-01
    5.54742760e+01]
 [  2.09700000e+03   1.56285714e+01   1.32190476e+00   2.38000000e+02
    1.80000000e+01   2.68200000e+03   2.91666667e+01   4.53333333e-01
    6.53976905e+01]
 [  2.20100000e+03   1.58505109e+01   1.34854015e+00   2.59000000e+02
    2.00000000e+01   2.81800000e+03   2.74000000e+01   4.72627737e-01
    6.49375036e+01]
 [  2.32800000e+03   1.48356976e+01   1.38827839e+00   2.68000000e+02
    2.20000000e+01   2.93100000e+03   2.48181818e+01   4.90842491e-01
    6.41961938e+01]
 [  2.22800000e+03   1.68602267e+01   1.27889908e+00   2.45000000e+02
    1.70000000e+01   2.83600000e+03   3.20588235e+01   4.49541284e-01
    6.61004317e+01]
 [  2.20100000e+03   1.62301587e+01   1.31428571e+00   2.60000000e+02
    1.80000000e+01   2.81800000e+03   3.11111111e+01   4.64285714e-01
    6.40686508e+01]
 [  2.26900000e+03   1.56044444e+01   1.32974910e+00   2.64000000e+02
    2.00000000e+01   2.89700000e+03   2.79000000e+01   4.73118280e-01
    6.60197258e+01]
 [  2.15700000e+03   1.71489830e+01   1.33457944e+00   2.53000000e+02
    1.70000000e+01   2.75300000e+03   3.14705882e+01   4.72897196e-01
    6.19869324e+01]
 [  2.14400000e+03   1.62558824e+01   1.30330882e+00   2.59000000e+02
    1.70000000e+01   2.75600000e+03   3.20000000e+01   4.76102941e-01
    6.40950735e+01]]
Training labels:  ['user', 'user', 'user', 'user', 'user', 'user', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'cm', 'cm', 'cm', 'cm', 'cm', 'cm', 'cm', 'cm', 'cm', 'cm', 'cm', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm']
Predicted author of doc: ['user']
Certainty:  [[ 0.06527625  0.12059265  0.09105791  0.18878326  0.06604532  0.06698062
   0.08456975  0.31669424]]
Classifier internal label rep:  ['b' 'cm' 'e' 'g' 'h' 'm' 'p' 'user']
[('avgSentenceLength', 1), ('characterSpace', 2), ('unique_words', 3), ('complexity', 4), ('sentenceCount', 5), ('gunningFog', 6), ('avgSyllablesPerWord', 7), ('letterSpace', 8), ('fleschReadingEase', 9)]
Clustering features...
Clustering the following samples:
[[ 1821.]
 [ 1852.]
 [ 3045.]
 [ 1725.]
 [ 3380.]
 [ 2297.]
 [ 2160.]
 [ 2341.]
 [ 2405.]
 [ 2284.]
 [ 2355.]
 [ 2159.]
 [ 2222.]
 [ 2563.]
 [ 2722.]
 [ 2786.]
 [ 2670.]
 [ 2583.]
 [ 2738.]
 [ 2667.]
 [ 2622.]
 [ 2623.]
 [ 2664.]
 [ 2611.]
 [ 2271.]
 [ 2177.]
 [ 2290.]
 [ 2331.]
 [ 2341.]
 [ 2241.]
 [ 2311.]
 [ 2381.]
 [ 2596.]
 [ 2501.]
 [ 2645.]
 [ 2643.]
 [ 2663.]
 [ 2612.]
 [ 2568.]
 [ 2771.]
 [ 2511.]
 [ 2200.]
 [ 2041.]
 [ 2030.]
 [ 2074.]
 [ 1944.]
 [ 1974.]
 [ 2031.]
 [ 2058.]
 [ 2056.]
 [ 2199.]
 [ 2105.]
 [ 2409.]
 [ 2476.]
 [ 2520.]
 [ 2656.]
 [ 2507.]
 [ 2484.]
 [ 2487.]
 [ 2097.]
 [ 2201.]
 [ 2328.]
 [ 2228.]
 [ 2201.]
 [ 2269.]
 [ 2157.]
 [ 2144.]]
Reinitializing k means with  7  clusters.
Reinitializing k means with  6  clusters.
Reinitializing k means with  5  clusters.
Reinitializing k means with  4  clusters.
Reinitializing k means with  3  clusters.
Reinitializing k means with  2  clusters.
Cluster labels:  [1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0
 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]
[matrix([[ 3045.]]), matrix([[ 3380.]]), matrix([[ 2405.]]), matrix([[ 2563.]]), matrix([[ 2722.]]), matrix([[ 2786.]]), matrix([[ 2670.]]), matrix([[ 2583.]]), matrix([[ 2738.]]), matrix([[ 2667.]]), matrix([[ 2622.]]), matrix([[ 2623.]]), matrix([[ 2664.]]), matrix([[ 2611.]]), matrix([[ 2596.]]), matrix([[ 2501.]]), matrix([[ 2645.]]), matrix([[ 2643.]]), matrix([[ 2663.]]), matrix([[ 2612.]]), matrix([[ 2568.]]), matrix([[ 2771.]]), matrix([[ 2511.]]), matrix([[ 2409.]]), matrix([[ 2476.]]), matrix([[ 2520.]]), matrix([[ 2656.]]), matrix([[ 2507.]]), matrix([[ 2484.]]), matrix([[ 2487.]])]
[matrix([[ 1821.]]), matrix([[ 1852.]]), matrix([[ 1725.]]), matrix([[ 2297.]]), matrix([[ 2160.]]), matrix([[ 2341.]]), matrix([[ 2284.]]), matrix([[ 2355.]]), matrix([[ 2159.]]), matrix([[ 2222.]]), matrix([[ 2271.]]), matrix([[ 2177.]]), matrix([[ 2290.]]), matrix([[ 2331.]]), matrix([[ 2341.]]), matrix([[ 2241.]]), matrix([[ 2311.]]), matrix([[ 2381.]]), matrix([[ 2200.]]), matrix([[ 2041.]]), matrix([[ 2030.]]), matrix([[ 2074.]]), matrix([[ 1944.]]), matrix([[ 1974.]]), matrix([[ 2031.]]), matrix([[ 2058.]]), matrix([[ 2056.]]), matrix([[ 2199.]]), matrix([[ 2105.]]), matrix([[ 2097.]]), matrix([[ 2201.]]), matrix([[ 2328.]]), matrix([[ 2228.]]), matrix([[ 2201.]]), matrix([[ 2269.]]), matrix([[ 2157.]]), matrix([[ 2144.]])]
Clustering the following samples:
[[ 19.05920311]
 [ 14.69506509]
 [ 15.36534622]
 [ 14.85242682]
 [ 14.90903041]
 [ 17.01208791]
 [ 12.94239661]
 [ 11.48875969]
 [ 14.63797665]
 [ 12.80596667]
 [ 13.92506564]
 [ 10.04416987]
 [  9.49707661]
 [ 16.65354582]
 [ 22.34404762]
 [ 22.33757576]
 [ 21.32875083]
 [ 19.12650238]
 [ 20.64245439]
 [ 24.48170426]
 [ 23.01176471]
 [ 20.44323586]
 [ 21.23743821]
 [ 18.45610115]
 [ 14.35100775]
 [ 12.45454545]
 [ 12.70279898]
 [ 14.65313601]
 [ 16.57302758]
 [ 15.04041184]
 [ 14.58375242]
 [ 15.11120918]
 [ 18.34285714]
 [ 15.26870084]
 [ 13.01116231]
 [ 14.6125401 ]
 [ 15.49613611]
 [ 16.42182307]
 [ 14.67306458]
 [ 15.50677126]
 [ 13.78223979]
 [  6.31713555]
 [  6.45140861]
 [  4.38559441]
 [  5.51392045]
 [  4.01625544]
 [  5.31029906]
 [  5.44336346]
 [  6.06079545]
 [  5.03967251]
 [  5.86054159]
 [  4.3112269 ]
 [ 16.80335387]
 [ 16.98387833]
 [ 13.74761905]
 [ 14.7126688 ]
 [ 14.83002768]
 [ 14.62642396]
 [ 15.92043561]
 [ 15.62857143]
 [ 15.85051095]
 [ 14.83569764]
 [ 16.86022666]
 [ 16.23015873]
 [ 15.60444444]
 [ 17.14898296]
 [ 16.25588235]]
Cluster labels:  [7 6 6 6 6 3 0 4 6 0 0 4 4 3 5 5 2 7 2 5 5 2 2 7 6 0 0 6 3 6 6 6 7 6 0 6 6
 3 6 6 0 1 1 1 1 1 1 1 1 1 1 1 3 3 0 6 6 6 3 6 3 6 3 3 6 3 3]
[matrix([[ 12.94239661]]), matrix([[ 12.80596667]]), matrix([[ 13.92506564]]), matrix([[ 12.45454545]]), matrix([[ 12.70279898]]), matrix([[ 13.01116231]]), matrix([[ 13.78223979]]), matrix([[ 13.74761905]])]
[matrix([[ 6.31713555]]), matrix([[ 6.45140861]]), matrix([[ 4.38559441]]), matrix([[ 5.51392045]]), matrix([[ 4.01625544]]), matrix([[ 5.31029906]]), matrix([[ 5.44336346]]), matrix([[ 6.06079545]]), matrix([[ 5.03967251]]), matrix([[ 5.86054159]]), matrix([[ 4.3112269]])]
[matrix([[ 21.32875083]]), matrix([[ 20.64245439]]), matrix([[ 20.44323586]]), matrix([[ 21.23743821]])]
[matrix([[ 17.01208791]]), matrix([[ 16.65354582]]), matrix([[ 16.57302758]]), matrix([[ 16.42182307]]), matrix([[ 16.80335387]]), matrix([[ 16.98387833]]), matrix([[ 15.92043561]]), matrix([[ 15.85051095]]), matrix([[ 16.86022666]]), matrix([[ 16.23015873]]), matrix([[ 17.14898296]]), matrix([[ 16.25588235]])]
[matrix([[ 11.48875969]]), matrix([[ 10.04416987]]), matrix([[ 9.49707661]])]
[matrix([[ 22.34404762]]), matrix([[ 22.33757576]]), matrix([[ 24.48170426]]), matrix([[ 23.01176471]])]
[matrix([[ 14.69506509]]), matrix([[ 15.36534622]]), matrix([[ 14.85242682]]), matrix([[ 14.90903041]]), matrix([[ 14.63797665]]), matrix([[ 14.35100775]]), matrix([[ 14.65313601]]), matrix([[ 15.04041184]]), matrix([[ 14.58375242]]), matrix([[ 15.11120918]]), matrix([[ 15.26870084]]), matrix([[ 14.6125401]]), matrix([[ 15.49613611]]), matrix([[ 14.67306458]]), matrix([[ 15.50677126]]), matrix([[ 14.7126688]]), matrix([[ 14.83002768]]), matrix([[ 14.62642396]]), matrix([[ 15.62857143]]), matrix([[ 14.83569764]]), matrix([[ 15.60444444]])]
[matrix([[ 19.05920311]]), matrix([[ 19.12650238]]), matrix([[ 18.45610115]]), matrix([[ 18.34285714]])]
Clustering the following samples:
[[ 1.77259475]
 [ 1.5613079 ]
 [ 1.53462158]
 [ 1.52124646]
 [ 1.51432665]
 [ 1.63516484]
 [ 1.33268859]
 [ 1.45736434]
 [ 1.43774319]
 [ 1.45972495]
 [ 1.47104247]
 [ 1.32629559]
 [ 1.36132812]
 [ 1.62151394]
 [ 1.7847619 ]
 [ 1.70075758]
 [ 1.64606742]
 [ 1.67315175]
 [ 1.73507463]
 [ 1.82066277]
 [ 1.72352941]
 [ 1.72340426]
 [ 1.79303675]
 [ 1.5679702 ]
 [ 1.34883721]
 [ 1.325     ]
 [ 1.35305344]
 [ 1.44767442]
 [ 1.46641075]
 [ 1.43050193]
 [ 1.43907157]
 [ 1.49514563]
 [ 1.58458647]
 [ 1.56063618]
 [ 1.54166667]
 [ 1.58464567]
 [ 1.56332703]
 [ 1.55019305]
 [ 1.55772994]
 [ 1.60115607]
 [ 1.53557312]
 [ 1.19411765]
 [ 1.17647059]
 [ 1.03409091]
 [ 1.15039062]
 [ 1.0490566 ]
 [ 1.0589391 ]
 [ 1.14619883]
 [ 1.18359375]
 [ 1.19883041]
 [ 1.20309478]
 [ 1.14836224]
 [ 1.47792706]
 [ 1.51711027]
 [ 1.50285714]
 [ 1.57559199]
 [ 1.51310861]
 [ 1.50954198]
 [ 1.5142315 ]
 [ 1.32190476]
 [ 1.34854015]
 [ 1.38827839]
 [ 1.27889908]
 [ 1.31428571]
 [ 1.3297491 ]
 [ 1.33457944]
 [ 1.30330882]]
Cluster labels:  [2 7 0 0 0 4 6 3 3 3 3 6 6 4 2 4 4 4 2 2 2 2 2 7 6 6 6 3 3 3 3 0 7 7 0 7 7
 7 7 7 0 1 1 5 1 5 5 1 1 1 1 1 3 0 0 7 0 0 0 6 6 6 6 6 6 6 6]
[matrix([[ 1.53462158]]), matrix([[ 1.52124646]]), matrix([[ 1.51432665]]), matrix([[ 1.49514563]]), matrix([[ 1.54166667]]), matrix([[ 1.53557312]]), matrix([[ 1.51711027]]), matrix([[ 1.50285714]]), matrix([[ 1.51310861]]), matrix([[ 1.50954198]]), matrix([[ 1.5142315]])]
[matrix([[ 1.19411765]]), matrix([[ 1.17647059]]), matrix([[ 1.15039062]]), matrix([[ 1.14619883]]), matrix([[ 1.18359375]]), matrix([[ 1.19883041]]), matrix([[ 1.20309478]]), matrix([[ 1.14836224]])]
[matrix([[ 1.77259475]]), matrix([[ 1.7847619]]), matrix([[ 1.73507463]]), matrix([[ 1.82066277]]), matrix([[ 1.72352941]]), matrix([[ 1.72340426]]), matrix([[ 1.79303675]])]
[matrix([[ 1.45736434]]), matrix([[ 1.43774319]]), matrix([[ 1.45972495]]), matrix([[ 1.47104247]]), matrix([[ 1.44767442]]), matrix([[ 1.46641075]]), matrix([[ 1.43050193]]), matrix([[ 1.43907157]]), matrix([[ 1.47792706]])]
[matrix([[ 1.63516484]]), matrix([[ 1.62151394]]), matrix([[ 1.70075758]]), matrix([[ 1.64606742]]), matrix([[ 1.67315175]])]
[matrix([[ 1.03409091]]), matrix([[ 1.0490566]]), matrix([[ 1.0589391]])]
[matrix([[ 1.33268859]]), matrix([[ 1.32629559]]), matrix([[ 1.36132812]]), matrix([[ 1.34883721]]), matrix([[ 1.325]]), matrix([[ 1.35305344]]), matrix([[ 1.32190476]]), matrix([[ 1.34854015]]), matrix([[ 1.38827839]]), matrix([[ 1.27889908]]), matrix([[ 1.31428571]]), matrix([[ 1.3297491]]), matrix([[ 1.33457944]]), matrix([[ 1.30330882]])]
[matrix([[ 1.5613079]]), matrix([[ 1.5679702]]), matrix([[ 1.58458647]]), matrix([[ 1.56063618]]), matrix([[ 1.58464567]]), matrix([[ 1.56332703]]), matrix([[ 1.55019305]]), matrix([[ 1.55772994]]), matrix([[ 1.60115607]]), matrix([[ 1.57559199]])]
Clustering the following samples:
[[ 223.]
 [ 215.]
 [ 328.]
 [ 217.]
 [ 366.]
 [ 266.]
 [ 277.]
 [ 266.]
 [ 246.]
 [ 282.]
 [ 285.]
 [ 276.]
 [ 295.]
 [ 219.]
 [ 287.]
 [ 274.]
 [ 287.]
 [ 268.]
 [ 304.]
 [ 271.]
 [ 273.]
 [ 284.]
 [ 270.]
 [ 280.]
 [ 218.]
 [ 226.]
 [ 233.]
 [ 242.]
 [ 234.]
 [ 235.]
 [ 228.]
 [ 229.]
 [ 221.]
 [ 248.]
 [ 247.]
 [ 240.]
 [ 246.]
 [ 246.]
 [ 262.]
 [ 263.]
 [ 243.]
 [ 285.]
 [ 270.]
 [ 284.]
 [ 282.]
 [ 256.]
 [ 278.]
 [ 260.]
 [ 269.]
 [ 258.]
 [ 271.]
 [ 283.]
 [ 283.]
 [ 300.]
 [ 295.]
 [ 304.]
 [ 275.]
 [ 268.]
 [ 270.]
 [ 238.]
 [ 259.]
 [ 268.]
 [ 245.]
 [ 260.]
 [ 264.]
 [ 253.]
 [ 259.]]
Reinitializing k means with  7  clusters.
Reinitializing k means with  6  clusters.
Reinitializing k means with  5  clusters.
Reinitializing k means with  4  clusters.
Reinitializing k means with  3  clusters.
Reinitializing k means with  2  clusters.
Cluster labels:  [1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0]
[matrix([[ 328.]]), matrix([[ 366.]]), matrix([[ 266.]]), matrix([[ 277.]]), matrix([[ 266.]]), matrix([[ 282.]]), matrix([[ 285.]]), matrix([[ 276.]]), matrix([[ 295.]]), matrix([[ 287.]]), matrix([[ 274.]]), matrix([[ 287.]]), matrix([[ 268.]]), matrix([[ 304.]]), matrix([[ 271.]]), matrix([[ 273.]]), matrix([[ 284.]]), matrix([[ 270.]]), matrix([[ 280.]]), matrix([[ 262.]]), matrix([[ 263.]]), matrix([[ 285.]]), matrix([[ 270.]]), matrix([[ 284.]]), matrix([[ 282.]]), matrix([[ 278.]]), matrix([[ 260.]]), matrix([[ 269.]]), matrix([[ 258.]]), matrix([[ 271.]]), matrix([[ 283.]]), matrix([[ 283.]]), matrix([[ 300.]]), matrix([[ 295.]]), matrix([[ 304.]]), matrix([[ 275.]]), matrix([[ 268.]]), matrix([[ 270.]]), matrix([[ 259.]]), matrix([[ 268.]]), matrix([[ 260.]]), matrix([[ 264.]]), matrix([[ 259.]])]
[matrix([[ 223.]]), matrix([[ 215.]]), matrix([[ 217.]]), matrix([[ 246.]]), matrix([[ 219.]]), matrix([[ 218.]]), matrix([[ 226.]]), matrix([[ 233.]]), matrix([[ 242.]]), matrix([[ 234.]]), matrix([[ 235.]]), matrix([[ 228.]]), matrix([[ 229.]]), matrix([[ 221.]]), matrix([[ 248.]]), matrix([[ 247.]]), matrix([[ 240.]]), matrix([[ 246.]]), matrix([[ 246.]]), matrix([[ 243.]]), matrix([[ 256.]]), matrix([[ 238.]]), matrix([[ 245.]]), matrix([[ 253.]])]
Clustering the following samples:
[[ 15.]
 [ 18.]
 [ 30.]
 [ 15.]
 [ 31.]
 [ 20.]
 [ 21.]
 [ 32.]
 [ 20.]
 [ 27.]
 [ 25.]
 [ 32.]
 [ 31.]
 [ 20.]
 [ 16.]
 [ 15.]
 [ 17.]
 [ 18.]
 [ 18.]
 [ 14.]
 [ 15.]
 [ 17.]
 [ 18.]
 [ 19.]
 [ 20.]
 [ 22.]
 [ 24.]
 [ 22.]
 [ 19.]
 [ 21.]
 [ 22.]
 [ 22.]
 [ 19.]
 [ 23.]
 [ 31.]
 [ 27.]
 [ 25.]
 [ 23.]
 [ 25.]
 [ 28.]
 [ 30.]
 [ 46.]
 [ 36.]
 [ 65.]
 [ 44.]
 [ 65.]
 [ 45.]
 [ 44.]
 [ 44.]
 [ 50.]
 [ 44.]
 [ 71.]
 [ 19.]
 [ 20.]
 [ 28.]
 [ 29.]
 [ 23.]
 [ 26.]
 [ 23.]
 [ 18.]
 [ 20.]
 [ 22.]
 [ 17.]
 [ 18.]
 [ 20.]
 [ 17.]
 [ 17.]]
Reinitializing k means with  7  clusters.
Cluster labels:  [4 5 3 4 3 5 0 3 5 6 6 3 3 5 4 4 4 5 5 4 4 4 5 5 5 0 0 0 5 0 0 0 5 0 3 6 6
 0 6 6 3 1 3 2 1 2 1 1 1 1 1 2 5 5 6 3 0 6 0 5 5 0 4 5 5 4 4]
[matrix([[ 21.]]), matrix([[ 22.]]), matrix([[ 24.]]), matrix([[ 22.]]), matrix([[ 21.]]), matrix([[ 22.]]), matrix([[ 22.]]), matrix([[ 23.]]), matrix([[ 23.]]), matrix([[ 23.]]), matrix([[ 23.]]), matrix([[ 22.]])]
[matrix([[ 46.]]), matrix([[ 44.]]), matrix([[ 45.]]), matrix([[ 44.]]), matrix([[ 44.]]), matrix([[ 50.]]), matrix([[ 44.]])]
[matrix([[ 65.]]), matrix([[ 65.]]), matrix([[ 71.]])]
[matrix([[ 30.]]), matrix([[ 31.]]), matrix([[ 32.]]), matrix([[ 32.]]), matrix([[ 31.]]), matrix([[ 31.]]), matrix([[ 30.]]), matrix([[ 36.]]), matrix([[ 29.]])]
[matrix([[ 15.]]), matrix([[ 15.]]), matrix([[ 16.]]), matrix([[ 15.]]), matrix([[ 17.]]), matrix([[ 14.]]), matrix([[ 15.]]), matrix([[ 17.]]), matrix([[ 17.]]), matrix([[ 17.]]), matrix([[ 17.]])]
[matrix([[ 18.]]), matrix([[ 20.]]), matrix([[ 20.]]), matrix([[ 20.]]), matrix([[ 18.]]), matrix([[ 18.]]), matrix([[ 18.]]), matrix([[ 19.]]), matrix([[ 20.]]), matrix([[ 19.]]), matrix([[ 19.]]), matrix([[ 19.]]), matrix([[ 20.]]), matrix([[ 18.]]), matrix([[ 20.]]), matrix([[ 18.]]), matrix([[ 20.]])]
[matrix([[ 27.]]), matrix([[ 25.]]), matrix([[ 27.]]), matrix([[ 25.]]), matrix([[ 25.]]), matrix([[ 28.]]), matrix([[ 28.]]), matrix([[ 26.]])]
Clustering the following samples:
[[ 2236.]
 [ 2329.]
 [ 3826.]
 [ 2171.]
 [ 4245.]
 [ 2860.]
 [ 2735.]
 [ 2912.]
 [ 2970.]
 [ 2859.]
 [ 2926.]
 [ 2756.]
 [ 2812.]
 [ 3099.]
 [ 3310.]
 [ 3395.]
 [ 3282.]
 [ 3171.]
 [ 3338.]
 [ 3259.]
 [ 3193.]
 [ 3203.]
 [ 3250.]
 [ 3214.]
 [ 2851.]
 [ 2758.]
 [ 2880.]
 [ 2900.]
 [ 2911.]
 [ 2811.]
 [ 2887.]
 [ 2963.]
 [ 3174.]
 [ 3055.]
 [ 3240.]
 [ 3205.]
 [ 3259.]
 [ 3175.]
 [ 3141.]
 [ 3379.]
 [ 3063.]
 [ 2776.]
 [ 2588.]
 [ 2704.]
 [ 2655.]
 [ 2604.]
 [ 2574.]
 [ 2612.]
 [ 2640.]
 [ 2639.]
 [ 2784.]
 [ 2753.]
 [ 3029.]
 [ 3084.]
 [ 3113.]
 [ 3284.]
 [ 3115.]
 [ 3082.]
 [ 3078.]
 [ 2682.]
 [ 2818.]
 [ 2931.]
 [ 2836.]
 [ 2818.]
 [ 2897.]
 [ 2753.]
 [ 2756.]]
Reinitializing k means with  7  clusters.
Reinitializing k means with  6  clusters.
Reinitializing k means with  5  clusters.
Reinitializing k means with  4  clusters.
Reinitializing k means with  3  clusters.
Reinitializing k means with  2  clusters.
Cluster labels:  [1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0
 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]
[matrix([[ 3826.]]), matrix([[ 4245.]]), matrix([[ 3099.]]), matrix([[ 3310.]]), matrix([[ 3395.]]), matrix([[ 3282.]]), matrix([[ 3171.]]), matrix([[ 3338.]]), matrix([[ 3259.]]), matrix([[ 3193.]]), matrix([[ 3203.]]), matrix([[ 3250.]]), matrix([[ 3214.]]), matrix([[ 3174.]]), matrix([[ 3055.]]), matrix([[ 3240.]]), matrix([[ 3205.]]), matrix([[ 3259.]]), matrix([[ 3175.]]), matrix([[ 3141.]]), matrix([[ 3379.]]), matrix([[ 3063.]]), matrix([[ 3029.]]), matrix([[ 3084.]]), matrix([[ 3113.]]), matrix([[ 3284.]]), matrix([[ 3115.]]), matrix([[ 3082.]]), matrix([[ 3078.]])]
[matrix([[ 2236.]]), matrix([[ 2329.]]), matrix([[ 2171.]]), matrix([[ 2860.]]), matrix([[ 2735.]]), matrix([[ 2912.]]), matrix([[ 2970.]]), matrix([[ 2859.]]), matrix([[ 2926.]]), matrix([[ 2756.]]), matrix([[ 2812.]]), matrix([[ 2851.]]), matrix([[ 2758.]]), matrix([[ 2880.]]), matrix([[ 2900.]]), matrix([[ 2911.]]), matrix([[ 2811.]]), matrix([[ 2887.]]), matrix([[ 2963.]]), matrix([[ 2776.]]), matrix([[ 2588.]]), matrix([[ 2704.]]), matrix([[ 2655.]]), matrix([[ 2604.]]), matrix([[ 2574.]]), matrix([[ 2612.]]), matrix([[ 2640.]]), matrix([[ 2639.]]), matrix([[ 2784.]]), matrix([[ 2753.]]), matrix([[ 2682.]]), matrix([[ 2818.]]), matrix([[ 2931.]]), matrix([[ 2836.]]), matrix([[ 2818.]]), matrix([[ 2897.]]), matrix([[ 2753.]]), matrix([[ 2756.]])]
Clustering the following samples:
[[ 22.86666667]
 [ 20.38888889]
 [ 20.7       ]
 [ 23.53333333]
 [ 22.51612903]
 [ 22.75      ]
 [ 24.61904762]
 [ 16.125     ]
 [ 25.7       ]
 [ 18.85185185]
 [ 20.72      ]
 [ 16.28125   ]
 [ 16.51612903]
 [ 25.1       ]
 [ 32.8125    ]
 [ 35.2       ]
 [ 31.41176471]
 [ 28.55555556]
 [ 29.77777778]
 [ 36.64285714]
 [ 34.        ]
 [ 30.41176471]
 [ 28.72222222]
 [ 28.26315789]
 [ 25.8       ]
 [ 23.63636364]
 [ 21.83333333]
 [ 23.45454545]
 [ 27.42105263]
 [ 24.66666667]
 [ 23.5       ]
 [ 23.40909091]
 [ 28.        ]
 [ 21.86956522]
 [ 16.25806452]
 [ 18.81481481]
 [ 21.16      ]
 [ 22.52173913]
 [ 20.44      ]
 [ 18.53571429]
 [ 16.86666667]
 [ 11.08695652]
 [ 13.69444444]
 [  8.12307692]
 [ 11.63636364]
 [  8.15384615]
 [ 11.31111111]
 [ 11.65909091]
 [ 11.63636364]
 [ 10.26      ]
 [ 11.75      ]
 [  7.30985915]
 [ 27.42105263]
 [ 26.3       ]
 [ 18.75      ]
 [ 18.93103448]
 [ 23.2173913 ]
 [ 20.15384615]
 [ 22.91304348]
 [ 29.16666667]
 [ 27.4       ]
 [ 24.81818182]
 [ 32.05882353]
 [ 31.11111111]
 [ 27.9       ]
 [ 31.47058824]
 [ 32.        ]]
Cluster labels:  [5 2 2 5 5 5 5 4 0 2 2 4 4 5 3 6 3 0 3 6 6 3 0 0 0 5 5 5 0 5 5 5 0 5 4 2 2
 5 2 2 4 1 1 7 1 7 1 1 1 1 1 7 0 0 2 2 5 2 5 0 0 5 3 3 0 3 3]
[matrix([[ 25.7]]), matrix([[ 28.55555556]]), matrix([[ 28.72222222]]), matrix([[ 28.26315789]]), matrix([[ 25.8]]), matrix([[ 27.42105263]]), matrix([[ 28.]]), matrix([[ 27.42105263]]), matrix([[ 26.3]]), matrix([[ 29.16666667]]), matrix([[ 27.4]]), matrix([[ 27.9]])]
[matrix([[ 11.08695652]]), matrix([[ 13.69444444]]), matrix([[ 11.63636364]]), matrix([[ 11.31111111]]), matrix([[ 11.65909091]]), matrix([[ 11.63636364]]), matrix([[ 10.26]]), matrix([[ 11.75]])]
[matrix([[ 20.38888889]]), matrix([[ 20.7]]), matrix([[ 18.85185185]]), matrix([[ 20.72]]), matrix([[ 18.81481481]]), matrix([[ 21.16]]), matrix([[ 20.44]]), matrix([[ 18.53571429]]), matrix([[ 18.75]]), matrix([[ 18.93103448]]), matrix([[ 20.15384615]])]
[matrix([[ 32.8125]]), matrix([[ 31.41176471]]), matrix([[ 29.77777778]]), matrix([[ 30.41176471]]), matrix([[ 32.05882353]]), matrix([[ 31.11111111]]), matrix([[ 31.47058824]]), matrix([[ 32.]])]
[matrix([[ 16.125]]), matrix([[ 16.28125]]), matrix([[ 16.51612903]]), matrix([[ 16.25806452]]), matrix([[ 16.86666667]])]
[matrix([[ 22.86666667]]), matrix([[ 23.53333333]]), matrix([[ 22.51612903]]), matrix([[ 22.75]]), matrix([[ 24.61904762]]), matrix([[ 25.1]]), matrix([[ 23.63636364]]), matrix([[ 21.83333333]]), matrix([[ 23.45454545]]), matrix([[ 24.66666667]]), matrix([[ 23.5]]), matrix([[ 23.40909091]]), matrix([[ 21.86956522]]), matrix([[ 22.52173913]]), matrix([[ 23.2173913]]), matrix([[ 22.91304348]]), matrix([[ 24.81818182]])]
[matrix([[ 35.2]]), matrix([[ 36.64285714]]), matrix([[ 34.]])]
[matrix([[ 8.12307692]]), matrix([[ 8.15384615]]), matrix([[ 7.30985915]])]
Clustering the following samples:
[[ 0.65014577]
 [ 0.58583106]
 [ 0.52818035]
 [ 0.61473088]
 [ 0.5243553 ]
 [ 0.58461538]
 [ 0.53578337]
 [ 0.51550388]
 [ 0.47859922]
 [ 0.5540275 ]
 [ 0.55019305]
 [ 0.52975048]
 [ 0.57617188]
 [ 0.43625498]
 [ 0.54666667]
 [ 0.51893939]
 [ 0.53745318]
 [ 0.52140078]
 [ 0.56716418]
 [ 0.52826511]
 [ 0.53529412]
 [ 0.54932302]
 [ 0.52224371]
 [ 0.52141527]
 [ 0.42248062]
 [ 0.43461538]
 [ 0.44465649]
 [ 0.46899225]
 [ 0.44913628]
 [ 0.45366795]
 [ 0.4410058 ]
 [ 0.44466019]
 [ 0.41541353]
 [ 0.49304175]
 [ 0.49007937]
 [ 0.47244094]
 [ 0.46502836]
 [ 0.47490347]
 [ 0.51272016]
 [ 0.50674374]
 [ 0.48023715]
 [ 0.55882353]
 [ 0.54766734]
 [ 0.53787879]
 [ 0.55078125]
 [ 0.48301887]
 [ 0.54616896]
 [ 0.50682261]
 [ 0.52539062]
 [ 0.50292398]
 [ 0.52417795]
 [ 0.54527938]
 [ 0.54318618]
 [ 0.57034221]
 [ 0.56190476]
 [ 0.55373406]
 [ 0.51498127]
 [ 0.51145038]
 [ 0.51233397]
 [ 0.45333333]
 [ 0.47262774]
 [ 0.49084249]
 [ 0.44954128]
 [ 0.46428571]
 [ 0.47311828]
 [ 0.4728972 ]
 [ 0.47610294]]
Reinitializing k means with  7  clusters.
Reinitializing k means with  6  clusters.
Reinitializing k means with  5  clusters.
Reinitializing k means with  4  clusters.
Reinitializing k means with  3  clusters.
Cluster labels:  [2 2 1 2 1 2 1 1 0 1 1 1 2 0 1 1 1 1 2 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 1 0 2 1 1 1 0 1 1 1 1 1 1 1 2 2 1 1 1 1 0 0 0 0 0 0 0 0]
[matrix([[ 0.47859922]]), matrix([[ 0.43625498]]), matrix([[ 0.42248062]]), matrix([[ 0.43461538]]), matrix([[ 0.44465649]]), matrix([[ 0.46899225]]), matrix([[ 0.44913628]]), matrix([[ 0.45366795]]), matrix([[ 0.4410058]]), matrix([[ 0.44466019]]), matrix([[ 0.41541353]]), matrix([[ 0.49304175]]), matrix([[ 0.49007937]]), matrix([[ 0.47244094]]), matrix([[ 0.46502836]]), matrix([[ 0.47490347]]), matrix([[ 0.48023715]]), matrix([[ 0.48301887]]), matrix([[ 0.45333333]]), matrix([[ 0.47262774]]), matrix([[ 0.49084249]]), matrix([[ 0.44954128]]), matrix([[ 0.46428571]]), matrix([[ 0.47311828]]), matrix([[ 0.4728972]]), matrix([[ 0.47610294]])]
[matrix([[ 0.52818035]]), matrix([[ 0.5243553]]), matrix([[ 0.53578337]]), matrix([[ 0.51550388]]), matrix([[ 0.5540275]]), matrix([[ 0.55019305]]), matrix([[ 0.52975048]]), matrix([[ 0.54666667]]), matrix([[ 0.51893939]]), matrix([[ 0.53745318]]), matrix([[ 0.52140078]]), matrix([[ 0.52826511]]), matrix([[ 0.53529412]]), matrix([[ 0.54932302]]), matrix([[ 0.52224371]]), matrix([[ 0.52141527]]), matrix([[ 0.51272016]]), matrix([[ 0.50674374]]), matrix([[ 0.54766734]]), matrix([[ 0.53787879]]), matrix([[ 0.55078125]]), matrix([[ 0.54616896]]), matrix([[ 0.50682261]]), matrix([[ 0.52539062]]), matrix([[ 0.50292398]]), matrix([[ 0.52417795]]), matrix([[ 0.54527938]]), matrix([[ 0.54318618]]), matrix([[ 0.55373406]]), matrix([[ 0.51498127]]), matrix([[ 0.51145038]]), matrix([[ 0.51233397]])]
[matrix([[ 0.65014577]]), matrix([[ 0.58583106]]), matrix([[ 0.61473088]]), matrix([[ 0.58461538]]), matrix([[ 0.57617188]]), matrix([[ 0.56716418]]), matrix([[ 0.55882353]]), matrix([[ 0.57034221]]), matrix([[ 0.56190476]])]
Clustering the following samples:
[[  33.6638173 ]
 [  54.05362928]
 [  55.99551449]
 [  54.25121624]
 [  55.86909465]
 [  45.40880495]
 [  69.10121212]
 [  67.17510174]
 [  59.11642607]
 [  64.20763953]
 [  61.35400695]
 [  78.10492472]
 [  74.90276966]
 [  44.17842032]
 [  22.53945536]
 [  27.22290909]
 [  35.69475545]
 [  36.30247298]
 [  29.82324212]
 [  15.61442982]
 [  26.51441176]
 [  30.16705882]
 [  25.99103535]
 [  45.49761541]
 [  66.53637209]
 [  70.74909091]
 [  70.20584606]
 [  60.55538055]
 [  54.94428225]
 [  60.77787001]
 [  61.23704545]
 [  56.58545234]
 [  44.35898496]
 [  52.60757023]
 [  59.90806452]
 [  53.67693934]
 [  53.10013308]
 [  52.82910274]
 [  54.30444697]
 [  52.56344653]
 [  59.80584717]
 [  94.55938619]
 [  93.40572712]
 [ 111.10598601]
 [  97.70104403]
 [ 109.80865747]
 [ 105.76797468]
 [  98.03260167]
 [  94.89205966]
 [  95.00004737]
 [  93.12693182]
 [ 102.26404787]
 [  53.97000202]
 [  51.79297148]
 [  60.66203571]
 [  54.32491803]
 [  55.26035906]
 [  58.67159425]
 [  55.47427605]
 [  65.39769048]
 [  64.93750365]
 [  64.19619381]
 [  66.10043173]
 [  64.06865079]
 [  66.01972581]
 [  61.98693238]
 [  64.09507353]]
Reinitializing k means with  7  clusters.
Cluster labels:  [2 0 0 0 0 5 6 3 3 3 3 6 6 5 2 2 5 5 2 2 2 2 2 5 3 6 6 3 0 3 3 0 5 0 3 0 0
 0 0 0 3 1 1 4 1 4 4 1 1 1 1 4 0 0 3 0 0 0 0 3 3 3 3 3 3 3 3]
[matrix([[ 54.05362928]]), matrix([[ 55.99551449]]), matrix([[ 54.25121624]]), matrix([[ 55.86909465]]), matrix([[ 54.94428225]]), matrix([[ 56.58545234]]), matrix([[ 52.60757023]]), matrix([[ 53.67693934]]), matrix([[ 53.10013308]]), matrix([[ 52.82910274]]), matrix([[ 54.30444697]]), matrix([[ 52.56344653]]), matrix([[ 53.97000202]]), matrix([[ 51.79297148]]), matrix([[ 54.32491803]]), matrix([[ 55.26035906]]), matrix([[ 58.67159425]]), matrix([[ 55.47427605]])]
[matrix([[ 94.55938619]]), matrix([[ 93.40572712]]), matrix([[ 97.70104403]]), matrix([[ 98.03260167]]), matrix([[ 94.89205966]]), matrix([[ 95.00004737]]), matrix([[ 93.12693182]])]
[matrix([[ 33.6638173]]), matrix([[ 22.53945536]]), matrix([[ 27.22290909]]), matrix([[ 29.82324212]]), matrix([[ 15.61442982]]), matrix([[ 26.51441176]]), matrix([[ 30.16705882]]), matrix([[ 25.99103535]])]
[matrix([[ 67.17510174]]), matrix([[ 59.11642607]]), matrix([[ 64.20763953]]), matrix([[ 61.35400695]]), matrix([[ 66.53637209]]), matrix([[ 60.55538055]]), matrix([[ 60.77787001]]), matrix([[ 61.23704545]]), matrix([[ 59.90806452]]), matrix([[ 59.80584717]]), matrix([[ 60.66203571]]), matrix([[ 65.39769048]]), matrix([[ 64.93750365]]), matrix([[ 64.19619381]]), matrix([[ 66.10043173]]), matrix([[ 64.06865079]]), matrix([[ 66.01972581]]), matrix([[ 61.98693238]]), matrix([[ 64.09507353]])]
[matrix([[ 111.10598601]]), matrix([[ 109.80865747]]), matrix([[ 105.76797468]]), matrix([[ 102.26404787]])]
[matrix([[ 45.40880495]]), matrix([[ 44.17842032]]), matrix([[ 35.69475545]]), matrix([[ 36.30247298]]), matrix([[ 45.49761541]]), matrix([[ 44.35898496]])]
[matrix([[ 69.10121212]]), matrix([[ 78.10492472]]), matrix([[ 74.90276966]]), matrix([[ 70.74909091]]), matrix([[ 70.20584606]])]
generate_target_clusters on:  [{0: [matrix([[ 2637.6]])], 1: [matrix([[ 2159.35135135]])]}, {0: [matrix([[ 13.17147431]])], 1: [matrix([[ 5.33729213]])], 2: [matrix([[ 20.91296982]])], 3: [matrix([[ 16.55949282]])], 4: [matrix([[ 10.34333539]])], 5: [matrix([[ 23.04377309]])], 6: [matrix([[ 14.95211473]])], 7: [matrix([[ 18.74616594]])]}, {0: [matrix([[ 1.51812996]])], 1: [matrix([[ 1.17513236]])], 2: [matrix([[ 1.7647235]])], 3: [matrix([[ 1.4541623]])], 4: [matrix([[ 1.6553311]])], 5: [matrix([[ 1.0473622]])], 6: [matrix([[ 1.33333917]])], 7: [matrix([[ 1.57071445]])]}, {0: [matrix([[ 279.27906977]])], 1: [matrix([[ 235.33333333]])]}, {0: [matrix([[ 22.33333333]])], 1: [matrix([[ 45.28571429]])], 2: [matrix([[ 67.]])], 3: [matrix([[ 31.33333333]])], 4: [matrix([[ 15.90909091]])], 5: [matrix([[ 19.05882353]])], 6: [matrix([[ 26.375]])]}, {0: [matrix([[ 3250.37931034]])], 1: [matrix([[ 2745.97368421]])]}, {0: [matrix([[ 27.5541423]])], 1: [matrix([[ 11.62929128]])], 2: [matrix([[ 19.76783186]])], 3: [matrix([[ 31.38179126]])], 4: [matrix([[ 16.40942204]])], 5: [matrix([[ 23.36618221]])], 6: [matrix([[ 35.28095238]])], 7: [matrix([[ 7.86226074]])]}, {0: [matrix([[ 0.46157622]])], 1: [matrix([[ 0.53003237]])], 2: [matrix([[ 0.58552552]])]}, {0: [matrix([[ 54.45971939]])], 1: [matrix([[ 95.2453997]])], 2: [matrix([[ 26.44204495]])], 3: [matrix([[ 63.05989431]])], 4: [matrix([[ 107.23666651]])], 5: [matrix([[ 41.90684234]])], 6: [matrix([[ 72.61276869]])]}]  of len  9
Returning target configuration of len:  9
Initial target configuration:  [2159.3513513513512, 5.337292130618314, 1.3333391716131937, 235.33333333333334, 22.333333333333332, 2745.9736842105262, 35.280952380952385, 0.5855255165346226, 95.2453996954096]
User probability:  0.0652762580174
numAuthors:  8
Highest probability in array:  0.316694164889
letterSpace   letterSpace   7
gunningFog   gunningFog   5
avgSyllablesPerWord   avgSyllablesPerWord   6
unique_words   unique_words   2
sentenceCount   sentenceCount   4
characterSpace   characterSpace   1
avgSentenceLength   avgSentenceLength   0
complexity   complexity   3
fleschReadingEase   fleschReadingEase   8
F
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02700000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24000000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08004202   0.28795539   0.6875189    9.78563204   1.50249762
   10.04347242   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669528]
Probability:  0.316695276258
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669528]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Fe
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02800000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24100000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08346392   0.28795539   0.6875189    9.78563204   1.50249762
   10.04653847   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669527]
Probability:  0.316695266885
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669527]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02900000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24200000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08688582   0.28795539   0.6875189    9.78563204   1.50249762
   10.04960452   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669526]
Probability:  0.316695257585
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669526]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02900000e+03   1.57136533e+01   1.58171206e+00   5.30000000e+02
    4.50000000e+01   6.24300000e+03   2.28444444e+01   5.15564202e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.08688582   0.28846552   0.67940567   9.78563204   1.50249762
   10.05267058   0.0626539    0.09791216  -0.53126282]]
Probabilities: [ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669525]
Probability:  0.316695253558
Random chance:  0.125
[[ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669525]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few t
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03000000e+03   1.57136533e+01   1.58268482e+00   5.31000000e+02
    4.50000000e+01   6.24400000e+03   2.28444444e+01   5.16536965e-01
    4.97527527e+01]]
Scaled doc features:  [[  9.09030772   0.28846552   0.68453008   9.82235625   1.50249762
   10.05573663   0.0626539    0.11842765  -0.53511121]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
  0.08456969  0.31669519]
Probability:  0.316695192494
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
   0.08456969  0.31669519]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few tr
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03100000e+03   1.57136533e+01   1.58171206e+00   5.31000000e+02
    4.50000000e+01   6.24500000e+03   2.28444444e+01   5.16536965e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.09372963   0.28846552   0.67940567   9.82235625   1.50249762
   10.05880268   0.0626539    0.11842765  -0.53126282]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
  0.08456969  0.31669518]
Probability:  0.316695184472
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
   0.08456969  0.31669518]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few tra
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03200000e+03   1.57136533e+01   1.58171206e+00   5.31000000e+02
    4.50000000e+01   6.24600000e+03   2.28444444e+01   5.16536965e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.09715153   0.28846552   0.67940567   9.82235625   1.50249762
   10.06186873   0.0626539    0.11842765  -0.53126282]]
Probabilities: [ 0.0652762   0.12059243  0.09105776  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669518]
Probability:  0.316695175852
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105776  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669518]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traf
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03300000e+03   1.57136533e+01   1.58171206e+00   5.31000000e+02
    4.50000000e+01   6.24700000e+03   2.28444444e+01   5.16536965e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.10057343   0.28846552   0.67940567   9.82235625   1.50249762
   10.06493479   0.0626539    0.11842765  -0.53126282]]
Probabilities: [ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669517]
Probability:  0.3166951673
Random chance:  0.125
[[ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669517]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traff
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03400000e+03   1.57136533e+01   1.58171206e+00   5.31000000e+02
    4.50000000e+01   6.24800000e+03   2.28444444e+01   5.16536965e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.10399533   0.28846552   0.67940567   9.82235625   1.50249762
   10.06800084   0.0626539    0.11842765  -0.53126282]]
Probabilities: [ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669516]
Probability:  0.316695158815
Random chance:  0.125
[[ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669516]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffi
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03500000e+03   1.57136533e+01   1.58171206e+00   5.31000000e+02
    4.50000000e+01   6.24900000e+03   2.28444444e+01   5.16536965e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.10741723   0.28846552   0.67940567   9.82235625   1.50249762
   10.07106689   0.0626539    0.11842765  -0.53126282]]
Probabilities: [ 0.0652762   0.12059244  0.09105776  0.18878292  0.06604529  0.06698055
  0.08456969  0.31669515]
Probability:  0.316695150398
Random chance:  0.125
[[ 0.0652762   0.12059244  0.09105776  0.18878292  0.06604529  0.06698055
   0.08456969  0.31669515]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03600000e+03   1.57136533e+01   1.58365759e+00   5.30000000e+02
    4.50000000e+01   6.25000000e+03   2.28444444e+01   5.15564202e-01
    4.96704570e+01]]
Scaled doc features:  [[  9.11083913   0.28846552   0.68965449   9.78563204   1.50249762
   10.07413294   0.0626539    0.09791216  -0.53895961]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
  0.08456969  0.31669519]
Probability:  0.316695189363
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
   0.08456969  0.31669519]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03600000e+03   1.57161516e+01   1.58211856e+00   5.30000000e+02
    4.50000000e+01   6.25100000e+03   2.28666667e+01   5.15063168e-01
    4.97781030e+01]]
Scaled doc features:  [[  9.11083913   0.2889782    0.68154707   9.78563204   1.50249762
   10.077199     0.06579571   0.08734539  -0.53392575]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
  0.08456969  0.31669519]
Probability:  0.316695185549
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
   0.08456969  0.31669519]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic j
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03700000e+03   1.57161516e+01   1.58309038e+00   5.31000000e+02
    4.50000000e+01   6.25200000e+03   2.28666667e+01   5.16034985e-01
    4.96958873e+01]]
Scaled doc features:  [[  9.11426104   0.2889782    0.6866665    9.82235625   1.50249762
   10.08026505   0.06579571   0.10784094  -0.53777041]]
Probabilities: [ 0.0652762   0.12059244  0.09105776  0.18878292  0.0660453   0.06698055
  0.08456969  0.31669513]
Probability:  0.31669512831
Random chance:  0.125
[[ 0.0652762   0.12059244  0.09105776  0.18878292  0.0660453   0.06698055
   0.08456969  0.31669513]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic ja
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03800000e+03   1.57161516e+01   1.58309038e+00   5.31000000e+02
    4.50000000e+01   6.25300000e+03   2.28666667e+01   5.16034985e-01
    4.96958873e+01]]
Scaled doc features:  [[  9.11768294   0.2889782    0.6866665    9.82235625   1.50249762
   10.0833311    0.06579571   0.10784094  -0.53777041]]
Probabilities: [ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
  0.08456969  0.31669512]
Probability:  0.316695120136
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
   0.08456969  0.31669512]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jam
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03900000e+03   1.57161516e+01   1.58309038e+00   5.30000000e+02
    4.50000000e+01   6.25400000e+03   2.28666667e+01   5.15063168e-01
    4.96958873e+01]]
Scaled doc features:  [[  9.12110484   0.2889782    0.6866665    9.78563204   1.50249762
   10.08639715   0.06579571   0.08734539  -0.53777041]]
Probabilities: [ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669516]
Probability:  0.316695159139
Random chance:  0.125
[[ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669516]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04000000e+03   1.57161516e+01   1.58309038e+00   5.31000000e+02
    4.50000000e+01   6.25500000e+03   2.28666667e+01   5.16034985e-01
    4.96958873e+01]]
Scaled doc features:  [[  9.12452674   0.2889782    0.6866665    9.82235625   1.50249762
   10.08946321   0.06579571   0.10784094  -0.53777041]]
Probabilities: [ 0.0652762   0.12059245  0.09105777  0.18878293  0.0660453   0.06698055
  0.08456969  0.3166951 ]
Probability:  0.316695103981
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105777  0.18878293  0.0660453   0.06698055
   0.08456969  0.3166951 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04000000e+03   1.57186624e+01   1.58155340e+00   5.31000000e+02
    4.50000000e+01   6.25600000e+03   2.28888889e+01   5.15533981e-01
    4.98033603e+01]]
Scaled doc features:  [[  9.12452674   0.28949343   0.67856984   9.82235625   1.50249762
   10.09252926   0.06893752   0.09727478  -0.53274465]]
Probabilities: [ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
  0.08456969  0.3166951 ]
Probability:  0.316695100488
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
   0.08456969  0.3166951 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams a
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04100000e+03   1.57186624e+01   1.58252427e+00   5.31000000e+02
    4.50000000e+01   6.25700000e+03   2.28888889e+01   5.15533981e-01
    4.97212244e+01]]
Scaled doc features:  [[  9.12794864   0.28949343   0.6836843    9.82235625   1.50249762
   10.09559531   0.06893752   0.09727478  -0.53658557]]
Probabilities: [ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
  0.08456969  0.31669509]
Probability:  0.316695091934
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
   0.08456969  0.31669509]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams ar
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04200000e+03   1.57186624e+01   1.58252427e+00   5.32000000e+02
    4.50000000e+01   6.25800000e+03   2.28888889e+01   5.16504854e-01
    4.97212244e+01]]
Scaled doc features:  [[  9.13137055   0.28949343   0.6836843    9.85908047   1.50249762
   10.09866136   0.06893752   0.11775044  -0.53658557]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669504]
Probability:  0.316695040193
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669504]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04300000e+03   1.57186624e+01   1.58252427e+00   5.31000000e+02
    4.50000000e+01   6.25900000e+03   2.28888889e+01   5.15533981e-01
    4.97212244e+01]]
Scaled doc features:  [[  9.13479245   0.28949343   0.6836843    9.82235625   1.50249762
   10.10172742   0.06893752   0.09727478  -0.53658557]]
Probabilities: [ 0.0652762   0.12059246  0.09105777  0.18878294  0.0660453   0.06698056
  0.08456969  0.31669508]
Probability:  0.316695076227
Random chance:  0.125
[[ 0.0652762   0.12059246  0.09105777  0.18878294  0.0660453   0.06698056
   0.08456969  0.31669508]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04300000e+03   1.57211855e+01   1.58098933e+00   5.31000000e+02
    4.50000000e+01   6.26000000e+03   2.29111111e+01   5.15033948e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.13479245   0.2900112    0.6755984    9.82235625   1.50249762
   10.10479347   0.07207933   0.08672913  -0.53156788]]
Probabilities: [ 0.0652762   0.12059246  0.09105777  0.18878295  0.0660453   0.06698056
  0.08456969  0.31669507]
Probability:  0.316695072799
Random chance:  0.125
[[ 0.0652762   0.12059246  0.09105777  0.18878295  0.0660453   0.06698056
   0.08456969  0.31669507]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are c
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04400000e+03   1.57211855e+01   1.58195926e+00   5.32000000e+02
    4.50000000e+01   6.26100000e+03   2.29111111e+01   5.16003880e-01
    4.97464686e+01]]
Scaled doc features:  [[  9.13821435   0.2900112    0.6807079    9.85908047   1.50249762
   10.10785952   0.07207933   0.10718492  -0.53540507]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669502]
Probability:  0.316695021606
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669502]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are co
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04500000e+03   1.57211855e+01   1.58195926e+00   5.32000000e+02
    4.50000000e+01   6.26200000e+03   2.29111111e+01   5.16003880e-01
    4.97464686e+01]]
Scaled doc features:  [[  9.14163625   0.2900112    0.6807079    9.85908047   1.50249762
   10.11092557   0.07207933   0.10718492  -0.53540507]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878297  0.0660453   0.06698056
  0.0845697   0.31669501]
Probability:  0.316695014306
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878297  0.0660453   0.06698056
   0.0845697   0.31669501]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are com
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04600000e+03   1.57211855e+01   1.58195926e+00   5.32000000e+02
    4.50000000e+01   6.26300000e+03   2.29111111e+01   5.16003880e-01
    4.97464686e+01]]
Scaled doc features:  [[  9.14505815   0.2900112    0.6807079    9.85908047   1.50249762
   10.11399163   0.07207933   0.10718492  -0.53540507]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878297  0.0660453   0.06698056
  0.0845697   0.31669501]
Probability:  0.316695007064
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878297  0.0660453   0.06698056
   0.0845697   0.31669501]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comp
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04700000e+03   1.57211855e+01   1.58195926e+00   5.32000000e+02
    4.50000000e+01   6.26400000e+03   2.29111111e+01   5.16003880e-01
    4.97464686e+01]]
Scaled doc features:  [[  9.14848005   0.2900112    0.6807079    9.85908047   1.50249762
   10.11705768   0.07207933   0.10718492  -0.53540507]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878297  0.0660453   0.06698056
  0.0845697   0.316695  ]
Probability:  0.31669499988
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878297  0.0660453   0.06698056
   0.0845697   0.316695  ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are compa
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04800000e+03   1.57211855e+01   1.58098933e+00   5.32000000e+02
    4.50000000e+01   6.26500000e+03   2.29111111e+01   5.16003880e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.15190196   0.2900112    0.6755984    9.85908047   1.50249762
   10.12012373   0.07207933   0.10718492  -0.53156788]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669499]
Probability:  0.316694993286
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669499]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are compar
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04900000e+03   1.57211855e+01   1.58098933e+00   5.32000000e+02
    4.50000000e+01   6.26600000e+03   2.29111111e+01   5.16003880e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.15532386   0.2900112    0.6755984    9.85908047   1.50249762
   10.12318978   0.07207933   0.10718492  -0.53156788]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669499]
Probability:  0.316694986212
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669499]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are compara
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05000000e+03   1.57211855e+01   1.58098933e+00   5.32000000e+02
    4.50000000e+01   6.26700000e+03   2.29111111e+01   5.16003880e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.15874576   0.2900112    0.6755984    9.85908047   1.50249762
   10.12625583   0.07207933   0.10718492  -0.53156788]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669498]
Probability:  0.316694979195
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669498]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparab
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05100000e+03   1.57211855e+01   1.58098933e+00   5.32000000e+02
    4.50000000e+01   6.26800000e+03   2.29111111e+01   5.16003880e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.16216766   0.2900112    0.6755984    9.85908047   1.50249762
   10.12932189   0.07207933   0.10718492  -0.53156788]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669497]
Probability:  0.316694972233
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669497]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparabl
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05200000e+03   1.57211855e+01   1.58098933e+00   5.32000000e+02
    4.50000000e+01   6.26900000e+03   2.29111111e+01   5.16003880e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.16558956   0.2900112    0.6755984    9.85908047   1.50249762
   10.13238794   0.07207933   0.10718492  -0.53156788]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669497]
Probability:  0.316694965328
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669497]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05300000e+03   1.57599828e+01   1.58486906e+00   5.32000000e+02
    4.50000000e+01   6.27000000e+03   2.29111111e+01   5.16003880e-01
    4.95002998e+01]]
Scaled doc features:  [[  9.16901147   0.29797276   0.69603639   9.85908047   1.50249762
   10.13545399   0.07207933   0.10718492  -0.54691666]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669496]
Probability:  0.316694956158
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669496]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparablet
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05400000e+03   1.57211855e+01   1.58098933e+00   5.32000000e+02
    4.50000000e+01   6.27100000e+03   2.29111111e+01   5.16003880e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.17243337   0.2900112    0.6755984    9.85908047   1.50249762
   10.13852004   0.07207933   0.10718492  -0.53156788]]
Probabilities: [ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669495]
Probability:  0.316694951683
Random chance:  0.125
[[ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669495]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05300000e+03   1.57599828e+01   1.58486906e+00   5.32000000e+02
    4.50000000e+01   6.27000000e+03   2.29111111e+01   5.16003880e-01
    4.95002998e+01]]
Scaled doc features:  [[  9.16901147   0.29797276   0.69603639   9.85908047   1.50249762
   10.13545399   0.07207933   0.10718492  -0.54691666]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669496]
Probability:  0.316694956158
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669496]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05300000e+03   1.57624806e+01   1.58333333e+00   5.32000000e+02
    4.50000000e+01   6.27100000e+03   2.29333333e+01   5.15503876e-01
    4.96076667e+01]]
Scaled doc features:  [[  9.16901147   0.29848535   0.68794635   9.85908047   1.50249762
   10.13852004   0.07522113   0.09663988  -0.54189587]]
Probabilities: [ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669495]
Probability:  0.316694953206
Random chance:  0.125
[[ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669495]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable t
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05400000e+03   1.57624806e+01   1.58430233e+00   5.33000000e+02
    4.50000000e+01   6.27200000e+03   2.29333333e+01   5.16472868e-01
    4.95256899e+01]]
Scaled doc features:  [[  9.17243337   0.29848535   0.6930509    9.89580468   1.50249762
   10.1415861    0.07522113   0.11707585  -0.54572935]]
Probabilities: [ 0.06527621  0.1205925   0.0910578   0.18878301  0.0660453   0.06698057
  0.08456971  0.31669491]
Probability:  0.316694908449
Random chance:  0.125
[[ 0.06527621  0.1205925   0.0910578   0.18878301  0.0660453   0.06698057
   0.08456971  0.31669491]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable to
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05500000e+03   1.57624806e+01   1.58430233e+00   5.32000000e+02
    4.50000000e+01   6.27300000e+03   2.29333333e+01   5.15503876e-01
    4.95256899e+01]]
Scaled doc features:  [[  9.17585527   0.29848535   0.6930509    9.85908047   1.50249762
   10.14465215   0.07522113   0.09663988  -0.54572935]]
Probabilities: [ 0.06527621  0.12059249  0.09105779  0.188783    0.0660453   0.06698057
  0.0845697   0.31669494]
Probability:  0.316694939241
Random chance:  0.125
[[ 0.06527621  0.12059249  0.09105779  0.188783    0.0660453   0.06698057
   0.0845697   0.31669494]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable to 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05500000e+03   1.57649909e+01   1.58276864e+00   5.32000000e+02
    4.50000000e+01   6.27400000e+03   2.29555556e+01   5.15004840e-01
    4.96328846e+01]]
Scaled doc features:  [[  9.17585527   0.29900047   0.68497158   9.85908047   1.50249762
   10.1477182    0.07836294   0.08611526  -0.5407166 ]]
Probabilities: [ 0.06527621  0.12059249  0.0910578   0.188783    0.0660453   0.06698057
  0.0845697   0.31669494]
Probability:  0.316694936322
Random chance:  0.125
[[ 0.06527621  0.12059249  0.0910578   0.188783    0.0660453   0.06698057
   0.0845697   0.31669494]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable to t
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05600000e+03   1.57649909e+01   1.58373669e+00   5.33000000e+02
    4.50000000e+01   6.27500000e+03   2.29555556e+01   5.15972894e-01
    4.95509872e+01]]
Scaled doc features:  [[  9.17927717   0.29900047   0.69007119   9.89580468   1.50249762
   10.15078425   0.07836294   0.10653144  -0.54454637]]
Probabilities: [ 0.06527622  0.1205925   0.0910578   0.18878301  0.0660453   0.06698057
  0.08456971  0.31669489]
Probability:  0.316694892553
Random chance:  0.125
[[ 0.06527622  0.1205925   0.0910578   0.18878301  0.0660453   0.06698057
   0.08456971  0.31669489]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparable
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05300000e+03   1.57599828e+01   1.58486906e+00   5.32000000e+02
    4.50000000e+01   6.27000000e+03   2.29111111e+01   5.16003880e-01
    4.95002998e+01]]
Scaled doc features:  [[  9.16901147   0.29797276   0.69603639   9.85908047   1.50249762
   10.13545399   0.07207933   0.10718492  -0.54691666]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669496]
Probability:  0.316694956158
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669496]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Few traffic jams are comparablet
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05400000e+03   1.57211855e+01   1.58098933e+00   5.32000000e+02
    4.50000000e+01   6.27100000e+03   2.29111111e+01   5.16003880e-01
    4.98285248e+01]]
Scaled doc features:  [[  9.17243337   0.2900112    0.6755984    9.85908047   1.50249762
   10.13852004   0.07207933   0.10718492  -0.53156788]]
Probabilities: [ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669495]
Probability:  0.316694951683
Random chance:  0.125
[[ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669495]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
F
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02700000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24000000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08004202   0.28795539   0.6875189    9.78563204   1.50249762
   10.04347242   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669528]
Probability:  0.316695276258
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669528]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Commuters stuck in traffic on the Leesburg Pike in Northern Virginia are just a few hundred yards away from an even bigger jam, one that stretches around the world.
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.16000000e+03   1.56573254e+01   1.58009479e+00   5.38000000e+02
    4.60000000e+01   6.40300000e+03   2.29347826e+01   5.09952607e-01
    4.98801767e+01]]
Scaled doc features:  [[  9.53515496   0.27690651   0.67088604  10.07942573   1.58387784
   10.54323896   0.07542603  -0.02043595  -0.52915248]]
Probabilities: [ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
  0.08456974  0.31669436]
Probability:  0.316694361601
Random chance:  0.125
[[ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
   0.08456974  0.31669436]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
I
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02700000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24000000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08004202   0.28795539   0.6875189    9.78563204   1.50249762
   10.04347242   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669528]
Probability:  0.316695276258
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669528]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
In
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02800000e+03   1.57111674e+01   1.58325219e+00   5.29000000e+02
    4.50000000e+01   6.24100000e+03   2.28222222e+01   5.15092502e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08346392   0.28795539   0.6875189    9.74890783   1.50249762
   10.04653847   0.0595121    0.08796405  -0.53630103]]
Probabilities: [ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698054
  0.08456968  0.31669532]
Probability:  0.316695321359
Random chance:  0.125
[[ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698054
   0.08456968  0.31669532]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Int
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02900000e+03   1.57111674e+01   1.58227848e+00   5.30000000e+02
    4.50000000e+01   6.24200000e+03   2.28222222e+01   5.16066212e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.08688582   0.28795539   0.6823895    9.78563204   1.50249762
   10.04960452   0.0595121    0.10849951  -0.53244889]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669526]
Probability:  0.3166952583
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669526]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Inte
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03000000e+03   1.57111674e+01   1.58227848e+00   5.30000000e+02
    4.50000000e+01   6.24300000e+03   2.28222222e+01   5.16066212e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.09030772   0.28795539   0.6823895    9.78563204   1.50249762
   10.05267058   0.0595121    0.10849951  -0.53244889]]
Probabilities: [ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669525]
Probability:  0.316695249067
Random chance:  0.125
[[ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669525]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Inter
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03100000e+03   1.57111674e+01   1.58422590e+00   5.30000000e+02
    4.50000000e+01   6.24400000e+03   2.28222222e+01   5.16066212e-01
    4.96449332e+01]]
Scaled doc features:  [[  9.09372963   0.28795539   0.6926483    9.78563204   1.50249762
   10.05573663   0.0595121    0.10849951  -0.54015317]]
Probabilities: [ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669524]
Probability:  0.316695238493
Random chance:  0.125
[[ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669524]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Intern
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03200000e+03   1.57111674e+01   1.58422590e+00   5.30000000e+02
    4.50000000e+01   6.24500000e+03   2.28222222e+01   5.16066212e-01
    4.96449332e+01]]
Scaled doc features:  [[  9.09715153   0.28795539   0.6926483    9.78563204   1.50249762
   10.05880268   0.0595121    0.10849951  -0.54015317]]
Probabilities: [ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
  0.08456968  0.31669523]
Probability:  0.316695229417
Random chance:  0.125
[[ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
   0.08456968  0.31669523]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Interne
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03300000e+03   1.57111674e+01   1.58227848e+00   5.30000000e+02
    4.50000000e+01   6.24600000e+03   2.28222222e+01   5.16066212e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.10057343   0.28795539   0.6823895    9.78563204   1.50249762
   10.06186873   0.0595121    0.10849951  -0.53244889]]
Probabilities: [ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
  0.08456968  0.31669522]
Probability:  0.316695221806
Random chance:  0.125
[[ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
   0.08456968  0.31669522]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Internet
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03400000e+03   1.57501158e+01   1.58519961e+00   5.29000000e+02
    4.50000000e+01   6.24700000e+03   2.28222222e+01   5.15092502e-01
    4.95625574e+01]]
Scaled doc features:  [[  9.10399533   0.29594796   0.6977777    9.74890783   1.50249762
   10.06493479   0.0595121    0.08796405  -0.54400531]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669526]
Probability:  0.316695262151
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669526]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Interne
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03300000e+03   1.57111674e+01   1.58227848e+00   5.30000000e+02
    4.50000000e+01   6.24600000e+03   2.28222222e+01   5.16066212e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.10057343   0.28795539   0.6823895    9.78563204   1.50249762
   10.06186873   0.0595121    0.10849951  -0.53244889]]
Probabilities: [ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
  0.08456968  0.31669522]
Probability:  0.316695221806
Random chance:  0.125
[[ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
   0.08456968  0.31669522]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Intern
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03200000e+03   1.57111674e+01   1.58422590e+00   5.30000000e+02
    4.50000000e+01   6.24500000e+03   2.28222222e+01   5.16066212e-01
    4.96449332e+01]]
Scaled doc features:  [[  9.09715153   0.28795539   0.6926483    9.78563204   1.50249762
   10.05880268   0.0595121    0.10849951  -0.54015317]]
Probabilities: [ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
  0.08456968  0.31669523]
Probability:  0.316695229417
Random chance:  0.125
[[ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
   0.08456968  0.31669523]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Inter
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03100000e+03   1.57111674e+01   1.58422590e+00   5.30000000e+02
    4.50000000e+01   6.24400000e+03   2.28222222e+01   5.16066212e-01
    4.96449332e+01]]
Scaled doc features:  [[  9.09372963   0.28795539   0.6926483    9.78563204   1.50249762
   10.05573663   0.0595121    0.10849951  -0.54015317]]
Probabilities: [ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669524]
Probability:  0.316695238493
Random chance:  0.125
[[ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669524]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Inte
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03000000e+03   1.57111674e+01   1.58227848e+00   5.30000000e+02
    4.50000000e+01   6.24300000e+03   2.28222222e+01   5.16066212e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.09030772   0.28795539   0.6823895    9.78563204   1.50249762
   10.05267058   0.0595121    0.10849951  -0.53244889]]
Probabilities: [ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669525]
Probability:  0.316695249067
Random chance:  0.125
[[ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669525]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Int
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02900000e+03   1.57111674e+01   1.58227848e+00   5.30000000e+02
    4.50000000e+01   6.24200000e+03   2.28222222e+01   5.16066212e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.08688582   0.28795539   0.6823895    9.78563204   1.50249762
   10.04960452   0.0595121    0.10849951  -0.53244889]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669526]
Probability:  0.3166952583
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669526]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
In
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02800000e+03   1.57111674e+01   1.58325219e+00   5.29000000e+02
    4.50000000e+01   6.24100000e+03   2.28222222e+01   5.15092502e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08346392   0.28795539   0.6875189    9.74890783   1.50249762
   10.04653847   0.0595121    0.08796405  -0.53630103]]
Probabilities: [ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698054
  0.08456968  0.31669532]
Probability:  0.316695321359
Random chance:  0.125
[[ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698054
   0.08456968  0.31669532]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
I
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02700000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24000000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08004202   0.28795539   0.6875189    9.78563204   1.50249762
   10.04347242   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669528]
Probability:  0.316695276258
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669528]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>

This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02600000e+03   1.57111674e+01   1.58227848e+00   5.29000000e+02
    4.50000000e+01   6.23900000e+03   2.28222222e+01   5.15092502e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.07662012   0.28795539   0.6823895    9.74890783   1.50249762
   10.04040637   0.0595121    0.08796405  -0.53244889]]
Probabilities: [ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698053
  0.08456968  0.31669534]
Probability:  0.316695341881
Random chance:  0.125
[[ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698053
   0.08456968  0.31669534]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
T
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02700000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24000000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08004202   0.28795539   0.6875189    9.78563204   1.50249762
   10.04347242   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669528]
Probability:  0.316695276258
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669528]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Th
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02800000e+03   1.57111674e+01   1.58422590e+00   5.30000000e+02
    4.50000000e+01   6.24100000e+03   2.28222222e+01   5.16066212e-01
    4.96449332e+01]]
Scaled doc features:  [[  9.08346392   0.28795539   0.6926483    9.78563204   1.50249762
   10.04653847   0.0595121    0.10849951  -0.54015317]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669527]
Probability:  0.316695266154
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669527]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02900000e+03   1.57111674e+01   1.58325219e+00   5.29000000e+02
    4.50000000e+01   6.24200000e+03   2.28222222e+01   5.15092502e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08688582   0.28795539   0.6875189    9.74890783   1.50249762
   10.04960452   0.0595121    0.08796405  -0.53630103]]
Probabilities: [ 0.06527619  0.1205924   0.09105773  0.18878286  0.06604529  0.06698054
  0.08456968  0.31669531]
Probability:  0.316695311599
Random chance:  0.125
[[ 0.06527619  0.1205924   0.09105773  0.18878286  0.06604529  0.06698054
   0.08456968  0.31669531]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02900000e+03   1.57136533e+01   1.58171206e+00   5.29000000e+02
    4.50000000e+01   6.24300000e+03   2.28444444e+01   5.14591440e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.08688582   0.28846552   0.67940567   9.74890783   1.50249762
   10.05267058   0.0626539    0.07739667  -0.53126282]]
Probabilities: [ 0.06527619  0.1205924   0.09105773  0.18878286  0.06604529  0.06698054
  0.08456968  0.31669531]
Probability:  0.316695307319
Random chance:  0.125
[[ 0.06527619  0.1205924   0.09105773  0.18878286  0.06604529  0.06698054
   0.08456968  0.31669531]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The i
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03000000e+03   1.57136533e+01   1.58268482e+00   5.30000000e+02
    4.50000000e+01   6.24400000e+03   2.28444444e+01   5.15564202e-01
    4.97527527e+01]]
Scaled doc features:  [[  9.09030772   0.28846552   0.68453008   9.78563204   1.50249762
   10.05573663   0.0626539    0.09791216  -0.53511121]]
Probabilities: [ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
  0.08456968  0.31669524]
Probability:  0.316695243663
Random chance:  0.125
[[ 0.06527619  0.12059242  0.09105774  0.18878288  0.06604529  0.06698054
   0.08456968  0.31669524]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The in
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03100000e+03   1.57136533e+01   1.58268482e+00   5.29000000e+02
    4.50000000e+01   6.24500000e+03   2.28444444e+01   5.14591440e-01
    4.97527527e+01]]
Scaled doc features:  [[  9.09372963   0.28846552   0.68453008   9.74890783   1.50249762
   10.05880268   0.0626539    0.07739667  -0.53511121]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669529]
Probability:  0.316695287368
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669529]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The int
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03200000e+03   1.57136533e+01   1.58171206e+00   5.30000000e+02
    4.50000000e+01   6.24600000e+03   2.28444444e+01   5.15564202e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.09715153   0.28846552   0.67940567   9.78563204   1.50249762
   10.06186873   0.0626539    0.09791216  -0.53126282]]
Probabilities: [ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
  0.08456968  0.31669523]
Probability:  0.316695226191
Random chance:  0.125
[[ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
   0.08456968  0.31669523]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The inte
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03300000e+03   1.57136533e+01   1.58171206e+00   5.30000000e+02
    4.50000000e+01   6.24700000e+03   2.28444444e+01   5.15564202e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.10057343   0.28846552   0.67940567   9.78563204   1.50249762
   10.06493479   0.0626539    0.09791216  -0.53126282]]
Probabilities: [ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
  0.08456969  0.31669522]
Probability:  0.316695217213
Random chance:  0.125
[[ 0.0652762   0.12059242  0.09105775  0.18878289  0.06604529  0.06698054
   0.08456969  0.31669522]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The inter
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03400000e+03   1.57136533e+01   1.58365759e+00   5.30000000e+02
    4.50000000e+01   6.24800000e+03   2.28444444e+01   5.15564202e-01
    4.96704570e+01]]
Scaled doc features:  [[  9.10399533   0.28846552   0.68965449   9.78563204   1.50249762
   10.06800084   0.0626539    0.09791216  -0.53895961]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
  0.08456969  0.31669521]
Probability:  0.316695206943
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
   0.08456969  0.31669521]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The intern
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03500000e+03   1.57136533e+01   1.58365759e+00   5.30000000e+02
    4.50000000e+01   6.24900000e+03   2.28444444e+01   5.15564202e-01
    4.96704570e+01]]
Scaled doc features:  [[  9.10741723   0.28846552   0.68965449   9.78563204   1.50249762
   10.07106689   0.0626539    0.09791216  -0.53895961]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
  0.08456969  0.3166952 ]
Probability:  0.316695198118
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
   0.08456969  0.3166952 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The interne
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03600000e+03   1.57136533e+01   1.58171206e+00   5.30000000e+02
    4.50000000e+01   6.25000000e+03   2.28444444e+01   5.15564202e-01
    4.98350484e+01]]
Scaled doc features:  [[  9.11083913   0.28846552   0.67940567   9.78563204   1.50249762
   10.07413294   0.0626539    0.09791216  -0.53126282]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
  0.08456969  0.31669519]
Probability:  0.316695190703
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.1887829   0.06604529  0.06698055
   0.08456969  0.31669519]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03700000e+03   1.57525638e+01   1.58463035e+00   5.30000000e+02
    4.50000000e+01   6.25100000e+03   2.28444444e+01   5.15564202e-01
    4.95881613e+01]]
Scaled doc features:  [[  9.11426104   0.29645032   0.6947789    9.78563204   1.50249762
   10.077199     0.0626539    0.09791216  -0.542808  ]]
Probabilities: [ 0.0652762   0.12059243  0.09105775  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669518]
Probability:  0.316695179696
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105775  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669518]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03700000e+03   1.57550243e+01   1.58309038e+00   5.30000000e+02
    4.50000000e+01   6.25200000e+03   2.28666667e+01   5.15063168e-01
    4.96958873e+01]]
Scaled doc features:  [[  9.11426104   0.29695524   0.6866665    9.78563204   1.50249762
   10.08026505   0.06579571   0.08734539  -0.53777041]]
Probabilities: [ 0.0652762   0.12059243  0.09105776  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669518]
Probability:  0.31669517593
Random chance:  0.125
[[ 0.0652762   0.12059243  0.09105776  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669518]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet i
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03800000e+03   1.57550243e+01   1.58406220e+00   5.31000000e+02
    4.50000000e+01   6.25300000e+03   2.28666667e+01   5.16034985e-01
    4.96136715e+01]]
Scaled doc features:  [[  9.11768294   0.29695524   0.69178593   9.82235625   1.50249762
   10.0833311    0.06579571   0.10784094  -0.54161506]]
Probabilities: [ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
  0.08456969  0.31669512]
Probability:  0.316695119217
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
   0.08456969  0.31669512]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03900000e+03   1.57550243e+01   1.58406220e+00   5.30000000e+02
    4.50000000e+01   6.25400000e+03   2.28666667e+01   5.15063168e-01
    4.96136715e+01]]
Scaled doc features:  [[  9.12110484   0.29695524   0.69178593   9.78563204   1.50249762
   10.08639715   0.06579571   0.08734539  -0.54161506]]
Probabilities: [ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669516]
Probability:  0.316695158183
Random chance:  0.125
[[ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669516]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03900000e+03   1.57574973e+01   1.58252427e+00   5.30000000e+02
    4.50000000e+01   6.25500000e+03   2.28888889e+01   5.14563107e-01
    4.97212244e+01]]
Scaled doc features:  [[  9.12110484   0.29746272   0.6836843    9.78563204   1.50249762
   10.08946321   0.06893752   0.07679913  -0.53658557]]
Probabilities: [ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
  0.08456969  0.31669515]
Probability:  0.316695154457
Random chance:  0.125
[[ 0.0652762   0.12059244  0.09105776  0.18878291  0.06604529  0.06698055
   0.08456969  0.31669515]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is l
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04000000e+03   1.57574973e+01   1.58349515e+00   5.31000000e+02
    4.50000000e+01   6.25600000e+03   2.28888889e+01   5.15533981e-01
    4.96390885e+01]]
Scaled doc features:  [[  9.12452674   0.29746272   0.68879876   9.82235625   1.50249762
   10.09252926   0.06893752   0.09727478  -0.54042649]]
Probabilities: [ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
  0.08456969  0.3166951 ]
Probability:  0.316695098987
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
   0.08456969  0.3166951 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is li
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04100000e+03   1.57574973e+01   1.58349515e+00   5.31000000e+02
    4.50000000e+01   6.25700000e+03   2.28888889e+01   5.15533981e-01
    4.96390885e+01]]
Scaled doc features:  [[  9.12794864   0.29746272   0.68879876   9.82235625   1.50249762
   10.09559531   0.06893752   0.09727478  -0.54042649]]
Probabilities: [ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
  0.08456969  0.31669509]
Probability:  0.316695091047
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105777  0.18878294  0.0660453   0.06698055
   0.08456969  0.31669509]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is lik
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04200000e+03   1.57574973e+01   1.58252427e+00   5.31000000e+02
    4.50000000e+01   6.25800000e+03   2.28888889e+01   5.15533981e-01
    4.97212244e+01]]
Scaled doc features:  [[  9.13137055   0.29746272   0.6836843    9.82235625   1.50249762
   10.09866136   0.06893752   0.09727478  -0.53658557]]
Probabilities: [ 0.0652762   0.12059246  0.09105777  0.18878294  0.0660453   0.06698055
  0.08456969  0.31669508]
Probability:  0.316695083774
Random chance:  0.125
[[ 0.0652762   0.12059246  0.09105777  0.18878294  0.0660453   0.06698055
   0.08456969  0.31669508]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04300000e+03   1.57574973e+01   1.58349515e+00   5.30000000e+02
    4.50000000e+01   6.25900000e+03   2.28888889e+01   5.14563107e-01
    4.96390885e+01]]
Scaled doc features:  [[  9.13479245   0.29746272   0.68879876   9.78563204   1.50249762
   10.10172742   0.06893752   0.07679913  -0.54042649]]
Probabilities: [ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
  0.08456969  0.31669512]
Probability:  0.316695120598
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
   0.08456969  0.31669512]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04300000e+03   1.57599828e+01   1.58195926e+00   5.30000000e+02
    4.50000000e+01   6.26000000e+03   2.29111111e+01   5.14064016e-01
    4.97464686e+01]]
Scaled doc features:  [[  9.13479245   0.29797276   0.6807079    9.78563204   1.50249762
   10.10479347   0.07207933   0.06627334  -0.53540507]]
Probabilities: [ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
  0.08456969  0.31669512]
Probability:  0.316695116971
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105776  0.18878293  0.0660453   0.06698055
   0.08456969  0.31669512]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04400000e+03   1.57599828e+01   1.58292919e+00   5.30000000e+02
    4.50000000e+01   6.26100000e+03   2.29111111e+01   5.14064016e-01
    4.96644123e+01]]
Scaled doc features:  [[  9.13821435   0.29797276   0.6858174    9.78563204   1.50249762
   10.10785952   0.07207933   0.06627334  -0.53924227]]
Probabilities: [ 0.0652762   0.12059245  0.09105777  0.18878293  0.0660453   0.06698055
  0.08456969  0.31669511]
Probability:  0.316695108247
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105777  0.18878293  0.0660453   0.06698055
   0.08456969  0.31669511]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04400000e+03   1.57624806e+01   1.58139535e+00   5.30000000e+02
    4.50000000e+01   6.26200000e+03   2.29333333e+01   5.13565891e-01
    4.97716202e+01]]
Scaled doc features:  [[  9.13821435   0.29848535   0.67773726   9.78563204   1.50249762
   10.11092557   0.07522113   0.05576794  -0.53422891]]
Probabilities: [ 0.0652762   0.12059245  0.09105777  0.18878293  0.0660453   0.06698055
  0.08456969  0.3166951 ]
Probability:  0.316695104631
Random chance:  0.125
[[ 0.0652762   0.12059245  0.09105777  0.18878293  0.0660453   0.06698055
   0.08456969  0.3166951 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a b
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04500000e+03   1.57624806e+01   1.58236434e+00   5.31000000e+02
    4.50000000e+01   6.26300000e+03   2.29333333e+01   5.14534884e-01
    4.96896434e+01]]
Scaled doc features:  [[  9.14163625   0.29848535   0.68284181   9.82235625   1.50249762
   10.11399163   0.07522113   0.07620391  -0.53806239]]
Probabilities: [ 0.06527621  0.12059246  0.09105778  0.18878295  0.0660453   0.06698056
  0.0845697   0.31669505]
Probability:  0.316695052022
Random chance:  0.125
[[ 0.06527621  0.12059246  0.09105778  0.18878295  0.0660453   0.06698056
   0.0845697   0.31669505]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a bi
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04600000e+03   1.57624806e+01   1.58236434e+00   5.31000000e+02
    4.50000000e+01   6.26400000e+03   2.29333333e+01   5.14534884e-01
    4.96896434e+01]]
Scaled doc features:  [[  9.14505815   0.29848535   0.68284181   9.82235625   1.50249762
   10.11705768   0.07522113   0.07620391  -0.53806239]]
Probabilities: [ 0.06527621  0.12059246  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669504]
Probability:  0.316695044457
Random chance:  0.125
[[ 0.06527621  0.12059246  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669504]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04700000e+03   1.57624806e+01   1.58236434e+00   5.31000000e+02
    4.50000000e+01   6.26500000e+03   2.29333333e+01   5.14534884e-01
    4.96896434e+01]]
Scaled doc features:  [[  9.14848005   0.29848535   0.68284181   9.82235625   1.50249762
   10.12012373   0.07522113   0.07620391  -0.53806239]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669504]
Probability:  0.316695036952
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669504]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a bigg
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04800000e+03   1.57624806e+01   1.58139535e+00   5.31000000e+02
    4.50000000e+01   6.26600000e+03   2.29333333e+01   5.14534884e-01
    4.97716202e+01]]
Scaled doc features:  [[  9.15190196   0.29848535   0.67773726   9.82235625   1.50249762
   10.12318978   0.07522113   0.07620391  -0.53422891]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669503]
Probability:  0.316695030067
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669503]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04700000e+03   1.57624806e+01   1.58236434e+00   5.31000000e+02
    4.50000000e+01   6.26500000e+03   2.29333333e+01   5.14534884e-01
    4.96896434e+01]]
Scaled doc features:  [[  9.14848005   0.29848535   0.68284181   9.82235625   1.50249762
   10.12012373   0.07522113   0.07620391  -0.53806239]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669504]
Probability:  0.316695036952
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669504]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04700000e+03   1.57649909e+01   1.58083253e+00   5.31000000e+02
    4.50000000e+01   6.26600000e+03   2.29555556e+01   5.14036786e-01
    4.97966794e+01]]
Scaled doc features:  [[  9.14848005   0.29900047   0.67477237   9.82235625   1.50249762
   10.12318978   0.07836294   0.06569907  -0.53305707]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669503]
Probability:  0.316695033613
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669503]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big t
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04800000e+03   1.57649909e+01   1.58180058e+00   5.32000000e+02
    4.50000000e+01   6.26700000e+03   2.29555556e+01   5.15004840e-01
    4.97147820e+01]]
Scaled doc features:  [[  9.15190196   0.29900047   0.67987198   9.85908047   1.50249762
   10.12625583   0.07836294   0.08611526  -0.53688684]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669498]
Probability:  0.316694984689
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669498]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big tr
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04900000e+03   1.57649909e+01   1.58083253e+00   5.32000000e+02
    4.50000000e+01   6.26800000e+03   2.29555556e+01   5.15004840e-01
    4.97966794e+01]]
Scaled doc features:  [[  9.15532386   0.29900047   0.67477237   9.85908047   1.50249762
   10.12932189   0.07836294   0.08611526  -0.53305707]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669498]
Probability:  0.316694978208
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669498]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big tra
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05000000e+03   1.57649909e+01   1.58083253e+00   5.32000000e+02
    4.50000000e+01   6.26900000e+03   2.29555556e+01   5.15004840e-01
    4.97966794e+01]]
Scaled doc features:  [[  9.15874576   0.29900047   0.67477237   9.85908047   1.50249762
   10.13238794   0.07836294   0.08611526  -0.53305707]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669497]
Probability:  0.316694971255
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669497]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traf
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05100000e+03   1.57649909e+01   1.58083253e+00   5.32000000e+02
    4.50000000e+01   6.27000000e+03   2.29555556e+01   5.15004840e-01
    4.97966794e+01]]
Scaled doc features:  [[  9.16216766   0.29900047   0.67477237   9.85908047   1.50249762
   10.13545399   0.07836294   0.08611526  -0.53305707]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669496]
Probability:  0.316694964358
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669496]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traff
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05200000e+03   1.57649909e+01   1.58083253e+00   5.32000000e+02
    4.50000000e+01   6.27100000e+03   2.29555556e+01   5.15004840e-01
    4.97966794e+01]]
Scaled doc features:  [[  9.16558956   0.29900047   0.67477237   9.85908047   1.50249762
   10.13852004   0.07836294   0.08611526  -0.53305707]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669496]
Probability:  0.316694957517
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669496]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffi
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05300000e+03   1.57649909e+01   1.58083253e+00   5.32000000e+02
    4.50000000e+01   6.27200000e+03   2.29555556e+01   5.15004840e-01
    4.97966794e+01]]
Scaled doc features:  [[  9.16901147   0.29900047   0.67477237   9.85908047   1.50249762
   10.1415861    0.07836294   0.08611526  -0.53305707]]
Probabilities: [ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698057
  0.0845697   0.31669495]
Probability:  0.31669495073
Random chance:  0.125
[[ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698057
   0.0845697   0.31669495]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05400000e+03   1.57649909e+01   1.58276864e+00   5.31000000e+02
    4.50000000e+01   6.27300000e+03   2.29555556e+01   5.14036786e-01
    4.96328846e+01]]
Scaled doc features:  [[  9.17243337   0.29900047   0.68497158   9.82235625   1.50249762
   10.14465215   0.07836294   0.06569907  -0.5407166 ]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669498]
Probability:  0.31669498186
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669498]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic 
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05400000e+03   1.57675134e+01   1.58123791e+00   5.31000000e+02
    4.50000000e+01   6.27400000e+03   2.29777778e+01   5.13539652e-01
    4.97398283e+01]]
Scaled doc features:  [[  9.17243337   0.29951813   0.67690789   9.82235625   1.50249762
   10.1477182    0.08150475   0.05521455  -0.5357156 ]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
  0.0845697   0.31669498]
Probability:  0.316694978702
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878298  0.0660453   0.06698056
   0.0845697   0.31669498]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic j
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05500000e+03   1.57675134e+01   1.58220503e+00   5.32000000e+02
    4.50000000e+01   6.27500000e+03   2.29777778e+01   5.14506770e-01
    4.96580101e+01]]
Scaled doc features:  [[  9.17585527   0.29951813   0.68200257   9.85908047   1.50249762
   10.15078425   0.08150475   0.07561099  -0.53954166]]
Probabilities: [ 0.06527621  0.12059249  0.0910578   0.188783    0.0660453   0.06698057
  0.0845697   0.31669493]
Probability:  0.316694932878
Random chance:  0.125
[[ 0.06527621  0.12059249  0.0910578   0.188783    0.0660453   0.06698057
   0.0845697   0.31669493]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic ja
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05600000e+03   1.57675134e+01   1.58220503e+00   5.32000000e+02
    4.50000000e+01   6.27600000e+03   2.29777778e+01   5.14506770e-01
    4.96580101e+01]]
Scaled doc features:  [[  9.17927717   0.29951813   0.68200257   9.85908047   1.50249762
   10.15385031   0.08150475   0.07561099  -0.53954166]]
Probabilities: [ 0.06527621  0.12059249  0.0910578   0.188783    0.0660453   0.06698057
  0.0845697   0.31669493]
Probability:  0.31669492629
Random chance:  0.125
[[ 0.06527621  0.12059249  0.0910578   0.188783    0.0660453   0.06698057
   0.0845697   0.31669493]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05700000e+03   1.57675134e+01   1.58220503e+00   5.31000000e+02
    4.50000000e+01   6.27700000e+03   2.29777778e+01   5.13539652e-01
    4.96580101e+01]]
Scaled doc features:  [[  9.18269907   0.29951813   0.68200257   9.82235625   1.50249762
   10.15691636   0.08150475   0.05521455  -0.53954166]]
Probabilities: [ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
  0.0845697   0.31669496]
Probability:  0.316694957426
Random chance:  0.125
[[ 0.06527621  0.12059248  0.09105779  0.18878299  0.0660453   0.06698056
   0.0845697   0.31669496]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam.
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05700000e+03   1.55677067e+01   1.58220503e+00   5.31000000e+02
    4.60000000e+01   6.27800000e+03   2.24782609e+01   5.13539652e-01
    5.01650198e+01]]
Scaled doc features:  [[  9.18269907   0.25851591   0.68200257   9.82235625   1.58387784
   10.15998241   0.01088238   0.05521455  -0.51583237]]
Probabilities: [ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698057
  0.0845697   0.31669494]
Probability:  0.316694940008
Random chance:  0.125
[[ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698057
   0.0845697   0.31669494]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam.

With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.55592322e+01   1.58242843e+00   5.25000000e+02
    4.50000000e+01   6.15700000e+03   2.25111111e+01   5.18262586e-01
    5.01127770e+01]]
Scaled doc features:  [[ 8.84735269  0.25677687  0.68317942  9.60201099  1.50249762  9.78899007
   0.01552679  0.15482087 -0.5182754 ]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669682]
Probability:  0.316696823368
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669682]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.55568204e+01   1.58399209e+00   5.25000000e+02
    4.50000000e+01   6.15600000e+03   2.24888889e+01   5.18774704e-01
    5.00030466e+01]]
Scaled doc features:  [[ 8.84735269  0.25628194  0.69141664  9.60201099  1.50249762  9.78592402
   0.01238498  0.16562138 -0.52340672]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669683]
Probability:  0.316696832268
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669683]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam. 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.55592322e+01   1.58242843e+00   5.25000000e+02
    4.50000000e+01   6.15700000e+03   2.25111111e+01   5.18262586e-01
    5.01127770e+01]]
Scaled doc features:  [[ 8.84735269  0.25677687  0.68317942  9.60201099  1.50249762  9.78899007
   0.01552679  0.15482087 -0.5182754 ]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669682]
Probability:  0.316696823368
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669682]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam. o
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96000000e+03   1.55592322e+01   1.58341560e+00   5.26000000e+02
    4.50000000e+01   6.15800000e+03   2.25111111e+01   5.19249753e-01
    5.00292627e+01]]
Scaled doc features:  [[ 8.8507746   0.25677687  0.68837971  9.6387352   1.50249762  9.79205612
   0.01552679  0.17564014 -0.52218078]]
Probabilities: [ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
  0.08456959  0.31669668]
Probability:  0.31669667823
Random chance:  0.125
[[ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
   0.08456959  0.31669668]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam. 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.55592322e+01   1.58242843e+00   5.25000000e+02
    4.50000000e+01   6.15700000e+03   2.25111111e+01   5.18262586e-01
    5.01127770e+01]]
Scaled doc features:  [[ 8.84735269  0.25677687  0.68317942  9.60201099  1.50249762  9.78899007
   0.01552679  0.15482087 -0.5182754 ]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669682]
Probability:  0.316696823368
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669682]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.55568204e+01   1.58399209e+00   5.25000000e+02
    4.50000000e+01   6.15600000e+03   2.24888889e+01   5.18774704e-01
    5.00030466e+01]]
Scaled doc features:  [[ 8.84735269  0.25628194  0.69141664  9.60201099  1.50249762  9.78592402
   0.01238498  0.16562138 -0.52340672]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669683]
Probability:  0.316696832268
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669683]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.57612648e+01   1.58399209e+00   5.25000000e+02
    4.40000000e+01   6.15500000e+03   2.30000000e+01   5.18774704e-01
    4.94842688e+01]]
Scaled doc features:  [[ 8.84735269  0.29823585  0.69141664  9.60201099  1.42111739  9.78285797
   0.08464656  0.16562138 -0.54766632]]
Probabilities: [ 0.0652761   0.12059204  0.09105747  0.18878227  0.06604525  0.06698042
  0.08456957  0.31669689]
Probability:  0.316696887111
Random chance:  0.125
[[ 0.0652761   0.12059204  0.09105747  0.18878227  0.06604525  0.06698042
   0.08456957  0.31669689]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.57638787e+01   1.58242843e+00   5.25000000e+02
    4.40000000e+01   6.15600000e+03   2.30227273e+01   5.18262586e-01
    4.95934866e+01]]
Scaled doc features:  [[ 8.84735269  0.29877224  0.68317942  9.60201099  1.42111739  9.78592402
   0.08785977  0.15482087 -0.54255897]]
Probabilities: [ 0.0652761   0.12059204  0.09105747  0.18878227  0.06604525  0.06698042
  0.08456957  0.31669688]
Probability:  0.316696877948
Random chance:  0.125
[[ 0.0652761   0.12059204  0.09105747  0.18878227  0.06604525  0.06698042
   0.08456957  0.31669688]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam o
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96000000e+03   1.57638787e+01   1.58341560e+00   5.26000000e+02
    4.40000000e+01   6.15700000e+03   2.30227273e+01   5.19249753e-01
    4.95099723e+01]]
Scaled doc features:  [[ 8.8507746   0.29877224  0.68837971  9.6387352   1.42111739  9.78899007
   0.08785977  0.17564014 -0.54646435]]
Probabilities: [ 0.06527611  0.12059207  0.09105749  0.18878233  0.06604526  0.06698043
  0.08456958  0.31669673]
Probability:  0.316696729781
Random chance:  0.125
[[ 0.06527611  0.12059207  0.09105749  0.18878233  0.06604526  0.06698043
   0.08456958  0.31669673]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96100000e+03   1.57638787e+01   1.58341560e+00   5.25000000e+02
    4.40000000e+01   6.15800000e+03   2.30227273e+01   5.18262586e-01
    4.95099723e+01]]
Scaled doc features:  [[ 8.8541965   0.29877224  0.68837971  9.60201099  1.42111739  9.79205612
   0.08785977  0.15482087 -0.54646435]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669683]
Probability:  0.316696832432
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669683]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96100000e+03   1.57665053e+01   1.58185404e+00   5.25000000e+02
    4.40000000e+01   6.15900000e+03   2.30454545e+01   5.17751479e-01
    4.96190116e+01]]
Scaled doc features:  [[ 8.8541965   0.29931125  0.68015361  9.60201099  1.42111739  9.79512218
   0.09107298  0.14404166 -0.54136535]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669682]
Probability:  0.316696823341
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669682]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of i
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96200000e+03   1.57665053e+01   1.58284024e+00   5.26000000e+02
    4.40000000e+01   6.16000000e+03   2.30454545e+01   5.18737673e-01
    4.95355796e+01]]
Scaled doc features:  [[ 8.8576184   0.29931125  0.68534877  9.6387352   1.42111739  9.79818823
   0.09107298  0.1648404  -0.54526687]]
Probabilities: [ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
  0.08456959  0.31669668]
Probability:  0.316696678266
Random chance:  0.125
[[ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
   0.08456959  0.31669668]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96100000e+03   1.57665053e+01   1.58185404e+00   5.25000000e+02
    4.40000000e+01   6.15900000e+03   2.30454545e+01   5.17751479e-01
    4.96190116e+01]]
Scaled doc features:  [[ 8.8541965   0.29931125  0.68015361  9.60201099  1.42111739  9.79512218
   0.09107298  0.14404166 -0.54136535]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669682]
Probability:  0.316696823341
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669682]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of b
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96200000e+03   1.57665053e+01   1.58284024e+00   5.26000000e+02
    4.40000000e+01   6.16000000e+03   2.30454545e+01   5.18737673e-01
    4.95355796e+01]]
Scaled doc features:  [[ 8.8576184   0.29931125  0.68534877  9.6387352   1.42111739  9.79818823
   0.09107298  0.1648404  -0.54526687]]
Probabilities: [ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
  0.08456959  0.31669668]
Probability:  0.316696678266
Random chance:  0.125
[[ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
   0.08456959  0.31669668]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bi
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96300000e+03   1.57665053e+01   1.58284024e+00   5.26000000e+02
    4.40000000e+01   6.16100000e+03   2.30454545e+01   5.18737673e-01
    4.95355796e+01]]
Scaled doc features:  [[ 8.8610403   0.29931125  0.68534877  9.6387352   1.42111739  9.80125428
   0.09107298  0.1648404  -0.54526687]]
Probabilities: [ 0.06527611  0.12059209  0.09105751  0.18878235  0.06604526  0.06698043
  0.08456959  0.31669666]
Probability:  0.316696657898
Random chance:  0.125
[[ 0.06527611  0.12059209  0.09105751  0.18878235  0.06604526  0.06698043
   0.08456959  0.31669666]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bit
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96400000e+03   1.57665053e+01   1.58284024e+00   5.26000000e+02
    4.40000000e+01   6.16200000e+03   2.30454545e+01   5.18737673e-01
    4.95355796e+01]]
Scaled doc features:  [[ 8.8644622   0.29931125  0.68534877  9.6387352   1.42111739  9.80432033
   0.09107298  0.1648404  -0.54526687]]
Probabilities: [ 0.06527611  0.1205921   0.09105751  0.18878236  0.06604526  0.06698043
  0.08456959  0.31669664]
Probability:  0.316696637684
Random chance:  0.125
[[ 0.06527611  0.1205921   0.09105751  0.18878236  0.06604526  0.06698043
   0.08456959  0.31669664]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96500000e+03   1.57665053e+01   1.58284024e+00   5.26000000e+02
    4.40000000e+01   6.16300000e+03   2.30454545e+01   5.18737673e-01
    4.95355796e+01]]
Scaled doc features:  [[ 8.86788411  0.29931125  0.68534877  9.6387352   1.42111739  9.80738639
   0.09107298  0.1648404  -0.54526687]]
Probabilities: [ 0.06527611  0.1205921   0.09105751  0.18878237  0.06604526  0.06698044
  0.08456959  0.31669662]
Probability:  0.316696617623
Random chance:  0.125
[[ 0.06527611  0.1205921   0.09105751  0.18878237  0.06604526  0.06698044
   0.08456959  0.31669662]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96500000e+03   1.57691446e+01   1.58128079e+00   5.26000000e+02
    4.40000000e+01   6.16400000e+03   2.30681818e+01   5.18226601e-01
    4.96444408e+01]]
Scaled doc features:  [[ 8.86788411  0.29985287  0.67713376  9.6387352   1.42111739  9.81045244
   0.09428619  0.15406194 -0.5401762 ]]
Probabilities: [ 0.06527611  0.1205921   0.09105751  0.18878237  0.06604526  0.06698044
  0.08456959  0.31669661]
Probability:  0.316696609271
Random chance:  0.125
[[ 0.06527611  0.1205921   0.09105751  0.18878237  0.06604526  0.06698044
   0.08456959  0.31669661]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits a
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96600000e+03   1.57691446e+01   1.58226601e+00   5.26000000e+02
    4.40000000e+01   6.16500000e+03   2.30681818e+01   5.18226601e-01
    4.95610910e+01]]
Scaled doc features:  [[ 8.87130601  0.29985287  0.68232381  9.6387352   1.42111739  9.81351849
   0.09428619  0.15406194 -0.54407388]]
Probabilities: [ 0.06527612  0.12059211  0.09105752  0.18878238  0.06604526  0.06698044
  0.08456959  0.31669659]
Probability:  0.316696587818
Random chance:  0.125
[[ 0.06527612  0.12059211  0.09105752  0.18878238  0.06604526  0.06698044
   0.08456959  0.31669659]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits an
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96700000e+03   1.57691446e+01   1.58226601e+00   5.26000000e+02
    4.40000000e+01   6.16600000e+03   2.30681818e+01   5.18226601e-01
    4.95610910e+01]]
Scaled doc features:  [[ 8.87472791  0.29985287  0.68232381  9.6387352   1.42111739  9.81658454
   0.09428619  0.15406194 -0.54407388]]
Probabilities: [ 0.06527612  0.12059211  0.09105752  0.18878239  0.06604526  0.06698044
  0.08456959  0.31669657]
Probability:  0.316696568134
Random chance:  0.125
[[ 0.06527612  0.12059211  0.09105752  0.18878239  0.06604526  0.06698044
   0.08456959  0.31669657]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96800000e+03   1.57691446e+01   1.58226601e+00   5.26000000e+02
    4.40000000e+01   6.16700000e+03   2.30681818e+01   5.18226601e-01
    4.95610910e+01]]
Scaled doc features:  [[ 8.87814981  0.29985287  0.68232381  9.6387352   1.42111739  9.8196506
   0.09428619  0.15406194 -0.54407388]]
Probabilities: [ 0.06527612  0.12059212  0.09105753  0.1887824   0.06604526  0.06698044
  0.0845696   0.31669655]
Probability:  0.316696548598
Random chance:  0.125
[[ 0.06527612  0.12059212  0.09105753  0.1887824   0.06604526  0.06698044
   0.0845696   0.31669655]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96800000e+03   1.57717967e+01   1.58070866e+00   5.26000000e+02
    4.40000000e+01   6.16800000e+03   2.30909091e+01   5.17716535e-01
    4.96697745e+01]]
Scaled doc features:  [[ 8.87814981  0.3003971   0.67411986  9.6387352   1.42111739  9.82271665
   0.09749941  0.14330469 -0.53899152]]
Probabilities: [ 0.06527612  0.12059212  0.09105753  0.1887824   0.06604526  0.06698044
  0.0845696   0.31669654]
Probability:  0.316696540377
Random chance:  0.125
[[ 0.06527612  0.12059212  0.09105753  0.1887824   0.06604526  0.06698044
   0.0845696   0.31669654]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and b
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96900000e+03   1.57717967e+01   1.58169291e+00   5.27000000e+02
    4.40000000e+01   6.16900000e+03   2.30909091e+01   5.18700787e-01
    4.95865068e+01]]
Scaled doc features:  [[ 8.88157171  0.3003971   0.6793048   9.67545941  1.42111739  9.8257827
   0.09749941  0.16406249 -0.54288537]]
Probabilities: [ 0.06527613  0.12059215  0.09105755  0.18878245  0.06604526  0.06698045
  0.08456961  0.31669641]
Probability:  0.316696410023
Random chance:  0.125
[[ 0.06527613  0.12059215  0.09105755  0.18878245  0.06604526  0.06698045
   0.08456961  0.31669641]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and by
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.97000000e+03   1.57717967e+01   1.58169291e+00   5.26000000e+02
    4.40000000e+01   6.17000000e+03   2.30909091e+01   5.17716535e-01
    4.95865068e+01]]
Scaled doc features:  [[ 8.88499361  0.3003971   0.6793048   9.6387352   1.42111739  9.82884875
   0.09749941  0.14330469 -0.54288537]]
Probabilities: [ 0.06527612  0.12059213  0.09105753  0.18878241  0.06604526  0.06698045
  0.0845696   0.3166965 ]
Probability:  0.316696500336
Random chance:  0.125
[[ 0.06527612  0.12059213  0.09105753  0.18878241  0.06604526  0.06698045
   0.0845696   0.3166965 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and byt
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.97100000e+03   1.57717967e+01   1.58070866e+00   5.27000000e+02
    4.40000000e+01   6.17100000e+03   2.30909091e+01   5.18700787e-01
    4.96697745e+01]]
Scaled doc features:  [[ 8.88841552  0.3003971   0.67411986  9.67545941  1.42111739  9.8319148
   0.09749941  0.16406249 -0.53899152]]
Probabilities: [ 0.06527613  0.12059216  0.09105755  0.18878246  0.06604526  0.06698046
  0.08456961  0.31669638]
Probability:  0.316696375063
Random chance:  0.125
[[ 0.06527613  0.12059216  0.09105755  0.18878246  0.06604526  0.06698046
   0.08456961  0.31669638]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and byte
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.97200000e+03   1.57717967e+01   1.58169291e+00   5.27000000e+02
    4.40000000e+01   6.17200000e+03   2.30909091e+01   5.18700787e-01
    4.95865068e+01]]
Scaled doc features:  [[ 8.89183742  0.3003971   0.6793048   9.67545941  1.42111739  9.83498086
   0.09749941  0.16406249 -0.54288537]]
Probabilities: [ 0.06527613  0.12059216  0.09105756  0.18878247  0.06604527  0.06698046
  0.08456961  0.31669636]
Probability:  0.316696355612
Random chance:  0.125
[[ 0.06527613  0.12059216  0.09105756  0.18878247  0.06604527  0.06698046
   0.08456961  0.31669636]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.97300000e+03   1.57717967e+01   1.58169291e+00   5.26000000e+02
    4.40000000e+01   6.17300000e+03   2.30909091e+01   5.17716535e-01
    4.95865068e+01]]
Scaled doc features:  [[ 8.89525932  0.3003971   0.6793048   9.6387352   1.42111739  9.83804691
   0.09749941  0.14330469 -0.54288537]]
Probabilities: [ 0.06527612  0.12059214  0.09105754  0.18878243  0.06604526  0.06698045
  0.0845696   0.31669644]
Probability:  0.316696443703
Random chance:  0.125
[[ 0.06527612  0.12059214  0.09105754  0.18878243  0.06604526  0.06698045
   0.0845696   0.31669644]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.97300000e+03   1.55665442e+01   1.58169291e+00   5.26000000e+02
    4.50000000e+01   6.17400000e+03   2.25777778e+01   5.17716535e-01
    5.01073351e+01]]
Scaled doc features:  [[ 8.89525932  0.25827736  0.6793048   9.6387352   1.50249762  9.84111296
   0.02495221  0.14330469 -0.51852988]]
Probabilities: [ 0.06527613  0.12059215  0.09105755  0.18878245  0.06604526  0.06698045
  0.08456961  0.3166964 ]
Probability:  0.316696398182
Random chance:  0.125
[[ 0.06527613  0.12059215  0.09105755  0.18878245  0.06604526  0.06698045
   0.08456961  0.3166964 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
T
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83400000e+03   1.55810085e+01   1.58400810e+00   5.17000000e+02
    4.40000000e+01   6.00400000e+03   2.24545455e+01   5.23279352e-01
    5.00365513e+01]]
Scaled doc features:  [[  8.41961497e+00   2.61245566e-01   6.91500938e-01   9.30821729e+00
    1.42111739e+00   9.31988405e+00   7.52946334e-03   2.60624061e-01
   -5.21839936e-01]]
Probabilities: [ 0.06527557  0.12058998  0.09105598  0.18877895  0.06604503  0.06697973
  0.08456897  0.3167058 ]
Probability:  0.316705799505
Random chance:  0.125
[[ 0.06527557  0.12058998  0.09105598  0.18877895  0.06604503  0.06697973
   0.08456897  0.3167058 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
Th
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83500000e+03   1.55810085e+01   1.58502024e+00   5.17000000e+02
    4.40000000e+01   6.00500000e+03   2.24545455e+01   5.23279352e-01
    4.99509238e+01]]
Scaled doc features:  [[  8.42303687e+00   2.61245566e-01   6.96832813e-01   9.30821729e+00
    1.42111739e+00   9.32295011e+00   7.52946334e-03   2.60624061e-01
   -5.25844136e-01]]
Probabilities: [ 0.06527557  0.12059     0.09105599  0.18877898  0.06604503  0.06697974
  0.08456898  0.3167057 ]
Probability:  0.316705704885
Random chance:  0.125
[[ 0.06527557  0.12059     0.09105599  0.18877898  0.06604503  0.06697974
   0.08456898  0.3167057 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
The
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83600000e+03   1.55810085e+01   1.58400810e+00   5.16000000e+02
    4.40000000e+01   6.00600000e+03   2.24545455e+01   5.22267206e-01
    5.00365513e+01]]
Scaled doc features:  [[  8.42645877e+00   2.61245566e-01   6.91500938e-01   9.27149308e+00
    1.42111739e+00   9.32601616e+00   7.52946334e-03   2.39277987e-01
   -5.21839936e-01]]
Probabilities: [ 0.06527555  0.1205899   0.09105592  0.18877881  0.06604502  0.0669797
  0.08456895  0.31670616]
Probability:  0.316706155367
Random chance:  0.125
[[ 0.06527555  0.1205899   0.09105592  0.18877881  0.06604502  0.0669797
   0.08456895  0.31670616]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
Ther
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83700000e+03   1.55810085e+01   1.58299595e+00   5.17000000e+02
    4.40000000e+01   6.00700000e+03   2.24545455e+01   5.23279352e-01
    5.01221789e+01]]
Scaled doc features:  [[  8.42988067e+00   2.61245566e-01   6.86169062e-01   9.30821729e+00
    1.42111739e+00   9.32908221e+00   7.52946334e-03   2.60624061e-01
   -5.17835736e-01]]
Probabilities: [ 0.06527558  0.12059004  0.09105602  0.18877904  0.06604503  0.06697975
  0.08456899  0.31670555]
Probability:  0.316705548637
Random chance:  0.125
[[ 0.06527558  0.12059004  0.09105602  0.18877904  0.06604503  0.06697975
   0.08456899  0.31670555]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83800000e+03   1.55810085e+01   1.58400810e+00   5.17000000e+02
    4.40000000e+01   6.00800000e+03   2.24545455e+01   5.23279352e-01
    5.00365513e+01]]
Scaled doc features:  [[  8.43330257e+00   2.61245566e-01   6.91500938e-01   9.30821729e+00
    1.42111739e+00   9.33214826e+00   7.52946334e-03   2.60624061e-01
   -5.21839936e-01]]
Probabilities: [ 0.06527559  0.12059006  0.09105604  0.18877907  0.06604504  0.06697976
  0.084569    0.31670546]
Probability:  0.316705456011
Random chance:  0.125
[[ 0.06527559  0.12059006  0.09105604  0.18877907  0.06604504  0.06697976
   0.084569    0.31670546]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There 
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83800000e+03   1.55834268e+01   1.58240647e+00   5.17000000e+02
    4.40000000e+01   6.00900000e+03   2.24772727e+01   5.22750253e-01
    5.01489807e+01]]
Scaled doc features:  [[ 8.43330257  0.26174183  0.68306374  9.30821729  1.42111739  9.33521432
   0.01074268  0.2494654  -0.5165824 ]]
Probabilities: [ 0.06527559  0.12059007  0.09105604  0.18877909  0.06604504  0.06697976
  0.084569    0.31670542]
Probability:  0.31670542457
Random chance:  0.125
[[ 0.06527559  0.12059007  0.09105604  0.18877909  0.06604504  0.06697976
   0.084569    0.31670542]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There a
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83900000e+03   1.55834268e+01   1.58341759e+00   5.17000000e+02
    4.40000000e+01   6.01000000e+03   2.24772727e+01   5.22750253e-01
    5.00634398e+01]]
Scaled doc features:  [[ 8.43672448  0.26174183  0.68839023  9.30821729  1.42111739  9.33828037
   0.01074268  0.2494654  -0.52058255]]
Probabilities: [ 0.0652756   0.12059009  0.09105606  0.18877912  0.06604504  0.06697977
  0.084569    0.31670533]
Probability:  0.316705332939
Random chance:  0.125
[[ 0.0652756   0.12059009  0.09105606  0.18877912  0.06604504  0.06697977
   0.084569    0.31670533]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There ar
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84000000e+03   1.55834268e+01   1.58341759e+00   5.18000000e+02
    4.40000000e+01   6.01100000e+03   2.24772727e+01   5.23761375e-01
    5.00634398e+01]]
Scaled doc features:  [[ 8.44014638  0.26174183  0.68839023  9.34494151  1.42111739  9.34134642
   0.01074268  0.27078989 -0.52058255]]
Probabilities: [ 0.06527563  0.12059022  0.09105615  0.18877933  0.06604505  0.06697981
  0.08456904  0.31670476]
Probability:  0.316704756887
Random chance:  0.125
[[ 0.06527563  0.12059022  0.09105615  0.18877933  0.06604505  0.06697981
   0.08456904  0.31670476]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84100000e+03   1.55834268e+01   1.58341759e+00   5.17000000e+02
    4.40000000e+01   6.01200000e+03   2.24772727e+01   5.22750253e-01
    5.00634398e+01]]
Scaled doc features:  [[ 8.44356828  0.26174183  0.68839023  9.30821729  1.42111739  9.34441247
   0.01074268  0.2494654  -0.52058255]]
Probabilities: [ 0.06527561  0.12059013  0.09105608  0.18877918  0.06604504  0.06697978
  0.08456902  0.31670517]
Probability:  0.31670516639
Random chance:  0.125
[[ 0.06527561  0.12059013  0.09105608  0.18877918  0.06604504  0.06697978
   0.08456902  0.31670517]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are 
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84100000e+03   1.55858586e+01   1.58181818e+00   5.17000000e+02
    4.40000000e+01   6.01300000e+03   2.25000000e+01   5.22222222e-01
    5.01756818e+01]]
Scaled doc features:  [[ 8.44356828  0.26224086  0.6799647   9.30821729  1.42111739  9.34747853
   0.01395589  0.23832927 -0.51533378]]
Probabilities: [ 0.06527561  0.12059013  0.09105609  0.18877919  0.06604504  0.06697978
  0.08456902  0.31670514]
Probability:  0.316705135241
Random chance:  0.125
[[ 0.06527561  0.12059013  0.09105609  0.18877919  0.06604504  0.06697978
   0.08456902  0.31670514]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are t
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84200000e+03   1.55858586e+01   1.58282828e+00   5.18000000e+02
    4.40000000e+01   6.01400000e+03   2.25000000e+01   5.23232323e-01
    5.00902273e+01]]
Scaled doc features:  [[ 8.44699018  0.26224086  0.6852858   9.34494151  1.42111739  9.35054458
   0.01395589  0.25963222 -0.51932989]]
Probabilities: [ 0.06527564  0.12059026  0.09105618  0.18877941  0.06604506  0.06697982
  0.08456906  0.31670456]
Probability:  0.316704562957
Random chance:  0.125
[[ 0.06527564  0.12059026  0.09105618  0.18877941  0.06604506  0.06697982
   0.08456906  0.31670456]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are to
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84300000e+03   1.55858586e+01   1.58282828e+00   5.17000000e+02
    4.40000000e+01   6.01500000e+03   2.25000000e+01   5.22222222e-01
    5.00902273e+01]]
Scaled doc features:  [[ 8.45041208  0.26224086  0.6852858   9.30821729  1.42111739  9.35361063
   0.01395589  0.23832927 -0.51932989]]
Probabilities: [ 0.06527562  0.12059017  0.09105612  0.18877926  0.06604505  0.06697979
  0.08456903  0.31670496]
Probability:  0.316704964264
Random chance:  0.125
[[ 0.06527562  0.12059017  0.09105612  0.18877926  0.06604505  0.06697979
   0.08456903  0.31670496]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84400000e+03   1.55858586e+01   1.58282828e+00   5.17000000e+02
    4.40000000e+01   6.01600000e+03   2.25000000e+01   5.22222222e-01
    5.00902273e+01]]
Scaled doc features:  [[ 8.45383399  0.26224086  0.6852858   9.30821729  1.42111739  9.35667668
   0.01395589  0.23832927 -0.51932989]]
Probabilities: [ 0.06527562  0.12059019  0.09105613  0.18877929  0.06604505  0.0669798
  0.08456903  0.31670488]
Probability:  0.31670488324
Random chance:  0.125
[[ 0.06527562  0.12059019  0.09105613  0.18877929  0.06604505  0.0669798
   0.08456903  0.31670488]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too 
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84400000e+03   1.55883038e+01   1.58123108e+00   5.17000000e+02
    4.40000000e+01   6.01700000e+03   2.25227273e+01   5.21695257e-01
    5.02022825e+01]]
Scaled doc features:  [[ 8.45383399  0.26274264  0.67687191  9.30821729  1.42111739  9.35974274
   0.0171691   0.22721562 -0.51408985]]
Probabilities: [ 0.06527563  0.1205902   0.09105614  0.1887793   0.06604505  0.0669798
  0.08456904  0.31670485]
Probability:  0.316704852394
Random chance:  0.125
[[ 0.06527563  0.1205902   0.09105614  0.1887793   0.06604505  0.0669798
   0.08456904  0.31670485]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too m
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84500000e+03   1.55883038e+01   1.58224016e+00   5.18000000e+02
    4.40000000e+01   6.01800000e+03   2.25227273e+01   5.22704339e-01
    5.01169142e+01]]
Scaled doc features:  [[ 8.45725589  0.26274264  0.68218764  9.34494151  1.42111739  9.36280879
   0.0171691   0.24849708 -0.51808193]]
Probabilities: [ 0.06527566  0.12059033  0.09105623  0.18877951  0.06604507  0.06697985
  0.08456907  0.3167043 ]
Probability:  0.316704295299
Random chance:  0.125
[[ 0.06527566  0.12059033  0.09105623  0.18877951  0.06604507  0.06697985
   0.08456907  0.3167043 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too ma
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84600000e+03   1.55883038e+01   1.58224016e+00   5.18000000e+02
    4.40000000e+01   6.01900000e+03   2.25227273e+01   5.22704339e-01
    5.01169142e+01]]
Scaled doc features:  [[ 8.46067779  0.26274264  0.68218764  9.34494151  1.42111739  9.36587484
   0.0171691   0.24849708 -0.51808193]]
Probabilities: [ 0.06527566  0.12059034  0.09105624  0.18877953  0.06604507  0.06697985
  0.08456908  0.31670422]
Probability:  0.316704219182
Random chance:  0.125
[[ 0.06527566  0.12059034  0.09105624  0.18877953  0.06604507  0.06697985
   0.08456908  0.31670422]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too man
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84700000e+03   1.55883038e+01   1.58224016e+00   5.18000000e+02
    4.40000000e+01   6.02000000e+03   2.25227273e+01   5.22704339e-01
    5.01169142e+01]]
Scaled doc features:  [[ 8.46409969  0.26274264  0.68218764  9.34494151  1.42111739  9.36894089
   0.0171691   0.24849708 -0.51808193]]
Probabilities: [ 0.06527567  0.12059036  0.09105625  0.18877956  0.06604507  0.06697986
  0.08456908  0.31670414]
Probability:  0.316704143591
Random chance:  0.125
[[ 0.06527567  0.12059036  0.09105625  0.18877956  0.06604507  0.06697986
   0.08456908  0.31670414]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84800000e+03   1.55883038e+01   1.58324924e+00   5.17000000e+02
    4.40000000e+01   6.02100000e+03   2.25227273e+01   5.21695257e-01
    5.00315458e+01]]
Scaled doc features:  [[ 8.46752159  0.26274264  0.68750338  9.30821729  1.42111739  9.37200694
   0.0171691   0.22721562 -0.52207401]]
Probabilities: [ 0.06527564  0.12059027  0.09105619  0.18877942  0.06604506  0.06697983
  0.08456906  0.31670452]
Probability:  0.316704521007
Random chance:  0.125
[[ 0.06527564  0.12059027  0.09105619  0.18877942  0.06604506  0.06697983
   0.08456906  0.31670452]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many 
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84800000e+03   1.55907625e+01   1.58165323e+00   5.17000000e+02
    4.40000000e+01   6.02200000e+03   2.25454545e+01   5.21169355e-01
    5.01435007e+01]]
Scaled doc features:  [[ 8.46752159  0.26324718  0.67909573  9.30821729  1.42111739  9.375073
   0.02038231  0.21612438 -0.51683866]]
Probabilities: [ 0.06527565  0.12059028  0.0910562   0.18877943  0.06604506  0.06697983
  0.08456906  0.31670449]
Probability:  0.31670449086
Random chance:  0.125
[[ 0.06527565  0.12059028  0.0910562   0.18877943  0.06604506  0.06697983
   0.08456906  0.31670449]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many u
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84900000e+03   1.55907625e+01   1.58266129e+00   5.18000000e+02
    4.40000000e+01   6.02300000e+03   2.25454545e+01   5.22177419e-01
    5.00582185e+01]]
Scaled doc features:  [[ 8.47094349  0.26324718  0.6844061   9.34494151  1.42111739  9.37813905
   0.02038231  0.23738438 -0.52082672]]
Probabilities: [ 0.06527568  0.12059041  0.09105629  0.18877963  0.06604507  0.06697987
  0.0845691   0.31670395]
Probability:  0.316703952888
Random chance:  0.125
[[ 0.06527568  0.12059041  0.09105629  0.18877963  0.06604507  0.06697987
   0.0845691   0.31670395]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many us
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85000000e+03   1.55907625e+01   1.58266129e+00   5.18000000e+02
    4.40000000e+01   6.02400000e+03   2.25454545e+01   5.22177419e-01
    5.00582185e+01]]
Scaled doc features:  [[ 8.4743654   0.26324718  0.6844061   9.34494151  1.42111739  9.3812051
   0.02038231  0.23738438 -0.52082672]]
Probabilities: [ 0.06527568  0.12059042  0.0910563   0.18877966  0.06604508  0.06697988
  0.0845691   0.31670388]
Probability:  0.316703879143
Random chance:  0.125
[[ 0.06527568  0.12059042  0.0910563   0.18877966  0.06604508  0.06697988
   0.0845691   0.31670388]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many use
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85100000e+03   1.55907625e+01   1.58266129e+00   5.17000000e+02
    4.40000000e+01   6.02500000e+03   2.25454545e+01   5.21169355e-01
    5.00582185e+01]]
Scaled doc features:  [[ 8.4777873   0.26324718  0.6844061   9.30821729  1.42111739  9.38427115
   0.02038231  0.21612438 -0.52082672]]
Probabilities: [ 0.06527566  0.12059034  0.09105624  0.18877952  0.06604507  0.06697985
  0.08456908  0.31670425]
Probability:  0.316704252561
Random chance:  0.125
[[ 0.06527566  0.12059034  0.09105624  0.18877952  0.06604507  0.06697985
   0.08456908  0.31670425]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many user
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.55907625e+01   1.58366935e+00   5.17000000e+02
    4.40000000e+01   6.02600000e+03   2.25454545e+01   5.21169355e-01
    4.99729362e+01]]
Scaled doc features:  [[ 8.4812092   0.26324718  0.68971648  9.30821729  1.42111739  9.38733721
   0.02038231  0.21612438 -0.52481477]]
Probabilities: [ 0.06527567  0.12059036  0.09105625  0.18877955  0.06604507  0.06697985
  0.08456908  0.31670417]
Probability:  0.316704169768
Random chance:  0.125
[[ 0.06527567  0.12059036  0.09105625  0.18877955  0.06604507  0.06697985
   0.08456908  0.31670417]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85300000e+03   1.55907625e+01   1.58366935e+00   5.17000000e+02
    4.40000000e+01   6.02700000e+03   2.25454545e+01   5.21169355e-01
    4.99729362e+01]]
Scaled doc features:  [[ 8.4846311   0.26324718  0.68971648  9.30821729  1.42111739  9.39040326
   0.02038231  0.21612438 -0.52481477]]
Probabilities: [ 0.06527567  0.12059037  0.09105626  0.18877958  0.06604507  0.06697986
  0.08456909  0.31670409]
Probability:  0.316704094248
Random chance:  0.125
[[ 0.06527567  0.12059037  0.09105626  0.18877958  0.06604507  0.06697986
   0.08456909  0.31670409]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users 
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85300000e+03   1.55932345e+01   1.58207452e+00   5.17000000e+02
    4.40000000e+01   6.02800000e+03   2.25681818e+01   5.20644512e-01
    5.00847909e+01]]
Scaled doc features:  [[ 8.4846311   0.26375445  0.68131507  9.30821729  1.42111739  9.39346931
   0.02359552  0.20505548 -0.51958411]]
Probabilities: [ 0.06527567  0.12059038  0.09105627  0.18877959  0.06604507  0.06697986
  0.08456909  0.31670406]
Probability:  0.316704064996
Random chance:  0.125
[[ 0.06527567  0.12059038  0.09105627  0.18877959  0.06604507  0.06697986
   0.08456909  0.31670406]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users o
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85400000e+03   1.55932345e+01   1.58308157e+00   5.18000000e+02
    4.40000000e+01   6.02900000e+03   2.25681818e+01   5.21651561e-01
    4.99995945e+01]]
Scaled doc features:  [[ 8.488053    0.26375445  0.6866201   9.34494151  1.42111739  9.39653536
   0.02359552  0.22629407 -0.52356815]]
Probabilities: [ 0.0652757   0.1205905   0.09105635  0.18877978  0.06604508  0.0669799
  0.08456912  0.31670355]
Probability:  0.316703549434
Random chance:  0.125
[[ 0.0652757   0.1205905   0.09105635  0.18877978  0.06604508  0.0669799
   0.08456912  0.31670355]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85500000e+03   1.55932345e+01   1.58308157e+00   5.17000000e+02
    4.40000000e+01   6.03000000e+03   2.25681818e+01   5.20644512e-01
    4.99995945e+01]]
Scaled doc features:  [[ 8.49147491  0.26375445  0.6866201   9.30821729  1.42111739  9.39960142
   0.02359552  0.20505548 -0.52356815]]
Probabilities: [ 0.06527568  0.12059042  0.09105629  0.18877965  0.06604507  0.06697987
  0.0845691   0.31670391]
Probability:  0.316703909458
Random chance:  0.125
[[ 0.06527568  0.12059042  0.09105629  0.18877965  0.06604507  0.06697987
   0.0845691   0.31670391]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on 
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85500000e+03   1.55957198e+01   1.58148893e+00   5.17000000e+02
    4.40000000e+01   6.03100000e+03   2.25909091e+01   5.20120724e-01
    5.01112635e+01]]
Scaled doc features:  [[ 8.49147491  0.26426446  0.67823025  9.30821729  1.42111739  9.40266747
   0.02680874  0.19400885 -0.51834617]]
Probabilities: [ 0.06527568  0.12059042  0.0910563   0.18877966  0.06604508  0.06697988
  0.0845691   0.31670388]
Probability:  0.316703880322
Random chance:  0.125
[[ 0.06527568  0.12059042  0.0910563   0.18877966  0.06604508  0.06697988
   0.0845691   0.31670388]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on t
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85600000e+03   1.55957198e+01   1.58249497e+00   5.18000000e+02
    4.40000000e+01   6.03200000e+03   2.25909091e+01   5.21126761e-01
    5.00261528e+01]]
Scaled doc features:  [[ 8.49489681  0.26426446  0.68352994  9.34494151  1.42111739  9.40573352
   0.02680874  0.21522607 -0.5223262 ]]
Probabilities: [ 0.06527571  0.12059054  0.09105638  0.18877985  0.06604509  0.06697992
  0.08456914  0.31670337]
Probability:  0.316703374806
Random chance:  0.125
[[ 0.06527571  0.12059054  0.09105638  0.18877985  0.06604509  0.06697992
   0.08456914  0.31670337]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on th
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85700000e+03   1.55957198e+01   1.58350101e+00   5.18000000e+02
    4.40000000e+01   6.03300000e+03   2.25909091e+01   5.21126761e-01
    4.99410422e+01]]
Scaled doc features:  [[ 8.49831871  0.26426446  0.68882964  9.34494151  1.42111739  9.40879957
   0.02680874  0.21522607 -0.52630623]]
Probabilities: [ 0.06527572  0.12059056  0.0910564   0.18877988  0.06604509  0.06697992
  0.08456914  0.3167033 ]
Probability:  0.316703298979
Random chance:  0.125
[[ 0.06527572  0.12059056  0.0910564   0.18877988  0.06604509  0.06697992
   0.08456914  0.3167033 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85800000e+03   1.55957198e+01   1.58249497e+00   5.17000000e+02
    4.40000000e+01   6.03400000e+03   2.25909091e+01   5.20120724e-01
    5.00261528e+01]]
Scaled doc features:  [[ 8.50174061  0.26426446  0.68352994  9.30821729  1.42111739  9.41186563
   0.02680874  0.19400885 -0.5223262 ]]
Probabilities: [ 0.0652757   0.12059047  0.09105634  0.18877975  0.06604508  0.06697989
  0.08456912  0.31670366]
Probability:  0.316703655078
Random chance:  0.125
[[ 0.0652757   0.12059047  0.09105634  0.18877975  0.06604508  0.06697989
   0.08456912  0.31670366]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the 
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85800000e+03   1.55982184e+01   1.58090452e+00   5.17000000e+02
    4.40000000e+01   6.03500000e+03   2.26136364e+01   5.19597990e-01
    5.01376365e+01]]
Scaled doc features:  [[ 8.50174061  0.2647772   0.67515164  9.30821729  1.42111739  9.41493168
   0.03002195  0.18298442 -0.51711289]]
Probabilities: [ 0.0652757   0.12059048  0.09105634  0.18877976  0.06604508  0.0669799
  0.08456912  0.31670363]
Probability:  0.316703626268
Random chance:  0.125
[[ 0.0652757   0.12059048  0.09105634  0.18877976  0.06604508  0.0669799
   0.08456912  0.31670363]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the i
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85900000e+03   1.55982184e+01   1.58190955e+00   5.18000000e+02
    4.40000000e+01   6.03600000e+03   2.26136364e+01   5.20603015e-01
    5.00526114e+01]]
Scaled doc features:  [[ 8.50516251  0.2647772   0.680446    9.34494151  1.42111739  9.41799773
   0.03002195  0.20418032 -0.52108892]]
Probabilities: [ 0.06527573  0.12059059  0.09105642  0.18877994  0.06604509  0.06697993
  0.08456915  0.31670313]
Probability:  0.316703134347
Random chance:  0.125
[[ 0.06527573  0.12059059  0.09105642  0.18877994  0.06604509  0.06697993
   0.08456915  0.31670313]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the in
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86000000e+03   1.55982184e+01   1.58190955e+00   5.17000000e+02
    4.40000000e+01   6.03700000e+03   2.26136364e+01   5.19597990e-01
    5.00526114e+01]]
Scaled doc features:  [[ 8.50858441  0.2647772   0.680446    9.30821729  1.42111739  9.42106378
   0.03002195  0.18298442 -0.52108892]]
Probabilities: [ 0.06527571  0.12059052  0.09105637  0.18877981  0.06604509  0.06697991
  0.08456913  0.31670348]
Probability:  0.316703477214
Random chance:  0.125
[[ 0.06527571  0.12059052  0.09105637  0.18877981  0.06604509  0.06697991
   0.08456913  0.31670348]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the int
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86100000e+03   1.55982184e+01   1.58090452e+00   5.18000000e+02
    4.40000000e+01   6.03800000e+03   2.26136364e+01   5.20603015e-01
    5.01376365e+01]]
Scaled doc features:  [[ 8.51200632  0.2647772   0.67515164  9.34494151  1.42111739  9.42412984
   0.03002195  0.20418032 -0.51711289]]
Probabilities: [ 0.06527573  0.12059062  0.09105644  0.18877999  0.0660451   0.06697994
  0.08456916  0.316703  ]
Probability:  0.316703004497
Random chance:  0.125
[[ 0.06527573  0.12059062  0.09105644  0.18877999  0.0660451   0.06697994
   0.08456916  0.316703  ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the inte
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86200000e+03   1.55982184e+01   1.58090452e+00   5.18000000e+02
    4.40000000e+01   6.03900000e+03   2.26136364e+01   5.20603015e-01
    5.01376365e+01]]
Scaled doc features:  [[ 8.51542822  0.2647772   0.67515164  9.34494151  1.42111739  9.42719589
   0.03002195  0.20418032 -0.51711289]]
Probabilities: [ 0.06527574  0.12059064  0.09105646  0.18878001  0.0660451   0.06697995
  0.08456917  0.31670294]
Probability:  0.316702937354
Random chance:  0.125
[[ 0.06527574  0.12059064  0.09105646  0.18878001  0.0660451   0.06697995
   0.08456917  0.31670294]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the inter
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86300000e+03   1.55982184e+01   1.58291457e+00   5.18000000e+02
    4.40000000e+01   6.04000000e+03   2.26136364e+01   5.20603015e-01
    4.99675862e+01]]
Scaled doc features:  [[ 8.51885012  0.2647772   0.68574037  9.34494151  1.42111739  9.43026194
   0.03002195  0.20418032 -0.52506495]]
Probabilities: [ 0.06527574  0.12059066  0.09105647  0.18878004  0.0660451   0.06697996
  0.08456917  0.31670286]
Probability:  0.316702859237
Random chance:  0.125
[[ 0.06527574  0.12059066  0.09105647  0.18878004  0.0660451   0.06697996
   0.08456917  0.31670286]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the intern
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86400000e+03   1.55982184e+01   1.58291457e+00   5.18000000e+02
    4.40000000e+01   6.04100000e+03   2.26136364e+01   5.20603015e-01
    4.99675862e+01]]
Scaled doc features:  [[ 8.52227202  0.2647772   0.68574037  9.34494151  1.42111739  9.43332799
   0.03002195  0.20418032 -0.52506495]]
Probabilities: [ 0.06527575  0.12059067  0.09105648  0.18878007  0.0660451   0.06697996
  0.08456918  0.31670279]
Probability:  0.316702793117
Random chance:  0.125
[[ 0.06527575  0.12059067  0.09105648  0.18878007  0.0660451   0.06697996
   0.08456918  0.31670279]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the interne
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86500000e+03   1.55982184e+01   1.58090452e+00   5.18000000e+02
    4.40000000e+01   6.04200000e+03   2.26136364e+01   5.20603015e-01
    5.01376365e+01]]
Scaled doc features:  [[ 8.52569392  0.2647772   0.67515164  9.34494151  1.42111739  9.43639405
   0.03002195  0.20418032 -0.51711289]]
Probabilities: [ 0.06527575  0.12059069  0.09105649  0.18878009  0.0660451   0.06697997
  0.08456918  0.31670274]
Probability:  0.316702738729
Random chance:  0.125
[[ 0.06527575  0.12059069  0.09105649  0.18878009  0.0660451   0.06697997
   0.08456918  0.31670274]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86600000e+03   1.56384194e+01   1.58391960e+00   5.17000000e+02
    4.40000000e+01   6.04300000e+03   2.26136364e+01   5.19597990e-01
    4.98825611e+01]]
Scaled doc features:  [[ 8.52911583  0.27302682  0.69103473  9.30821729  1.42111739  9.4394601
   0.03002195  0.18298442 -0.52904098]]
Probabilities: [ 0.06527573  0.12059061  0.09105644  0.18877997  0.0660451   0.06697994
  0.08456916  0.31670305]
Probability:  0.316703046187
Random chance:  0.125
[[ 0.06527573  0.12059061  0.09105644  0.18877997  0.0660451   0.06697994
   0.08456916  0.31670305]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86600000e+03   1.54374093e+01   1.58391960e+00   5.17000000e+02
    4.50000000e+01   6.04400000e+03   2.21111111e+01   5.19597990e-01
    5.03926242e+01]]
Scaled doc features:  [[ 8.52911583  0.23177767  0.69103473  9.30821729  1.50249762  9.44252615
  -0.04102574  0.18298442 -0.5051889 ]]
Probabilities: [ 0.06527574  0.12059066  0.09105647  0.18878004  0.0660451   0.06697996
  0.08456917  0.31670286]
Probability:  0.316702858554
Random chance:  0.125
[[ 0.06527574  0.12059066  0.09105647  0.18878004  0.0660451   0.06697996
   0.08456917  0.31670286]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, t.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79100000e+03   1.54545086e+01   1.57606491e+00   5.15000000e+02
    4.40000000e+01   5.95900000e+03   2.24090909e+01   5.22312373e-01
    5.07546814e+01]]
Scaled doc features:  [[  8.27247319e+00   2.35286606e-01   6.49657068e-01   9.23476887e+00
    1.42111739e+00   9.18191170e+00   1.10303887e-03   2.40230550e-01
   -4.88258021e-01]]
Probabilities: [ 0.06527521  0.12058858  0.09105496  0.18877668  0.06604487  0.06697926
  0.08456856  0.31671188]
Probability:  0.316711882495
Random chance:  0.125
[[ 0.06527521  0.12058858  0.09105496  0.18877668  0.06604487  0.06697926
   0.08456856  0.31671188]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, th.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79200000e+03   1.52553167e+01   1.57707911e+00   5.15000000e+02
    4.50000000e+01   5.96000000e+03   2.19111111e+01   5.22312373e-01
    5.11743297e+01]]
Scaled doc features:  [[ 8.27589509  0.19441056  0.65499976  9.23476887  1.50249762  9.18497775
  -0.06930201  0.24023055 -0.46863401]]
Probabilities: [ 0.06527524  0.12058868  0.09105504  0.18877685  0.06604488  0.0669793
  0.08456859  0.31671142]
Probability:  0.316711419722
Random chance:  0.125
[[ 0.06527524  0.12058868  0.09105504  0.18877685  0.06604488  0.0669793
   0.08456859  0.31671142]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79300000e+03   1.52553167e+01   1.57606491e+00   5.14000000e+02
    4.50000000e+01   5.96100000e+03   2.19111111e+01   5.21298174e-01
    5.12601309e+01]]
Scaled doc features:  [[ 8.27931699  0.19441056  0.64965707  9.19804466  1.50249762  9.1880438
  -0.06930201  0.21884118 -0.46462169]]
Probabilities: [ 0.0652752   0.12058853  0.09105493  0.1887766   0.06604487  0.06697925
  0.08456855  0.31671208]
Probability:  0.316712082956
Random chance:  0.125
[[ 0.0652752   0.12058853  0.09105493  0.1887766   0.06604487  0.06697925
   0.08456855  0.31671208]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79300000e+03   1.52576292e+01   1.57446809e+00   5.14000000e+02
    4.50000000e+01   5.96200000e+03   2.19333333e+01   5.20770010e-01
    5.13726667e+01]]
Scaled doc features:  [[ 8.27931699  0.19488511  0.64124517  9.19804466  1.50249762  9.19110985
  -0.0661602   0.20770223 -0.45935918]]
Probabilities: [ 0.0652752   0.12058854  0.09105494  0.18877662  0.06604487  0.06697925
  0.08456855  0.31671203]
Probability:  0.316712031399
Random chance:  0.125
[[ 0.0652752   0.12058854  0.09105494  0.18877662  0.06604487  0.06697925
   0.08456855  0.31671203]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the d.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79400000e+03   1.54570231e+01   1.57548126e+00   5.15000000e+02
    4.40000000e+01   5.96300000e+03   2.24318182e+01   5.21783181e-01
    5.07809903e+01]]
Scaled doc features:  [[  8.28273889e+00   2.35802614e-01   6.46582449e-01   9.23476887e+00
    1.42111739e+00   9.19417591e+00   4.31625111e-03   2.29069935e-01
   -4.87027743e-01]]
Probabilities: [ 0.06527523  0.12058868  0.09105504  0.18877684  0.06604488  0.0669793
  0.08456859  0.31671144]
Probability:  0.316711439781
Random chance:  0.125
[[ 0.06527523  0.12058868  0.09105504  0.18877684  0.06604488  0.0669793
   0.08456859  0.31671144]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the di.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79500000e+03   1.52576292e+01   1.57548126e+00   5.15000000e+02
    4.50000000e+01   5.96400000e+03   2.19333333e+01   5.21783181e-01
    5.12869524e+01]]
Scaled doc features:  [[ 8.2861608   0.19488511  0.64658245  9.23476887  1.50249762  9.19724196
  -0.0661602   0.22906994 -0.46336744]]
Probabilities: [ 0.06527526  0.12058878  0.09105511  0.18877701  0.06604489  0.06697933
  0.08456862  0.316711  ]
Probability:  0.31671099891
Random chance:  0.125
[[ 0.06527526  0.12058878  0.09105511  0.18877701  0.06604489  0.06697933
   0.08456862  0.316711  ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the dir.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79600000e+03   1.52576292e+01   1.57446809e+00   5.15000000e+02
    4.50000000e+01   5.96500000e+03   2.19333333e+01   5.21783181e-01
    5.13726667e+01]]
Scaled doc features:  [[ 8.2895827   0.19488511  0.64124517  9.23476887  1.50249762  9.20030801
  -0.0661602   0.22906994 -0.45935918]]
Probabilities: [ 0.06527527  0.12058881  0.09105513  0.18877705  0.0660449   0.06697934
  0.08456863  0.31671089]
Probability:  0.316710885894
Random chance:  0.125
[[ 0.06527527  0.12058881  0.09105513  0.18877705  0.0660449   0.06697934
   0.08456863  0.31671089]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the dire.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79700000e+03   1.52576292e+01   1.57548126e+00   5.15000000e+02
    4.50000000e+01   5.96600000e+03   2.19333333e+01   5.21783181e-01
    5.12869524e+01]]
Scaled doc features:  [[ 8.2930046   0.19488511  0.64658245  9.23476887  1.50249762  9.20337406
  -0.0661602   0.22906994 -0.46336744]]
Probabilities: [ 0.06527527  0.12058884  0.09105515  0.1887771   0.0660449   0.06697935
  0.08456864  0.31671075]
Probability:  0.316710754765
Random chance:  0.125
[[ 0.06527527  0.12058884  0.09105515  0.1887771   0.0660449   0.06697935
   0.08456864  0.31671075]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the direc.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79800000e+03   1.52576292e+01   1.57446809e+00   5.15000000e+02
    4.50000000e+01   5.96700000e+03   2.19333333e+01   5.21783181e-01
    5.13726667e+01]]
Scaled doc features:  [[ 8.2964265   0.19488511  0.64124517  9.23476887  1.50249762  9.20644012
  -0.0661602   0.22906994 -0.45935918]]
Probabilities: [ 0.06527528  0.12058886  0.09105517  0.18877714  0.0660449   0.06697936
  0.08456864  0.31671064]
Probability:  0.316710643235
Random chance:  0.125
[[ 0.06527528  0.12058886  0.09105517  0.18877714  0.0660449   0.06697936
   0.08456864  0.31671064]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the direct.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79900000e+03   1.52576292e+01   1.57649443e+00   5.15000000e+02
    4.50000000e+01   5.96800000e+03   2.19333333e+01   5.21783181e-01
    5.12012381e+01]]
Scaled doc features:  [[ 8.2998484   0.19488511  0.65191973  9.23476887  1.50249762  9.20950617
  -0.0661602   0.22906994 -0.4673757 ]]
Probabilities: [ 0.06527529  0.1205889   0.09105519  0.18877719  0.06604491  0.06697937
  0.08456865  0.3167105 ]
Probability:  0.316710504441
Random chance:  0.125
[[ 0.06527529  0.1205889   0.09105519  0.18877719  0.06604491  0.06697937
   0.08456865  0.3167105 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the directo.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80000000e+03   1.52576292e+01   1.57446809e+00   5.15000000e+02
    4.50000000e+01   5.96900000e+03   2.19333333e+01   5.21783181e-01
    5.13726667e+01]]
Scaled doc features:  [[ 8.30327031  0.19488511  0.64124517  9.23476887  1.50249762  9.21257222
  -0.0661602   0.22906994 -0.45935918]]
Probabilities: [ 0.0652753   0.12058892  0.09105521  0.18877723  0.06604491  0.06697938
  0.08456866  0.3167104 ]
Probability:  0.316710403796
Random chance:  0.125
[[ 0.0652753   0.12058892  0.09105521  0.18877723  0.06604491  0.06697938
   0.08456866  0.3167104 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80100000e+03   1.52981560e+01   1.57750760e+00   5.14000000e+02
    4.50000000e+01   5.97000000e+03   2.19333333e+01   5.20770010e-01
    5.11155238e+01]]
Scaled doc features:  [[ 8.30669221  0.2032016   0.65725701  9.19804466  1.50249762  9.21563827
  -0.0661602   0.20770223 -0.47138395]]
Probabilities: [ 0.06527526  0.12058878  0.09105511  0.18877701  0.06604489  0.06697933
  0.08456862  0.31671099]
Probability:  0.316710985181
Random chance:  0.125
[[ 0.06527526  0.12058878  0.09105511  0.18877701  0.06604489  0.06697933
   0.08456862  0.31671099]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80100000e+03   1.53004408e+01   1.57591093e+00   5.14000000e+02
    4.50000000e+01   5.97100000e+03   2.19555556e+01   5.20242915e-01
    5.12280463e+01]]
Scaled doc features:  [[ 8.30669221  0.20367047  0.64884593  9.19804466  1.50249762  9.21870432
  -0.0630184   0.19658584 -0.46612206]]
Probabilities: [ 0.06527526  0.1205888   0.09105512  0.18877703  0.0660449   0.06697933
  0.08456862  0.31671094]
Probability:  0.316710936336
Random chance:  0.125
[[ 0.06527526  0.1205888   0.09105512  0.18877703  0.0660449   0.06697933
   0.08456862  0.31671094]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director o.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80200000e+03   1.53004408e+01   1.57692308e+00   5.15000000e+02
    4.50000000e+01   5.97200000e+03   2.19555556e+01   5.21255061e-01
    5.11424188e+01]]
Scaled doc features:  [[ 8.31011411  0.20367047  0.65417781  9.23476887  1.50249762  9.22177038
  -0.0630184   0.21793191 -0.47012626]]
Probabilities: [ 0.06527531  0.12058899  0.09105526  0.18877735  0.06604492  0.0669794
  0.08456868  0.31671008]
Probability:  0.316710082892
Random chance:  0.125
[[ 0.06527531  0.12058899  0.09105526  0.18877735  0.06604492  0.0669794
   0.08456868  0.31671008]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80300000e+03   1.53004408e+01   1.57692308e+00   5.14000000e+02
    4.50000000e+01   5.97300000e+03   2.19555556e+01   5.20242915e-01
    5.11424188e+01]]
Scaled doc features:  [[ 8.31353601  0.20367047  0.65417781  9.19804466  1.50249762  9.22483643
  -0.0630184   0.19658584 -0.47012626]]
Probabilities: [ 0.06527528  0.12058885  0.09105516  0.18877712  0.0660449   0.06697935
  0.08456864  0.31671068]
Probability:  0.316710682506
Random chance:  0.125
[[ 0.06527528  0.12058885  0.09105516  0.18877712  0.0660449   0.06697935
   0.08456864  0.31671068]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80300000e+03   1.53027390e+01   1.57532861e+00   5.14000000e+02
    4.50000000e+01   5.97400000e+03   2.19777778e+01   5.19716886e-01
    5.12547547e+01]]
Scaled doc features:  [[ 8.31353601  0.20414208  0.64577835  9.19804466  1.50249762  9.22790248
  -0.05987659  0.18549192 -0.4648731 ]]
Probabilities: [ 0.06527528  0.12058887  0.09105517  0.18877714  0.0660449   0.06697936
  0.08456864  0.31671063]
Probability:  0.316710633818
Random chance:  0.125
[[ 0.06527528  0.12058887  0.09105517  0.18877714  0.0660449   0.06697936
   0.08456864  0.31671063]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of t.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80400000e+03   1.55025370e+01   1.57633974e+00   5.15000000e+02
    4.40000000e+01   5.97500000e+03   2.24772727e+01   5.20728008e-01
    5.06622264e+01]]
Scaled doc features:  [[ 8.31695791  0.24514249  0.65110483  9.23476887  1.42111739  9.23096853
   0.01074268  0.20681641 -0.4925815 ]]
Probabilities: [ 0.06527531  0.12058899  0.09105526  0.18877735  0.06604492  0.0669794
  0.08456868  0.31671009]
Probability:  0.316710085813
Random chance:  0.125
[[ 0.06527531  0.12058899  0.09105526  0.18877735  0.06604492  0.0669794
   0.08456868  0.31671009]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of th.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80500000e+03   1.53027390e+01   1.57735086e+00   5.15000000e+02
    4.50000000e+01   5.97600000e+03   2.19777778e+01   5.20728008e-01
    5.10836728e+01]]
Scaled doc features:  [[ 8.32037981  0.20414208  0.65643132  9.23476887  1.50249762  9.23403459
  -0.05987659  0.20681641 -0.4728734 ]]
Probabilities: [ 0.06527534  0.12058909  0.09105533  0.1887775   0.06604493  0.06697943
  0.08456871  0.31670967]
Probability:  0.316709672982
Random chance:  0.125
[[ 0.06527534  0.12058909  0.09105533  0.1887775   0.06604493  0.06697943
   0.08456871  0.31670967]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80600000e+03   1.53027390e+01   1.57633974e+00   5.14000000e+02
    4.50000000e+01   5.97700000e+03   2.19777778e+01   5.19716886e-01
    5.11692138e+01]]
Scaled doc features:  [[ 8.32380172  0.20414208  0.65110483  9.19804466  1.50249762  9.23710064
  -0.05987659  0.18549192 -0.46887325]]
Probabilities: [ 0.0652753   0.12058895  0.09105523  0.18877728  0.06604491  0.06697939
  0.08456867  0.31671027]
Probability:  0.316710265458
Random chance:  0.125
[[ 0.0652753   0.12058895  0.09105523  0.18877728  0.06604491  0.06697939
   0.08456867  0.31671027]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80600000e+03   1.53050505e+01   1.57474747e+00   5.14000000e+02
    4.50000000e+01   5.97800000e+03   2.20000000e+01   5.19191919e-01
    5.12813636e+01]]
Scaled doc features:  [[ 8.32380172  0.20461641  0.64271697  9.19804466  1.50249762  9.24016669
  -0.05673478  0.17442042 -0.46362878]]
Probabilities: [ 0.06527531  0.12058896  0.09105524  0.1887773   0.06604491  0.06697939
  0.08456867  0.31671022]
Probability:  0.316710217265
Random chance:  0.125
[[ 0.06527531  0.12058896  0.09105524  0.1887773   0.06604491  0.06697939
   0.08456867  0.31671022]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the N.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80700000e+03   1.55050505e+01   1.57575758e+00   5.15000000e+02
    4.40000000e+01   5.97900000e+03   2.25000000e+01   5.20202020e-01
    5.06884091e+01]]
Scaled doc features:  [[ 8.32722362  0.24565828  0.64803807  9.23476887  1.42111739  9.24323274
   0.01395589  0.19572337 -0.49135712]]
Probabilities: [ 0.06527534  0.12058908  0.09105533  0.1887775   0.06604493  0.06697943
  0.08456871  0.31670968]
Probability:  0.316709683014
Random chance:  0.125
[[ 0.06527534  0.12058908  0.09105533  0.1887775   0.06604493  0.06697943
   0.08456871  0.31670968]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the Na.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80800000e+03   1.53050505e+01   1.57575758e+00   5.15000000e+02
    4.50000000e+01   5.98000000e+03   2.20000000e+01   5.20202020e-01
    5.11959091e+01]]
Scaled doc features:  [[ 8.33064552  0.20461641  0.64803807  9.23476887  1.50249762  9.2462988
  -0.05673478  0.19572337 -0.4676249 ]]
Probabilities: [ 0.06527536  0.12058917  0.09105539  0.18877764  0.06604494  0.06697946
  0.08456874  0.31670929]
Probability:  0.316709289979
Random chance:  0.125
[[ 0.06527536  0.12058917  0.09105539  0.18877764  0.06604494  0.06697946
   0.08456874  0.31670929]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the Nat.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80900000e+03   1.53050505e+01   1.57575758e+00   5.15000000e+02
    4.50000000e+01   5.98100000e+03   2.20000000e+01   5.20202020e-01
    5.11959091e+01]]
Scaled doc features:  [[ 8.33406742  0.20461641  0.64803807  9.23476887  1.50249762  9.24936485
  -0.05673478  0.19572337 -0.4676249 ]]
Probabilities: [ 0.06527537  0.1205892   0.09105541  0.18877769  0.06604494  0.06697947
  0.08456874  0.31670918]
Probability:  0.316709178934
Random chance:  0.125
[[ 0.06527537  0.1205892   0.09105541  0.18877769  0.06604494  0.06697947
   0.08456874  0.31670918]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the Nati.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81000000e+03   1.53050505e+01   1.57474747e+00   5.15000000e+02
    4.50000000e+01   5.98200000e+03   2.20000000e+01   5.20202020e-01
    5.12813636e+01]]
Scaled doc features:  [[ 8.33748932  0.20461641  0.64271697  9.23476887  1.50249762  9.2524309
  -0.05673478  0.19572337 -0.46362878]]
Probabilities: [ 0.06527537  0.12058922  0.09105543  0.18877772  0.06604494  0.06697948
  0.08456875  0.31670908]
Probability:  0.316709077132
Random chance:  0.125
[[ 0.06527537  0.12058922  0.09105543  0.18877772  0.06604494  0.06697948
   0.08456875  0.31670908]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the Natio.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81100000e+03   1.53050505e+01   1.57474747e+00   5.15000000e+02
    4.50000000e+01   5.98300000e+03   2.20000000e+01   5.20202020e-01
    5.12813636e+01]]
Scaled doc features:  [[ 8.34091123  0.20461641  0.64271697  9.23476887  1.50249762  9.25549695
  -0.05673478  0.19572337 -0.46362878]]
Probabilities: [ 0.06527538  0.12058925  0.09105545  0.18877776  0.06604495  0.06697949
  0.08456876  0.31670897]
Probability:  0.316708967512
Random chance:  0.125
[[ 0.06527538  0.12058925  0.09105545  0.18877776  0.06604495  0.06697949
   0.08456876  0.31670897]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the Nation.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81200000e+03   1.53050505e+01   1.57676768e+00   5.15000000e+02
    4.50000000e+01   5.98400000e+03   2.20000000e+01   5.20202020e-01
    5.11104545e+01]]
Scaled doc features:  [[ 8.34433313  0.20461641  0.65335918  9.23476887  1.50249762  9.25856301
  -0.05673478  0.19572337 -0.47162101]]
Probabilities: [ 0.06527539  0.12058928  0.09105547  0.18877781  0.06604495  0.0669795
  0.08456877  0.31670884]
Probability:  0.316708841743
Random chance:  0.125
[[ 0.06527539  0.12058928  0.09105547  0.18877781  0.06604495  0.0669795
   0.08456877  0.31670884]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the Nationa.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81300000e+03   1.53050505e+01   1.57474747e+00   5.15000000e+02
    4.50000000e+01   5.98500000e+03   2.20000000e+01   5.20202020e-01
    5.12813636e+01]]
Scaled doc features:  [[ 8.34775503  0.20461641  0.64271697  9.23476887  1.50249762  9.26162906
  -0.05673478  0.19572337 -0.46362878]]
Probabilities: [ 0.06527539  0.1205893   0.09105548  0.18877785  0.06604495  0.0669795
  0.08456877  0.31670875]
Probability:  0.31670875048
Random chance:  0.125
[[ 0.06527539  0.1205893   0.09105548  0.18877785  0.06604495  0.0669795
   0.08456877  0.31670875]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81400000e+03   1.53454545e+01   1.57777778e+00   5.14000000e+02
    4.50000000e+01   5.98600000e+03   2.20000000e+01   5.19191919e-01
    5.10250000e+01]]
Scaled doc features:  [[ 8.35117693  0.2129077   0.65868028  9.19804466  1.50249762  9.26469511
  -0.05673478  0.17442042 -0.47561712]]
Probabilities: [ 0.06527536  0.12058918  0.0910554   0.18877765  0.06604494  0.06697946
  0.08456874  0.31670927]
Probability:  0.316709268797
Random chance:  0.125
[[ 0.06527536  0.12058918  0.0910554   0.18877765  0.06604494  0.06697946
   0.08456874  0.31670927]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81400000e+03   1.53477385e+01   1.57618567e+00   5.14000000e+02
    4.50000000e+01   5.98700000e+03   2.20222222e+01   5.18668012e-01
    5.11371367e+01]]
Scaled doc features:  [[ 8.35117693  0.2133764   0.65029323  9.19804466  1.50249762  9.26776116
  -0.05359297  0.16337126 -0.47037327]]
Probabilities: [ 0.06527537  0.12058919  0.09105541  0.18877767  0.06604494  0.06697947
  0.08456874  0.31670922]
Probability:  0.316709223192
Random chance:  0.125
[[ 0.06527537  0.12058919  0.09105541  0.18877767  0.06604494  0.06697947
   0.08456874  0.31670922]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National S.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81500000e+03   1.55479406e+01   1.57719475e+00   5.15000000e+02
    4.40000000e+01   5.98800000e+03   2.25227273e+01   5.19677094e-01
    5.05437557e+01]]
Scaled doc features:  [[ 8.35459883  0.25445972  0.65560897  9.23476887  1.42111739  9.27082722
   0.0171691   0.18465272 -0.49812154]]
Probabilities: [ 0.0652754   0.12058931  0.09105549  0.18877786  0.06604495  0.0669795
  0.08456877  0.31670872]
Probability:  0.316708719604
Random chance:  0.125
[[ 0.0652754   0.12058931  0.09105549  0.18877786  0.06604495  0.0669795
   0.08456877  0.31670872]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Sc.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81600000e+03   1.53477385e+01   1.57618567e+00   5.15000000e+02
    4.50000000e+01   5.98900000e+03   2.20222222e+01   5.19677094e-01
    5.11371367e+01]]
Scaled doc features:  [[ 8.35802073  0.2133764   0.65029323  9.23476887  1.50249762  9.27389327
  -0.05359297  0.18465272 -0.47037327]]
Probabilities: [ 0.06527542  0.12058939  0.09105555  0.18877799  0.06604496  0.06697953
  0.0845688   0.31670836]
Probability:  0.316708361076
Random chance:  0.125
[[ 0.06527542  0.12058939  0.09105555  0.18877799  0.06604496  0.06697953
   0.0845688   0.31670836]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Sci.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81700000e+03   1.53477385e+01   1.57719475e+00   5.15000000e+02
    4.50000000e+01   5.99000000e+03   2.20222222e+01   5.19677094e-01
    5.10517684e+01]]
Scaled doc features:  [[ 8.36144264  0.2133764   0.65560897  9.23476887  1.50249762  9.27695932
  -0.05359297  0.18465272 -0.47436535]]
Probabilities: [ 0.06527542  0.12058942  0.09105557  0.18877803  0.06604496  0.06697954
  0.08456881  0.31670825]
Probability:  0.316708248065
Random chance:  0.125
[[ 0.06527542  0.12058942  0.09105557  0.18877803  0.06604496  0.06697954
   0.08456881  0.31670825]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Scie.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81800000e+03   1.53477385e+01   1.57618567e+00   5.15000000e+02
    4.50000000e+01   5.99100000e+03   2.20222222e+01   5.19677094e-01
    5.11371367e+01]]
Scaled doc features:  [[ 8.36486454  0.2133764   0.65029323  9.23476887  1.50249762  9.28002537
  -0.05359297  0.18465272 -0.47037327]]
Probabilities: [ 0.06527543  0.12058944  0.09105558  0.18877807  0.06604497  0.06697955
  0.08456881  0.31670815]
Probability:  0.316708152228
Random chance:  0.125
[[ 0.06527543  0.12058944  0.09105558  0.18877807  0.06604497  0.06697955
   0.08456881  0.31670815]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Scien.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81900000e+03   1.53477385e+01   1.57618567e+00   5.15000000e+02
    4.50000000e+01   5.99200000e+03   2.20222222e+01   5.19677094e-01
    5.11371367e+01]]
Scaled doc features:  [[ 8.36828644  0.2133764   0.65029323  9.23476887  1.50249762  9.28309143
  -0.05359297  0.18465272 -0.47037327]]
Probabilities: [ 0.06527544  0.12058946  0.0910556   0.18877811  0.06604497  0.06697956
  0.08456882  0.31670805]
Probability:  0.316708048862
Random chance:  0.125
[[ 0.06527544  0.12058946  0.0910556   0.18877811  0.06604497  0.06697956
   0.08456882  0.31670805]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Scienc.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82000000e+03   1.53477385e+01   1.57618567e+00   5.15000000e+02
    4.50000000e+01   5.99300000e+03   2.20222222e+01   5.19677094e-01
    5.11371367e+01]]
Scaled doc features:  [[ 8.37170834  0.2133764   0.65029323  9.23476887  1.50249762  9.28615748
  -0.05359297  0.18465272 -0.47037327]]
Probabilities: [ 0.06527544  0.12058948  0.09105562  0.18877815  0.06604497  0.06697956
  0.08456883  0.31670795]
Probability:  0.316707946195
Random chance:  0.125
[[ 0.06527544  0.12058948  0.09105562  0.18877815  0.06604497  0.06697956
   0.08456883  0.31670795]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82100000e+03   1.53477385e+01   1.57820383e+00   5.14000000e+02
    4.50000000e+01   5.99400000e+03   2.20222222e+01   5.18668012e-01
    5.09664000e+01]]
Scaled doc features:  [[ 8.37513024  0.2133764   0.6609247   9.19804466  1.50249762  9.28922353
  -0.05359297  0.16337126 -0.47835743]]
Probabilities: [ 0.06527541  0.12058937  0.09105554  0.18877796  0.06604496  0.06697953
  0.08456879  0.31670844]
Probability:  0.316708444885
Random chance:  0.125
[[ 0.06527541  0.12058937  0.09105554  0.18877796  0.06604496  0.06697953
   0.08456879  0.31670844]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82100000e+03   1.53500358e+01   1.57661290e+00   5.14000000e+02
    4.50000000e+01   5.99500000e+03   2.20444444e+01   5.18145161e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.37513024  0.21384783  0.65254384  9.19804466  1.50249762  9.29228958
  -0.05045117  0.15234438 -0.47311823]]
Probabilities: [ 0.06527541  0.12058938  0.09105554  0.18877798  0.06604496  0.06697953
  0.0845688   0.3167084 ]
Probability:  0.316708401219
Random chance:  0.125
[[ 0.06527541  0.12058938  0.09105554  0.18877798  0.06604496  0.06697953
   0.0845688   0.3167084 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science o.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82200000e+03   1.53500358e+01   1.57762097e+00   5.15000000e+02
    4.50000000e+01   5.99600000e+03   2.20444444e+01   5.19153226e-01
    5.09931550e+01]]
Scaled doc features:  [[ 8.37855215  0.21384783  0.65785422  9.23476887  1.50249762  9.29535563
  -0.05045117  0.17360438 -0.47710628]]
Probabilities: [ 0.06527546  0.12058955  0.09105566  0.18877824  0.06604498  0.06697958
  0.08456885  0.31670768]
Probability:  0.316707678195
Random chance:  0.125
[[ 0.06527546  0.12058955  0.09105566  0.18877824  0.06604498  0.06697958
   0.08456885  0.31670768]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science ou.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82300000e+03   1.53500358e+01   1.57762097e+00   5.15000000e+02
    4.50000000e+01   5.99700000e+03   2.20444444e+01   5.19153226e-01
    5.09931550e+01]]
Scaled doc features:  [[ 8.38197405  0.21384783  0.65785422  9.23476887  1.50249762  9.29842169
  -0.05045117  0.17360438 -0.47710628]]
Probabilities: [ 0.06527546  0.12058957  0.09105568  0.18877828  0.06604498  0.06697959
  0.08456885  0.31670758]
Probability:  0.316707578055
Random chance:  0.125
[[ 0.06527546  0.12058957  0.09105568  0.18877828  0.06604498  0.06697959
   0.08456885  0.31670758]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science o.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82200000e+03   1.53500358e+01   1.57762097e+00   5.15000000e+02
    4.50000000e+01   5.99600000e+03   2.20444444e+01   5.19153226e-01
    5.09931550e+01]]
Scaled doc features:  [[ 8.37855215  0.21384783  0.65785422  9.23476887  1.50249762  9.29535563
  -0.05045117  0.17360438 -0.47710628]]
Probabilities: [ 0.06527546  0.12058955  0.09105566  0.18877824  0.06604498  0.06697958
  0.08456885  0.31670768]
Probability:  0.316707678195
Random chance:  0.125
[[ 0.06527546  0.12058955  0.09105566  0.18877824  0.06604498  0.06697958
   0.08456885  0.31670768]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82100000e+03   1.53500358e+01   1.57661290e+00   5.14000000e+02
    4.50000000e+01   5.99500000e+03   2.20444444e+01   5.18145161e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.37513024  0.21384783  0.65254384  9.19804466  1.50249762  9.29228958
  -0.05045117  0.15234438 -0.47311823]]
Probabilities: [ 0.06527541  0.12058938  0.09105554  0.18877798  0.06604496  0.06697953
  0.0845688   0.3167084 ]
Probability:  0.316708401219
Random chance:  0.125
[[ 0.06527541  0.12058938  0.09105554  0.18877798  0.06604496  0.06697953
   0.0845688   0.3167084 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science F.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82200000e+03   1.55504399e+01   1.57762097e+00   5.15000000e+02
    4.40000000e+01   5.99600000e+03   2.25454545e+01   5.19153226e-01
    5.04846298e+01]]
Scaled doc features:  [[ 8.37855215  0.25497261  0.65785422  9.23476887  1.42111739  9.29535563
   0.02038231  0.17360438 -0.50088645]]
Probabilities: [ 0.06527544  0.12058949  0.09105562  0.18877815  0.06604497  0.06697957
  0.08456883  0.31670792]
Probability:  0.316707924352
Random chance:  0.125
[[ 0.06527544  0.12058949  0.09105562  0.18877815  0.06604497  0.06697957
   0.08456883  0.31670792]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Fo.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82300000e+03   1.53500358e+01   1.57661290e+00   5.15000000e+02
    4.50000000e+01   5.99700000e+03   2.20444444e+01   5.19153226e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.38197405  0.21384783  0.65254384  9.23476887  1.50249762  9.29842169
  -0.05045117  0.17360438 -0.47311823]]
Probabilities: [ 0.06527546  0.12058957  0.09105568  0.18877828  0.06604498  0.06697959
  0.08456885  0.31670759]
Probability:  0.316707585956
Random chance:  0.125
[[ 0.06527546  0.12058957  0.09105568  0.18877828  0.06604498  0.06697959
   0.08456885  0.31670759]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Fou.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82400000e+03   1.53500358e+01   1.57661290e+00   5.15000000e+02
    4.50000000e+01   5.99800000e+03   2.20444444e+01   5.19153226e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.38539595  0.21384783  0.65254384  9.23476887  1.50249762  9.30148774
  -0.05045117  0.17360438 -0.47311823]]
Probabilities: [ 0.06527547  0.12058959  0.0910557   0.18877832  0.06604498  0.0669796
  0.08456886  0.31670749]
Probability:  0.316707486438
Random chance:  0.125
[[ 0.06527547  0.12058959  0.0910557   0.18877832  0.06604498  0.0669796
   0.08456886  0.31670749]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foun.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82500000e+03   1.53500358e+01   1.57661290e+00   5.15000000e+02
    4.50000000e+01   5.99900000e+03   2.20444444e+01   5.19153226e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.38881785  0.21384783  0.65254384  9.23476887  1.50249762  9.30455379
  -0.05045117  0.17360438 -0.47311823]]
Probabilities: [ 0.06527547  0.12058961  0.09105571  0.18877835  0.06604499  0.06697961
  0.08456886  0.31670739]
Probability:  0.316707387596
Random chance:  0.125
[[ 0.06527547  0.12058961  0.09105571  0.18877835  0.06604499  0.06697961
   0.08456886  0.31670739]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Found.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82600000e+03   1.53500358e+01   1.57762097e+00   5.15000000e+02
    4.50000000e+01   6.00000000e+03   2.20444444e+01   5.19153226e-01
    5.09931550e+01]]
Scaled doc features:  [[ 8.39223975  0.21384783  0.65785422  9.23476887  1.50249762  9.30761984
  -0.05045117  0.17360438 -0.47710628]]
Probabilities: [ 0.06527548  0.12058964  0.09105573  0.18877839  0.06604499  0.06697962
  0.08456887  0.31670728]
Probability:  0.3167072817
Random chance:  0.125
[[ 0.06527548  0.12058964  0.09105573  0.18877839  0.06604499  0.06697962
   0.08456887  0.31670728]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Founda.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82700000e+03   1.53500358e+01   1.57661290e+00   5.15000000e+02
    4.50000000e+01   6.00100000e+03   2.20444444e+01   5.19153226e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.39566165  0.21384783  0.65254384  9.23476887  1.50249762  9.3106859
  -0.05045117  0.17360438 -0.47311823]]
Probabilities: [ 0.06527549  0.12058966  0.09105575  0.18877843  0.06604499  0.06697962
  0.08456888  0.31670719]
Probability:  0.316707191926
Random chance:  0.125
[[ 0.06527549  0.12058966  0.09105575  0.18877843  0.06604499  0.06697962
   0.08456888  0.31670719]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundat.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82800000e+03   1.53500358e+01   1.57661290e+00   5.15000000e+02
    4.50000000e+01   6.00200000e+03   2.20444444e+01   5.19153226e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.39908356  0.21384783  0.65254384  9.23476887  1.50249762  9.31375195
  -0.05045117  0.17360438 -0.47311823]]
Probabilities: [ 0.06527549  0.12058968  0.09105576  0.18877846  0.06604499  0.06697963
  0.08456888  0.3167071 ]
Probability:  0.316707095089
Random chance:  0.125
[[ 0.06527549  0.12058968  0.09105576  0.18877846  0.06604499  0.06697963
   0.08456888  0.3167071 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundati.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82900000e+03   1.53500358e+01   1.57661290e+00   5.15000000e+02
    4.50000000e+01   6.00300000e+03   2.20444444e+01   5.19153226e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.40250546  0.21384783  0.65254384  9.23476887  1.50249762  9.316818
  -0.05045117  0.17360438 -0.47311823]]
Probabilities: [ 0.0652755   0.1205897   0.09105578  0.1887785   0.066045    0.06697964
  0.08456889  0.316707  ]
Probability:  0.316706998913
Random chance:  0.125
[[ 0.0652755   0.1205897   0.09105578  0.1887785   0.066045    0.06697964
   0.08456889  0.316707  ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundatio.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83000000e+03   1.53500358e+01   1.57661290e+00   5.15000000e+02
    4.50000000e+01   6.00400000e+03   2.20444444e+01   5.19153226e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.40592736  0.21384783  0.65254384  9.23476887  1.50249762  9.31988405
  -0.05045117  0.17360438 -0.47311823]]
Probabilities: [ 0.0652755   0.12058973  0.09105579  0.18877853  0.066045    0.06697964
  0.0845689   0.3167069 ]
Probability:  0.316706903392
Random chance:  0.125
[[ 0.0652755   0.12058973  0.09105579  0.18877853  0.066045    0.06697964
   0.0845689   0.3167069 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83100000e+03   1.53903584e+01   1.57963710e+00   5.14000000e+02
    4.50000000e+01   6.00500000e+03   2.20444444e+01   5.18145161e-01
    5.08225905e+01]]
Scaled doc features:  [[ 8.40934926  0.2221224   0.66847497  9.19804466  1.50249762  9.32295011
  -0.05045117  0.15234438 -0.48508239]]
Probabilities: [ 0.06527548  0.12058962  0.09105572  0.18877837  0.06604499  0.06697961
  0.08456887  0.31670735]
Probability:  0.316707352576
Random chance:  0.125
[[ 0.06527548  0.12058962  0.09105572  0.18877837  0.06604499  0.06697961
   0.08456887  0.31670735]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83100000e+03   1.53926284e+01   1.57804632e+00   5.14000000e+02
    4.50000000e+01   6.00600000e+03   2.20666667e+01   5.17623364e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.40934926  0.22258822  0.66009495  9.19804466  1.50249762  9.32601616
  -0.04730936  0.14133971 -0.47984382]]
Probabilities: [ 0.06527548  0.12058963  0.09105573  0.18877838  0.06604499  0.06697961
  0.08456887  0.31670731]
Probability:  0.316707311855
Random chance:  0.125
[[ 0.06527548  0.12058963  0.09105573  0.18877838  0.06604499  0.06697961
   0.08456887  0.31670731]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation i.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83200000e+03   1.53926284e+01   1.57905337e+00   5.15000000e+02
    4.50000000e+01   6.00700000e+03   2.20666667e+01   5.18630413e-01
    5.08494179e+01]]
Scaled doc features:  [[ 8.41277116  0.22258822  0.66539998  9.23476887  1.50249762  9.32908221
  -0.04730936  0.1625783  -0.48382786]]
Probabilities: [ 0.06527552  0.12058978  0.09105584  0.18877863  0.06604501  0.06697966
  0.08456892  0.31670664]
Probability:  0.31670664405
Random chance:  0.125
[[ 0.06527552  0.12058978  0.09105584  0.18877863  0.06604501  0.06697966
   0.08456892  0.31670664]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation in.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83300000e+03   1.53926284e+01   1.57905337e+00   5.14000000e+02
    4.50000000e+01   6.00800000e+03   2.20666667e+01   5.17623364e-01
    5.08494179e+01]]
Scaled doc features:  [[ 8.41619307  0.22258822  0.66539998  9.19804466  1.50249762  9.33214826
  -0.04730936  0.14133971 -0.48382786]]
Probabilities: [ 0.06527549  0.12058968  0.09105576  0.18877846  0.06604499  0.06697963
  0.08456888  0.31670711]
Probability:  0.316707108713
Random chance:  0.125
[[ 0.06527549  0.12058968  0.09105576  0.18877846  0.06604499  0.06697963
   0.08456888  0.31670711]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation inf.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83400000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.00900000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.41961497  0.22258822  0.66009495  9.23476887  1.50249762  9.33521432
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527553  0.12058983  0.09105587  0.1887787   0.06604501  0.06697968
  0.08456893  0.31670647]
Probability:  0.31670646595
Random chance:  0.125
[[ 0.06527553  0.12058983  0.09105587  0.1887787   0.06604501  0.06697968
   0.08456893  0.31670647]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infr.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83500000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01000000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.42303687  0.22258822  0.66009495  9.23476887  1.50249762  9.33828037
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527553  0.12058985  0.09105588  0.18877873  0.06604501  0.06697969
  0.08456893  0.31670637]
Probability:  0.316706374087
Random chance:  0.125
[[ 0.06527553  0.12058985  0.09105588  0.18877873  0.06604501  0.06697969
   0.08456893  0.31670637]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infra.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83600000e+03   1.53926284e+01   1.58006042e+00   5.15000000e+02
    4.50000000e+01   6.01100000e+03   2.20666667e+01   5.18630413e-01
    5.07642216e+01]]
Scaled doc features:  [[ 8.42645877  0.22258822  0.67070501  9.23476887  1.50249762  9.34134642
  -0.04730936  0.1625783  -0.4878119 ]]
Probabilities: [ 0.06527554  0.12058987  0.0910559   0.18877877  0.06604501  0.06697969
  0.08456894  0.31670627]
Probability:  0.316706268126
Random chance:  0.125
[[ 0.06527554  0.12058987  0.0910559   0.18877877  0.06604501  0.06697969
   0.08456894  0.31670627]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infras.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83700000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01200000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.42988067  0.22258822  0.66009495  9.23476887  1.50249762  9.34441247
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527555  0.12058989  0.09105591  0.1887788   0.06604502  0.0669797
  0.08456895  0.31670619]
Probability:  0.316706192247
Random chance:  0.125
[[ 0.06527555  0.12058989  0.09105591  0.1887788   0.06604502  0.0669797
   0.08456895  0.31670619]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrast.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83800000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01300000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.43330257  0.22258822  0.66009495  9.23476887  1.50249762  9.34747853
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527555  0.12058991  0.09105593  0.18877883  0.06604502  0.06697971
  0.08456895  0.3167061 ]
Probability:  0.316706102262
Random chance:  0.125
[[ 0.06527555  0.12058991  0.09105593  0.18877883  0.06604502  0.06697971
   0.08456895  0.3167061 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastr.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83900000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01400000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.43672448  0.22258822  0.66009495  9.23476887  1.50249762  9.35054458
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527556  0.12058993  0.09105594  0.18877887  0.06604502  0.06697971
  0.08456896  0.31670601]
Probability:  0.316706012895
Random chance:  0.125
[[ 0.06527556  0.12058993  0.09105594  0.18877887  0.06604502  0.06697971
   0.08456896  0.31670601]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastru.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84000000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01500000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.44014638  0.22258822  0.66009495  9.23476887  1.50249762  9.35361063
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527556  0.12058995  0.09105596  0.1887789   0.06604502  0.06697972
  0.08456896  0.31670592]
Probability:  0.316705924142
Random chance:  0.125
[[ 0.06527556  0.12058995  0.09105596  0.1887789   0.06604502  0.06697972
   0.08456896  0.31670592]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastruc.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84100000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01600000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.44356828  0.22258822  0.66009495  9.23476887  1.50249762  9.35667668
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527557  0.12058997  0.09105597  0.18877893  0.06604503  0.06697973
  0.08456897  0.31670584]
Probability:  0.316705836
Random chance:  0.125
[[ 0.06527557  0.12058997  0.09105597  0.18877893  0.06604503  0.06697973
   0.08456897  0.31670584]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastruct.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84200000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01700000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.44699018  0.22258822  0.66009495  9.23476887  1.50249762  9.35974274
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527557  0.12058999  0.09105599  0.18877896  0.06604503  0.06697973
  0.08456898  0.31670575]
Probability:  0.316705748465
Random chance:  0.125
[[ 0.06527557  0.12058999  0.09105599  0.18877896  0.06604503  0.06697973
   0.08456898  0.31670575]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructu.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84300000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01800000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.45041208  0.22258822  0.66009495  9.23476887  1.50249762  9.36280879
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527558  0.12059001  0.091056    0.188779    0.06604503  0.06697974
  0.08456898  0.31670566]
Probability:  0.316705661533
Random chance:  0.125
[[ 0.06527558  0.12059001  0.091056    0.188779    0.06604503  0.06697974
   0.08456898  0.31670566]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructur.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84400000e+03   1.53926284e+01   1.57804632e+00   5.15000000e+02
    4.50000000e+01   6.01900000e+03   2.20666667e+01   5.18630413e-01
    5.09346143e+01]]
Scaled doc features:  [[ 8.45383399  0.22258822  0.66009495  9.23476887  1.50249762  9.36587484
  -0.04730936  0.1625783  -0.47984382]]
Probabilities: [ 0.06527558  0.12059003  0.09105602  0.18877903  0.06604503  0.06697975
  0.08456899  0.31670558]
Probability:  0.3167055752
Random chance:  0.125
[[ 0.06527558  0.12059003  0.09105602  0.18877903  0.06604503  0.06697975
   0.08456899  0.31670558]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84500000e+03   1.54329104e+01   1.58207452e+00   5.15000000e+02
    4.50000000e+01   6.02000000e+03   2.20666667e+01   5.18630413e-01
    5.05938288e+01]]
Scaled doc features:  [[ 8.45725589  0.23085445  0.68131507  9.23476887  1.50249762  9.36894089
  -0.04730936  0.1625783  -0.49577997]]
Probabilities: [ 0.06527559  0.12059006  0.09105603  0.18877907  0.06604504  0.06697976
  0.084569    0.31670546]
Probability:  0.316705459391
Random chance:  0.125
[[ 0.06527559  0.12059006  0.09105603  0.18877907  0.06604504  0.06697976
   0.084569    0.31670546]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84500000e+03   1.54351531e+01   1.58048290e+00   5.15000000e+02
    4.50000000e+01   6.02100000e+03   2.20888889e+01   5.18108652e-01
    5.07059247e+01]]
Scaled doc features:  [[ 8.45725589  0.23131469  0.67293056  9.23476887  1.50249762  9.37200694
  -0.04416755  0.1515744  -0.49053803]]
Probabilities: [ 0.06527559  0.12059007  0.09105604  0.18877909  0.06604504  0.06697976
  0.084569    0.31670542]
Probability:  0.316705424835
Random chance:  0.125
[[ 0.06527559  0.12059007  0.09105604  0.18877909  0.06604504  0.06697976
   0.084569    0.31670542]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure p.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84600000e+03   1.56359612e+01   1.58148893e+00   5.16000000e+02
    4.40000000e+01   6.02200000e+03   2.25909091e+01   5.19114688e-01
    5.01112635e+01]]
Scaled doc features:  [[ 8.46067779  0.27252238  0.67823025  9.27149308  1.42111739  9.375073
   0.02680874  0.17279162 -0.51834617]]
Probabilities: [ 0.06527561  0.12059015  0.0910561   0.18877923  0.06604505  0.06697979
  0.08456902  0.31670504]
Probability:  0.31670503899
Random chance:  0.125
[[ 0.06527561  0.12059015  0.0910561   0.18877923  0.06604505  0.06697979
   0.08456902  0.31670504]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure pr.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84700000e+03   1.54351531e+01   1.58048290e+00   5.16000000e+02
    4.50000000e+01   6.02300000e+03   2.20888889e+01   5.19114688e-01
    5.07059247e+01]]
Scaled doc features:  [[ 8.46409969  0.23131469  0.67293056  9.27149308  1.50249762  9.37813905
  -0.04416755  0.17279162 -0.49053803]]
Probabilities: [ 0.06527563  0.12059022  0.09105615  0.18877933  0.06604505  0.06697981
  0.08456904  0.31670477]
Probability:  0.316704774507
Random chance:  0.125
[[ 0.06527563  0.12059022  0.09105615  0.18877933  0.06604505  0.06697981
   0.08456904  0.31670477]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure pro.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84800000e+03   1.54351531e+01   1.58148893e+00   5.16000000e+02
    4.50000000e+01   6.02400000e+03   2.20888889e+01   5.19114688e-01
    5.06208140e+01]]
Scaled doc features:  [[ 8.46752159  0.23131469  0.67823025  9.27149308  1.50249762  9.3812051
  -0.04416755  0.17279162 -0.49451806]]
Probabilities: [ 0.06527564  0.12059024  0.09105616  0.18877936  0.06604506  0.06697982
  0.08456905  0.31670469]
Probability:  0.316704688018
Random chance:  0.125
[[ 0.06527564  0.12059024  0.09105616  0.18877936  0.06604506  0.06697982
   0.08456905  0.31670469]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure proj.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.84900000e+03   1.54351531e+01   1.58048290e+00   5.16000000e+02
    4.50000000e+01   6.02500000e+03   2.20888889e+01   5.19114688e-01
    5.07059247e+01]]
Scaled doc features:  [[ 8.47094349  0.23131469  0.67293056  9.27149308  1.50249762  9.38427115
  -0.04416755  0.17279162 -0.49053803]]
Probabilities: [ 0.06527564  0.12059025  0.09105618  0.18877939  0.06604506  0.06697982
  0.08456905  0.31670462]
Probability:  0.316704615287
Random chance:  0.125
[[ 0.06527564  0.12059025  0.09105618  0.18877939  0.06604506  0.06697982
   0.08456905  0.31670462]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure proje.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85000000e+03   1.54351531e+01   1.58048290e+00   5.16000000e+02
    4.50000000e+01   6.02600000e+03   2.20888889e+01   5.19114688e-01
    5.07059247e+01]]
Scaled doc features:  [[ 8.4743654   0.23131469  0.67293056  9.27149308  1.50249762  9.38733721
  -0.04416755  0.17279162 -0.49053803]]
Probabilities: [ 0.06527564  0.12059027  0.09105619  0.18877942  0.06604506  0.06697983
  0.08456906  0.31670454]
Probability:  0.316704536502
Random chance:  0.125
[[ 0.06527564  0.12059027  0.09105619  0.18877942  0.06604506  0.06697983
   0.08456906  0.31670454]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure projec.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85100000e+03   1.54351531e+01   1.58048290e+00   5.16000000e+02
    4.50000000e+01   6.02700000e+03   2.20888889e+01   5.19114688e-01
    5.07059247e+01]]
Scaled doc features:  [[ 8.4777873   0.23131469  0.67293056  9.27149308  1.50249762  9.39040326
  -0.04416755  0.17279162 -0.49053803]]
Probabilities: [ 0.06527565  0.12059029  0.0910562   0.18877945  0.06604506  0.06697983
  0.08456906  0.31670446]
Probability:  0.316704458264
Random chance:  0.125
[[ 0.06527565  0.12059029  0.0910562   0.18877945  0.06604506  0.06697983
   0.08456906  0.31670446]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02800000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39346931
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.1205902   0.09105614  0.18877931  0.06604505  0.0669798
  0.08456904  0.31670483]
Probability:  0.316704830732
Random chance:  0.125
[[ 0.06527563  0.1205902   0.09105614  0.18877931  0.06604505  0.0669798
   0.08456904  0.31670483]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02900000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39653536
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
  0.08456904  0.31670479]
Probability:  0.316704790938
Random chance:  0.125
[[ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
   0.08456904  0.31670479]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02900000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39653536
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
  0.08456904  0.31670479]
Probability:  0.316704790938
Random chance:  0.125
[[ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
   0.08456904  0.31670479]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02900000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39653536
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
  0.08456904  0.31670479]
Probability:  0.316704790938
Random chance:  0.125
[[ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
   0.08456904  0.31670479]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02900000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39653536
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
  0.08456904  0.31670479]
Probability:  0.316704790938
Random chance:  0.125
[[ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
   0.08456904  0.31670479]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02900000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39653536
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
  0.08456904  0.31670479]
Probability:  0.316704790938
Random chance:  0.125
[[ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
   0.08456904  0.31670479]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02900000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39653536
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
  0.08456904  0.31670479]
Probability:  0.316704790938
Random chance:  0.125
[[ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
   0.08456904  0.31670479]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Bs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79200000e+03   1.53705346e+01   1.58290946e+00   5.10000000e+02
    4.50000000e+01   5.95800000e+03   2.18444444e+01   5.18819939e-01
    5.07487485e+01]]
Scaled doc features:  [[ 8.27589509  0.21805437  0.68571344  9.05114781  1.50249762  9.17884564
  -0.07872743  0.16657539 -0.48853546]]
Probabilities: [ 0.06527498  0.12058769  0.09105432  0.18877525  0.06604477  0.06697897
  0.0845683   0.31671572]
Probability:  0.316715715937
Random chance:  0.125
[[ 0.06527498  0.12058769  0.09105432  0.18877525  0.06604477  0.06697897
   0.0845683   0.31671572]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Bes, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79300000e+03   1.53705346e+01   1.58392675e+00   5.10000000e+02
    4.50000000e+01   5.95900000e+03   2.18444444e+01   5.18819939e-01
    5.06626854e+01]]
Scaled doc features:  [[ 8.27931699  0.21805437  0.69107243  9.05114781  1.50249762  9.1819117
  -0.07872743  0.16657539 -0.49256003]]
Probabilities: [ 0.06527499  0.12058773  0.09105435  0.18877531  0.06604478  0.06697898
  0.08456831  0.31671555]
Probability:  0.316715545687
Random chance:  0.125
[[ 0.06527499  0.12058773  0.09105435  0.18877531  0.06604478  0.06697898
   0.08456831  0.31671555]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Befs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79400000e+03   1.53705346e+01   1.58290946e+00   5.10000000e+02
    4.50000000e+01   5.96000000e+03   2.18444444e+01   5.18819939e-01
    5.07487485e+01]]
Scaled doc features:  [[ 8.28273889  0.21805437  0.68571344  9.05114781  1.50249762  9.18497775
  -0.07872743  0.16657539 -0.48853546]]
Probabilities: [ 0.065275    0.12058777  0.09105437  0.18877536  0.06604478  0.06697899
  0.08456832  0.3167154 ]
Probability:  0.316715404167
Random chance:  0.125
[[ 0.065275    0.12058777  0.09105437  0.18877536  0.06604478  0.06697899
   0.08456832  0.3167154 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Befos, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79500000e+03   1.53705346e+01   1.58290946e+00   5.10000000e+02
    4.50000000e+01   5.96100000e+03   2.18444444e+01   5.18819939e-01
    5.07487485e+01]]
Scaled doc features:  [[ 8.2861608   0.21805437  0.68571344  9.05114781  1.50249762  9.1880438
  -0.07872743  0.16657539 -0.48853546]]
Probabilities: [ 0.06527501  0.1205878   0.0910544   0.18877542  0.06604478  0.066979
  0.08456833  0.31671525]
Probability:  0.316715249831
Random chance:  0.125
[[ 0.06527501  0.1205878   0.0910544   0.18877542  0.06604478  0.066979
   0.08456833  0.31671525]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Befors, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79600000e+03   1.53705346e+01   1.58290946e+00   5.10000000e+02
    4.50000000e+01   5.96200000e+03   2.18444444e+01   5.18819939e-01
    5.07487485e+01]]
Scaled doc features:  [[ 8.2895827   0.21805437  0.68571344  9.05114781  1.50249762  9.19110985
  -0.07872743  0.16657539 -0.48853546]]
Probabilities: [ 0.06527502  0.12058784  0.09105443  0.18877548  0.06604479  0.06697901
  0.08456834  0.3167151 ]
Probability:  0.316715096519
Random chance:  0.125
[[ 0.06527502  0.12058784  0.09105443  0.18877548  0.06604479  0.06697901
   0.08456834  0.3167151 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Befores, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79700000e+03   1.53705346e+01   1.58290946e+00   5.10000000e+02
    4.50000000e+01   5.96300000e+03   2.18444444e+01   5.18819939e-01
    5.07487485e+01]]
Scaled doc features:  [[ 8.2930046   0.21805437  0.68571344  9.05114781  1.50249762  9.19417591
  -0.07872743  0.16657539 -0.48853546]]
Probabilities: [ 0.06527503  0.12058787  0.09105445  0.18877554  0.06604479  0.06697903
  0.08456835  0.31671494]
Probability:  0.316714944226
Random chance:  0.125
[[ 0.06527503  0.12058787  0.09105445  0.18877554  0.06604479  0.06697903
   0.08456835  0.31671494]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79700000e+03   1.53726829e+01   1.58434959e+00   5.11000000e+02
    4.50000000e+01   5.96400000e+03   2.18666667e+01   5.19308943e-01
    5.06043577e+01]]
Scaled doc features:  [[ 8.2930046   0.21849522  0.6932999   9.08787203  1.50249762  9.19724196
  -0.07558563  0.17688845 -0.49528761]]
Probabilities: [ 0.06527508  0.1205881   0.09105461  0.1887759   0.06604482  0.0669791
  0.08456842  0.31671398]
Probability:  0.316713975098
Random chance:  0.125
[[ 0.06527508  0.1205881   0.09105461  0.1887759   0.06604482  0.0669791
   0.08456842  0.31671398]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before ps, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79800000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.96500000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.2964265   0.21849522  0.68794635  9.08787203  1.50249762  9.20030801
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527509  0.12058813  0.09105463  0.18877595  0.06604482  0.06697911
  0.08456843  0.31671384]
Probability:  0.316713843662
Random chance:  0.125
[[ 0.06527509  0.12058813  0.09105463  0.18877595  0.06604482  0.06697911
   0.08456843  0.31671384]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before prs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79900000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.96600000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.2998484   0.21849522  0.68794635  9.08787203  1.50249762  9.20337406
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.0652751   0.12058816  0.09105466  0.188776    0.06604482  0.06697912
  0.08456844  0.3167137 ]
Probability:  0.316713700268
Random chance:  0.125
[[ 0.0652751   0.12058816  0.09105466  0.188776    0.06604482  0.06697912
   0.08456844  0.3167137 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before pris, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80000000e+03   1.53726829e+01   1.58434959e+00   5.11000000e+02
    4.50000000e+01   5.96700000e+03   2.18666667e+01   5.19308943e-01
    5.06043577e+01]]
Scaled doc features:  [[ 8.30327031  0.21849522  0.6932999   9.08787203  1.50249762  9.20644012
  -0.07558563  0.17688845 -0.49528761]]
Probabilities: [ 0.06527511  0.12058819  0.09105468  0.18877606  0.06604483  0.06697913
  0.08456845  0.31671355]
Probability:  0.316713545192
Random chance:  0.125
[[ 0.06527511  0.12058819  0.09105468  0.18877606  0.06604483  0.06697913
   0.08456845  0.31671355]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80100000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.96800000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.30669221  0.21849522  0.68794635  9.08787203  1.50249762  9.20950617
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527512  0.12058822  0.09105471  0.18877611  0.06604483  0.06697914
  0.08456846  0.31671342]
Probability:  0.31671341634
Random chance:  0.125
[[ 0.06527512  0.12058822  0.09105471  0.18877611  0.06604483  0.06697914
   0.08456846  0.31671342]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privas, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80200000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.96900000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31011411  0.21849522  0.68794635  9.08787203  1.50249762  9.21257222
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527512  0.12058826  0.09105473  0.18877616  0.06604484  0.06697915
  0.08456847  0.31671328]
Probability:  0.316713275794
Random chance:  0.125
[[ 0.06527512  0.12058826  0.09105473  0.18877616  0.06604484  0.06697915
   0.08456847  0.31671328]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privacs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80300000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.97000000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31353601  0.21849522  0.68794635  9.08787203  1.50249762  9.21563827
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527513  0.12058829  0.09105475  0.18877621  0.06604484  0.06697917
  0.08456848  0.31671314]
Probability:  0.316713136186
Random chance:  0.125
[[ 0.06527513  0.12058829  0.09105475  0.18877621  0.06604484  0.06697917
   0.08456848  0.31671314]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privacys, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80400000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.97100000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31695791  0.21849522  0.68794635  9.08787203  1.50249762  9.21870432
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527514  0.12058832  0.09105478  0.18877626  0.06604484  0.06697918
  0.08456848  0.316713  ]
Probability:  0.316712997511
Random chance:  0.125
[[ 0.06527514  0.12058832  0.09105478  0.18877626  0.06604484  0.06697918
   0.08456848  0.316713  ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privacs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80300000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.97000000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31353601  0.21849522  0.68794635  9.08787203  1.50249762  9.21563827
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527513  0.12058829  0.09105475  0.18877621  0.06604484  0.06697917
  0.08456848  0.31671314]
Probability:  0.316713136186
Random chance:  0.125
[[ 0.06527513  0.12058829  0.09105475  0.18877621  0.06604484  0.06697917
   0.08456848  0.31671314]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privas, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80200000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.96900000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31011411  0.21849522  0.68794635  9.08787203  1.50249762  9.21257222
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527512  0.12058826  0.09105473  0.18877616  0.06604484  0.06697915
  0.08456847  0.31671328]
Probability:  0.316713275794
Random chance:  0.125
[[ 0.06527512  0.12058826  0.09105473  0.18877616  0.06604484  0.06697915
   0.08456847  0.31671328]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privats, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80300000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.97000000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31353601  0.21849522  0.68794635  9.08787203  1.50249762  9.21563827
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527513  0.12058829  0.09105475  0.18877621  0.06604484  0.06697917
  0.08456848  0.31671314]
Probability:  0.316713136186
Random chance:  0.125
[[ 0.06527513  0.12058829  0.09105475  0.18877621  0.06604484  0.06697917
   0.08456848  0.31671314]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privates, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80400000e+03   1.53726829e+01   1.58536585e+00   5.11000000e+02
    4.50000000e+01   5.97100000e+03   2.18666667e+01   5.19308943e-01
    5.05183821e+01]]
Scaled doc features:  [[ 8.31695791  0.21849522  0.69865345  9.08787203  1.50249762  9.21870432
  -0.07558563  0.17688845 -0.49930809]]
Probabilities: [ 0.06527514  0.12058833  0.09105478  0.18877627  0.06604484  0.06697918
  0.08456849  0.31671297]
Probability:  0.316712972785
Random chance:  0.125
[[ 0.06527514  0.12058833  0.09105478  0.18877627  0.06604484  0.06697918
   0.08456849  0.31671297]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privatels, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80500000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.97200000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.32037981  0.21849522  0.68794635  9.08787203  1.50249762  9.22177038
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527515  0.12058835  0.0910548   0.18877631  0.06604485  0.06697919
  0.08456849  0.31671286]
Probability:  0.316712859762
Random chance:  0.125
[[ 0.06527515  0.12058835  0.0910548   0.18877631  0.06604485  0.06697919
   0.08456849  0.31671286]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privatelys, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80600000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.97300000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.32380172  0.21849522  0.68794635  9.08787203  1.50249762  9.22483643
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527516  0.12058838  0.09105482  0.18877636  0.06604485  0.0669792
  0.0845685   0.31671272]
Probability:  0.316712722935
Random chance:  0.125
[[ 0.06527516  0.12058838  0.09105482  0.18877636  0.06604485  0.0669792
   0.0845685   0.31671272]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80600000e+03   1.54154540e+01   1.58578680e+00   5.12000000e+02
    4.50000000e+01   5.97400000e+03   2.18888889e+01   5.19796954e-01
    5.04602143e+01]]
Scaled doc features:  [[ 8.32380172  0.22727225  0.70087097  9.12459624  1.50249762  9.22790248
  -0.07244382  0.18718056 -0.50202819]]
Probabilities: [ 0.06527521  0.12058859  0.09105497  0.18877669  0.06604487  0.06697927
  0.08456856  0.31671184]
Probability:  0.316711836248
Random chance:  0.125
[[ 0.06527521  0.12058859  0.09105497  0.18877669  0.06604487  0.06697927
   0.08456856  0.31671184]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately ms, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80700000e+03   1.54154540e+01   1.58578680e+00   5.12000000e+02
    4.50000000e+01   5.97500000e+03   2.18888889e+01   5.19796954e-01
    5.04602143e+01]]
Scaled doc features:  [[ 8.32722362  0.22727225  0.70087097  9.12459624  1.50249762  9.23096853
  -0.07244382  0.18718056 -0.50202819]]
Probabilities: [ 0.06527522  0.12058862  0.09105499  0.18877674  0.06604488  0.06697928
  0.08456857  0.31671171]
Probability:  0.316711706799
Random chance:  0.125
[[ 0.06527522  0.12058862  0.09105499  0.18877674  0.06604488  0.06697928
   0.08456857  0.31671171]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately mas, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80800000e+03   1.54154540e+01   1.58578680e+00   5.12000000e+02
    4.50000000e+01   5.97600000e+03   2.18888889e+01   5.19796954e-01
    5.04602143e+01]]
Scaled doc features:  [[ 8.33064552  0.22727225  0.70087097  9.12459624  1.50249762  9.23403459
  -0.07244382  0.18718056 -0.50202819]]
Probabilities: [ 0.06527523  0.12058865  0.09105501  0.18877679  0.06604488  0.06697928
  0.08456858  0.31671158]
Probability:  0.316711578218
Random chance:  0.125
[[ 0.06527523  0.12058865  0.09105501  0.18877679  0.06604488  0.06697928
   0.08456858  0.31671158]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately mans, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80900000e+03   1.54154540e+01   1.58578680e+00   5.12000000e+02
    4.50000000e+01   5.97700000e+03   2.18888889e+01   5.19796954e-01
    5.04602143e+01]]
Scaled doc features:  [[ 8.33406742  0.22727225  0.70087097  9.12459624  1.50249762  9.23710064
  -0.07244382  0.18718056 -0.50202819]]
Probabilities: [ 0.06527523  0.12058868  0.09105503  0.18877684  0.06604488  0.06697929
  0.08456859  0.31671145]
Probability:  0.316711450499
Random chance:  0.125
[[ 0.06527523  0.12058868  0.09105503  0.18877684  0.06604488  0.06697929
   0.08456859  0.31671145]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately manas, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81000000e+03   1.54154540e+01   1.58680203e+00   5.12000000e+02
    4.50000000e+01   5.97800000e+03   2.18888889e+01   5.19796954e-01
    5.03743260e+01]]
Scaled doc features:  [[ 8.33748932  0.22727225  0.70621908  9.12459624  1.50249762  9.24016669
  -0.07244382  0.18718056 -0.50604458]]
Probabilities: [ 0.06527524  0.12058871  0.09105506  0.18877689  0.06604489  0.06697931
  0.0845686   0.31671131]
Probability:  0.316711312037
Random chance:  0.125
[[ 0.06527524  0.12058871  0.09105506  0.18877689  0.06604489  0.06697931
   0.0845686   0.31671131]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately manags, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81100000e+03   1.54154540e+01   1.58477157e+00   5.12000000e+02
    4.50000000e+01   5.97900000e+03   2.18888889e+01   5.19796954e-01
    5.05461027e+01]]
Scaled doc features:  [[ 8.34091123  0.22727225  0.69552285  9.12459624  1.50249762  9.24323274
  -0.07244382  0.18718056 -0.49801179]]
Probabilities: [ 0.06527525  0.12058873  0.09105507  0.18877693  0.06604489  0.06697931
  0.08456861  0.31671121]
Probability:  0.31671120898
Random chance:  0.125
[[ 0.06527525  0.12058873  0.09105507  0.18877693  0.06604489  0.06697931
   0.08456861  0.31671121]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately manages, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81200000e+03   1.54560632e+01   1.58781726e+00   5.12000000e+02
    4.50000000e+01   5.98000000e+03   2.18888889e+01   5.19796954e-01
    5.02884377e+01]]
Scaled doc features:  [[ 8.34433313  0.23560562  0.7115672   9.12459624  1.50249762  9.2462988
  -0.07244382  0.18718056 -0.51006098]]
Probabilities: [ 0.06527526  0.12058877  0.0910551   0.18877699  0.06604489  0.06697933
  0.08456862  0.31671105]
Probability:  0.31671104611
Random chance:  0.125
[[ 0.06527526  0.12058877  0.0910551   0.18877699  0.06604489  0.06697933
   0.08456862  0.31671105]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately manageds, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81300000e+03   1.54154540e+01   1.58477157e+00   5.12000000e+02
    4.50000000e+01   5.98100000e+03   2.18888889e+01   5.19796954e-01
    5.05461027e+01]]
Scaled doc features:  [[ 8.34775503  0.22727225  0.69552285  9.12459624  1.50249762  9.24936485
  -0.07244382  0.18718056 -0.49801179]]
Probabilities: [ 0.06527526  0.12058879  0.09105512  0.18877702  0.06604489  0.06697933
  0.08456862  0.31671096]
Probability:  0.316710959329
Random chance:  0.125
[[ 0.06527526  0.12058879  0.09105512  0.18877702  0.06604489  0.06697933
   0.08456862  0.31671096]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81300000e+03   1.54175885e+01   1.58620690e+00   5.12000000e+02
    4.50000000e+01   5.98200000e+03   2.19111111e+01   5.19269777e-01
    5.04021188e+01]]
Scaled doc features:  [[ 8.34775503  0.22771025  0.70308398  9.12459624  1.50249762  9.2524309
  -0.06930201  0.17606243 -0.50474491]]
Probabilities: [ 0.06527527  0.12058881  0.09105513  0.18877705  0.0660449   0.06697934
  0.08456863  0.31671088]
Probability:  0.316710878036
Random chance:  0.125
[[ 0.06527527  0.12058881  0.09105513  0.18877705  0.0660449   0.06697934
   0.08456863  0.31671088]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed ns, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81400000e+03   1.54175885e+01   1.58519270e+00   5.12000000e+02
    4.50000000e+01   5.98300000e+03   2.19111111e+01   5.19269777e-01
    5.04879200e+01]]
Scaled doc features:  [[ 8.35117693  0.22771025  0.69774129  9.12459624  1.50249762  9.25549695
  -0.06930201  0.17606243 -0.50073259]]
Probabilities: [ 0.06527527  0.12058883  0.09105515  0.18877709  0.0660449   0.06697935
  0.08456864  0.31671077]
Probability:  0.316710766163
Random chance:  0.125
[[ 0.06527527  0.12058883  0.09105515  0.18877709  0.0660449   0.06697935
   0.08456864  0.31671077]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed nes, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81500000e+03   1.54175885e+01   1.58620690e+00   5.12000000e+02
    4.50000000e+01   5.98400000e+03   2.19111111e+01   5.19269777e-01
    5.04021188e+01]]
Scaled doc features:  [[ 8.35459883  0.22771025  0.70308398  9.12459624  1.50249762  9.25856301
  -0.06930201  0.17606243 -0.50474491]]
Probabilities: [ 0.06527528  0.12058887  0.09105517  0.18877714  0.0660449   0.06697936
  0.08456864  0.31671063]
Probability:  0.316710632859
Random chance:  0.125
[[ 0.06527528  0.12058887  0.09105517  0.18877714  0.0660449   0.06697936
   0.08456864  0.31671063]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed nets, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81600000e+03   1.54175885e+01   1.58620690e+00   5.12000000e+02
    4.50000000e+01   5.98500000e+03   2.19111111e+01   5.19269777e-01
    5.04021188e+01]]
Scaled doc features:  [[ 8.35802073  0.22771025  0.70308398  9.12459624  1.50249762  9.26162906
  -0.06930201  0.17606243 -0.50474491]]
Probabilities: [ 0.06527529  0.12058889  0.09105519  0.18877719  0.06604491  0.06697937
  0.08456865  0.31671051]
Probability:  0.316710511508
Random chance:  0.125
[[ 0.06527529  0.12058889  0.09105519  0.18877719  0.06604491  0.06697937
   0.08456865  0.31671051]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed netws, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81700000e+03   1.54175885e+01   1.58519270e+00   5.12000000e+02
    4.50000000e+01   5.98600000e+03   2.19111111e+01   5.19269777e-01
    5.04879200e+01]]
Scaled doc features:  [[ 8.36144264  0.22771025  0.69774129  9.12459624  1.50249762  9.26469511
  -0.06930201  0.17606243 -0.50073259]]
Probabilities: [ 0.0652753   0.12058892  0.09105521  0.18877723  0.06604491  0.06697938
  0.08456866  0.3167104 ]
Probability:  0.316710401863
Random chance:  0.125
[[ 0.0652753   0.12058892  0.09105521  0.18877723  0.06604491  0.06697938
   0.08456866  0.3167104 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed netwos, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81800000e+03   1.54175885e+01   1.58519270e+00   5.12000000e+02
    4.50000000e+01   5.98700000e+03   2.19111111e+01   5.19269777e-01
    5.04879200e+01]]
Scaled doc features:  [[ 8.36486454  0.22771025  0.69774129  9.12459624  1.50249762  9.26776116
  -0.06930201  0.17606243 -0.50073259]]
Probabilities: [ 0.0652753   0.12058895  0.09105523  0.18877727  0.06604491  0.06697938
  0.08456867  0.31671028]
Probability:  0.316710282066
Random chance:  0.125
[[ 0.0652753   0.12058895  0.09105523  0.18877727  0.06604491  0.06697938
   0.08456867  0.31671028]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.81900000e+03   1.54175885e+01   1.58519270e+00   5.12000000e+02
    4.50000000e+01   5.98800000e+03   2.19111111e+01   5.19269777e-01
    5.04879200e+01]]
Scaled doc features:  [[ 8.36828644  0.22771025  0.69774129  9.12459624  1.50249762  9.27082722
  -0.06930201  0.17606243 -0.50073259]]
Probabilities: [ 0.06527531  0.12058897  0.09105525  0.18877732  0.06604492  0.06697939
  0.08456868  0.31671016]
Probability:  0.316710163077
Random chance:  0.125
[[ 0.06527531  0.12058897  0.09105525  0.18877732  0.06604492  0.06697939
   0.08456868  0.31671016]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82000000e+03   1.54175885e+01   1.58722110e+00   5.11000000e+02
    4.50000000e+01   5.98900000e+03   2.19111111e+01   5.18255578e-01
    5.03163176e+01]]
Scaled doc features:  [[ 8.37170834  0.22771025  0.70842667  9.08787203  1.50249762  9.27389327
  -0.06930201  0.15467306 -0.50875723]]
Probabilities: [ 0.06527528  0.12058884  0.09105516  0.18877711  0.0660449   0.06697935
  0.08456864  0.31671072]
Probability:  0.316710723815
Random chance:  0.125
[[ 0.06527528  0.12058884  0.09105516  0.18877711  0.0660449   0.06697935
   0.08456864  0.31671072]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networkss, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82100000e+03   1.54175885e+01   1.58519270e+00   5.12000000e+02
    4.50000000e+01   5.99000000e+03   2.19111111e+01   5.19269777e-01
    5.04879200e+01]]
Scaled doc features:  [[ 8.37513024  0.22771025  0.69774129  9.12459624  1.50249762  9.27695932
  -0.06930201  0.17606243 -0.50073259]]
Probabilities: [ 0.06527532  0.12058903  0.09105529  0.18877741  0.06604492  0.06697941
  0.08456869  0.31670993]
Probability:  0.31670992751
Random chance:  0.125
[[ 0.06527532  0.12058903  0.09105529  0.18877741  0.06604492  0.06697941
   0.08456869  0.31670993]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82100000e+03   1.54197366e+01   1.58662614e+00   5.12000000e+02
    4.50000000e+01   5.99100000e+03   2.19333333e+01   5.18743668e-01
    5.03440952e+01]]
Scaled doc features:  [[ 8.37513024  0.22815107  0.70529251  9.12459624  1.50249762  9.28002537
  -0.0661602   0.16496683 -0.50745826]]
Probabilities: [ 0.06527533  0.12058905  0.0910553   0.18877744  0.06604492  0.06697942
  0.0845687   0.31670985]
Probability:  0.31670985036
Random chance:  0.125
[[ 0.06527533  0.12058905  0.0910553   0.18877744  0.06604492  0.06697942
   0.0845687   0.31670985]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks cs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82200000e+03   1.54197366e+01   1.58561297e+00   5.12000000e+02
    4.50000000e+01   5.99200000e+03   2.19333333e+01   5.18743668e-01
    5.04298095e+01]]
Scaled doc features:  [[ 8.37855215  0.22815107  0.69995523  9.12459624  1.50249762  9.28309143
  -0.0661602   0.16496683 -0.50345001]]
Probabilities: [ 0.06527533  0.12058907  0.09105532  0.18877747  0.06604493  0.06697943
  0.08456871  0.31670974]
Probability:  0.316709744824
Random chance:  0.125
[[ 0.06527533  0.12058907  0.09105532  0.18877747  0.06604493  0.06697943
   0.08456871  0.31670974]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks cas, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82300000e+03   1.54197366e+01   1.58662614e+00   5.12000000e+02
    4.50000000e+01   5.99300000e+03   2.19333333e+01   5.18743668e-01
    5.03440952e+01]]
Scaled doc features:  [[ 8.38197405  0.22815107  0.70529251  9.12459624  1.50249762  9.28615748
  -0.0661602   0.16496683 -0.50745826]]
Probabilities: [ 0.06527534  0.1205891   0.09105534  0.18877752  0.06604493  0.06697944
  0.08456871  0.31670962]
Probability:  0.316709619047
Random chance:  0.125
[[ 0.06527534  0.1205891   0.09105534  0.18877752  0.06604493  0.06697944
   0.08456871  0.31670962]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks cams, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82400000e+03   1.54197366e+01   1.58561297e+00   5.12000000e+02
    4.50000000e+01   5.99400000e+03   2.19333333e+01   5.18743668e-01
    5.04298095e+01]]
Scaled doc features:  [[ 8.38539595  0.22815107  0.69995523  9.12459624  1.50249762  9.28922353
  -0.0661602   0.16496683 -0.50345001]]
Probabilities: [ 0.06527535  0.12058912  0.09105536  0.18877756  0.06604493  0.06697944
  0.08456872  0.31670951]
Probability:  0.316709514925
Random chance:  0.125
[[ 0.06527535  0.12058912  0.09105536  0.18877756  0.06604493  0.06697944
   0.08456872  0.31670951]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks cames, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82500000e+03   1.54197366e+01   1.58561297e+00   5.12000000e+02
    4.50000000e+01   5.99500000e+03   2.19333333e+01   5.18743668e-01
    5.04298095e+01]]
Scaled doc features:  [[ 8.38881785  0.22815107  0.69995523  9.12459624  1.50249762  9.29228958
  -0.0661602   0.16496683 -0.50345001]]
Probabilities: [ 0.06527536  0.12058915  0.09105538  0.1887776   0.06604493  0.06697945
  0.08456873  0.3167094 ]
Probability:  0.316709401144
Random chance:  0.125
[[ 0.06527536  0.12058915  0.09105538  0.1887776   0.06604493  0.06697945
   0.08456873  0.3167094 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82500000e+03   1.54218983e+01   1.58603239e+00   5.13000000e+02
    4.50000000e+01   5.99600000e+03   2.19555556e+01   5.19230769e-01
    5.03717710e+01]]
Scaled doc features:  [[ 8.38881785  0.22859468  0.70216469  9.16132045  1.50249762  9.29535563
  -0.0630184   0.17523977 -0.50616406]]
Probabilities: [ 0.0652754   0.12058931  0.09105549  0.18877787  0.06604495  0.06697951
  0.08456878  0.31670869]
Probability:  0.316708690244
Random chance:  0.125
[[ 0.0652754   0.12058931  0.09105549  0.18877787  0.06604495  0.06697951
   0.08456878  0.31670869]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came is, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82600000e+03   1.54218983e+01   1.58603239e+00   5.12000000e+02
    4.50000000e+01   5.99700000e+03   2.19555556e+01   5.18218623e-01
    5.03717710e+01]]
Scaled doc features:  [[ 8.39223975  0.22859468  0.70216469  9.12459624  1.50249762  9.29842169
  -0.0630184   0.15389369 -0.50616406]]
Probabilities: [ 0.06527537  0.12058919  0.09105541  0.18877767  0.06604494  0.06697947
  0.08456874  0.31670922]
Probability:  0.316709223623
Random chance:  0.125
[[ 0.06527537  0.12058919  0.09105541  0.18877767  0.06604494  0.06697947
   0.08456874  0.31670922]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came ins, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82700000e+03   1.54218983e+01   1.58603239e+00   5.13000000e+02
    4.50000000e+01   5.99800000e+03   2.19555556e+01   5.19230769e-01
    5.03717710e+01]]
Scaled doc features:  [[ 8.39566165  0.22859468  0.70216469  9.16132045  1.50249762  9.30148774
  -0.0630184   0.17523977 -0.50616406]]
Probabilities: [ 0.06527541  0.12058936  0.09105553  0.18877795  0.06604496  0.06697952
  0.08456879  0.31670848]
Probability:  0.316708475456
Random chance:  0.125
[[ 0.06527541  0.12058936  0.09105553  0.18877795  0.06604496  0.06697952
   0.08456879  0.31670848]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came ints, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82800000e+03   1.54218983e+01   1.58502024e+00   5.13000000e+02
    4.50000000e+01   5.99900000e+03   2.19555556e+01   5.19230769e-01
    5.04573986e+01]]
Scaled doc features:  [[ 8.39908356  0.22859468  0.69683281  9.16132045  1.50249762  9.30455379
  -0.0630184   0.17523977 -0.50215986]]
Probabilities: [ 0.06527542  0.12058939  0.09105555  0.18877798  0.06604496  0.06697953
  0.0845688   0.31670838]
Probability:  0.316708378671
Random chance:  0.125
[[ 0.06527542  0.12058939  0.09105555  0.18877798  0.06604496  0.06697953
   0.0845688   0.31670838]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came intos, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82900000e+03   1.54218983e+01   1.58502024e+00   5.13000000e+02
    4.50000000e+01   6.00000000e+03   2.19555556e+01   5.19230769e-01
    5.04573986e+01]]
Scaled doc features:  [[ 8.40250546  0.22859468  0.69683281  9.16132045  1.50249762  9.30761984
  -0.0630184   0.17523977 -0.50215986]]
Probabilities: [ 0.06527542  0.12058941  0.09105556  0.18877802  0.06604496  0.06697954
  0.0845688   0.31670827]
Probability:  0.316708273026
Random chance:  0.125
[[ 0.06527542  0.12058941  0.09105556  0.18877802  0.06604496  0.06697954
   0.0845688   0.31670827]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82900000e+03   1.54240737e+01   1.58645096e+00   5.13000000e+02
    4.50000000e+01   6.00100000e+03   2.19777778e+01   5.18705763e-01
    5.03138043e+01]]
Scaled doc features:  [[ 8.40250546  0.22904109  0.70436968  9.16132045  1.50249762  9.3106859
  -0.05987659  0.16416743 -0.50887476]]
Probabilities: [ 0.06527543  0.12058943  0.09105558  0.18877805  0.06604497  0.06697954
  0.08456881  0.3167082 ]
Probability:  0.316708203619
Random chance:  0.125
[[ 0.06527543  0.12058943  0.09105558  0.18877805  0.06604497  0.06697954
   0.08456881  0.3167082 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into fs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83000000e+03   1.54240737e+01   1.58543984e+00   5.13000000e+02
    4.50000000e+01   6.00200000e+03   2.19777778e+01   5.18705763e-01
    5.03993452e+01]]
Scaled doc features:  [[ 8.40592736  0.22904109  0.6990432   9.16132045  1.50249762  9.31375195
  -0.05987659  0.16416743 -0.50487461]]
Probabilities: [ 0.06527543  0.12058945  0.09105559  0.18877808  0.06604497  0.06697955
  0.08456882  0.31670811]
Probability:  0.316708108576
Random chance:  0.125
[[ 0.06527543  0.12058945  0.09105559  0.18877808  0.06604497  0.06697955
   0.08456882  0.31670811]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into fas, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83100000e+03   1.54240737e+01   1.58543984e+00   5.13000000e+02
    4.50000000e+01   6.00300000e+03   2.19777778e+01   5.18705763e-01
    5.03993452e+01]]
Scaled doc features:  [[ 8.40934926  0.22904109  0.6990432   9.16132045  1.50249762  9.316818
  -0.05987659  0.16416743 -0.50487461]]
Probabilities: [ 0.06527544  0.12058947  0.09105561  0.18877812  0.06604497  0.06697956
  0.08456882  0.316708  ]
Probability:  0.316708004781
Random chance:  0.125
[[ 0.06527544  0.12058947  0.09105561  0.18877812  0.06604497  0.06697956
   0.08456882  0.316708  ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83200000e+03   1.54240737e+01   1.58543984e+00   5.13000000e+02
    4.50000000e+01   6.00400000e+03   2.19777778e+01   5.18705763e-01
    5.03993452e+01]]
Scaled doc features:  [[ 8.41277116  0.22904109  0.6990432   9.16132045  1.50249762  9.31988405
  -0.05987659  0.16416743 -0.50487461]]
Probabilities: [ 0.06527544  0.1205895   0.09105563  0.18877816  0.06604497  0.06697957
  0.08456883  0.3167079 ]
Probability:  0.316707901694
Random chance:  0.125
[[ 0.06527544  0.1205895   0.09105563  0.18877816  0.06604497  0.06697957
   0.08456883  0.3167079 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favos, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83300000e+03   1.54240737e+01   1.58543984e+00   5.13000000e+02
    4.50000000e+01   6.00500000e+03   2.19777778e+01   5.18705763e-01
    5.03993452e+01]]
Scaled doc features:  [[ 8.41619307  0.22904109  0.6990432   9.16132045  1.50249762  9.32295011
  -0.05987659  0.16416743 -0.50487461]]
Probabilities: [ 0.06527545  0.12058952  0.09105564  0.1887782   0.06604498  0.06697958
  0.08456884  0.3167078 ]
Probability:  0.316707799313
Random chance:  0.125
[[ 0.06527545  0.12058952  0.09105564  0.1887782   0.06604498  0.06697958
   0.08456884  0.3167078 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favors, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83400000e+03   1.54240737e+01   1.58746208e+00   5.13000000e+02
    4.50000000e+01   6.00600000e+03   2.19777778e+01   5.18705763e-01
    5.02282633e+01]]
Scaled doc features:  [[ 8.41961497  0.22904109  0.70969617  9.16132045  1.50249762  9.32601616
  -0.05987659  0.16416743 -0.51287491]]
Probabilities: [ 0.06527546  0.12058955  0.09105566  0.18877824  0.06604498  0.06697958
  0.08456884  0.31670768]
Probability:  0.316707679269
Random chance:  0.125
[[ 0.06527546  0.12058955  0.09105566  0.18877824  0.06604498  0.06697958
   0.08456884  0.31670768]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favor,s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83400000e+03   1.54240737e+01   1.58746208e+00   5.13000000e+02
    4.50000000e+01   6.00700000e+03   2.19777778e+01   5.18705763e-01
    5.02282633e+01]]
Scaled doc features:  [[ 8.41961497  0.22904109  0.70969617  9.16132045  1.50249762  9.32908221
  -0.05987659  0.16416743 -0.51287491]]
Probabilities: [ 0.06527546  0.12058956  0.09105567  0.18877826  0.06604498  0.06697959
  0.08456885  0.31670763]
Probability:  0.316707629456
Random chance:  0.125
[[ 0.06527546  0.12058956  0.09105567  0.18877826  0.06604498  0.06697959
   0.08456885  0.31670763]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favor,, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83300000e+03   1.54240737e+01   1.58746208e+00   5.13000000e+02
    4.50000000e+01   6.00600000e+03   2.19777778e+01   5.18705763e-01
    5.02282633e+01]]
Scaled doc features:  [[ 8.41619307  0.22904109  0.70969617  9.16132045  1.50249762  9.32601616
  -0.05987659  0.16416743 -0.51287491]]
Probabilities: [ 0.06527545  0.12058953  0.09105566  0.18877823  0.06604498  0.06697958
  0.08456884  0.31670773]
Probability:  0.316707730654
Random chance:  0.125
[[ 0.06527545  0.12058953  0.09105566  0.18877823  0.06604498  0.06697958
   0.08456884  0.31670773]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favor, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83300000e+03   1.54240737e+01   1.58746208e+00   5.13000000e+02
    4.50000000e+01   6.00500000e+03   2.19777778e+01   5.18705763e-01
    5.02282633e+01]]
Scaled doc features:  [[ 8.41619307  0.22904109  0.70969617  9.16132045  1.50249762  9.32295011
  -0.05987659  0.16416743 -0.51287491]]
Probabilities: [ 0.06527545  0.12058952  0.09105565  0.18877821  0.06604498  0.06697958
  0.08456884  0.31670778]
Probability:  0.316707780813
Random chance:  0.125
[[ 0.06527545  0.12058952  0.09105565  0.18877821  0.06604498  0.06697958
   0.08456884  0.31670778]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favor,, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83300000e+03   1.54240737e+01   1.58746208e+00   5.13000000e+02
    4.50000000e+01   6.00600000e+03   2.19777778e+01   5.18705763e-01
    5.02282633e+01]]
Scaled doc features:  [[ 8.41619307  0.22904109  0.70969617  9.16132045  1.50249762  9.32601616
  -0.05987659  0.16416743 -0.51287491]]
Probabilities: [ 0.06527545  0.12058953  0.09105566  0.18877823  0.06604498  0.06697958
  0.08456884  0.31670773]
Probability:  0.316707730654
Random chance:  0.125
[[ 0.06527545  0.12058953  0.09105566  0.18877823  0.06604498  0.06697958
   0.08456884  0.31670773]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privately managed networks came into favor,s, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83400000e+03   1.54240737e+01   1.58746208e+00   5.13000000e+02
    4.50000000e+01   6.00700000e+03   2.19777778e+01   5.18705763e-01
    5.02282633e+01]]
Scaled doc features:  [[ 8.41961497  0.22904109  0.70969617  9.16132045  1.50249762  9.32908221
  -0.05987659  0.16416743 -0.51287491]]
Probabilities: [ 0.06527546  0.12058956  0.09105567  0.18877826  0.06604498  0.06697959
  0.08456885  0.31670763]
Probability:  0.316707629456
Random chance:  0.125
[[ 0.06527546  0.12058956  0.09105567  0.18877826  0.06604498  0.06697959
   0.08456885  0.31670763]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privas, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80200000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.96900000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31011411  0.21849522  0.68794635  9.08787203  1.50249762  9.21257222
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527512  0.12058826  0.09105473  0.18877616  0.06604484  0.06697915
  0.08456847  0.31671328]
Probability:  0.316713275794
Random chance:  0.125
[[ 0.06527512  0.12058826  0.09105473  0.18877616  0.06604484  0.06697915
   0.08456847  0.31671328]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before privacys, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.80400000e+03   1.53726829e+01   1.58333333e+00   5.11000000e+02
    4.50000000e+01   5.97100000e+03   2.18666667e+01   5.19308943e-01
    5.06903333e+01]]
Scaled doc features:  [[ 8.31695791  0.21849522  0.68794635  9.08787203  1.50249762  9.21870432
  -0.07558563  0.17688845 -0.49126713]]
Probabilities: [ 0.06527514  0.12058832  0.09105478  0.18877626  0.06604484  0.06697918
  0.08456848  0.316713  ]
Probability:  0.316712997511
Random chance:  0.125
[[ 0.06527514  0.12058832  0.09105478  0.18877626  0.06604484  0.06697918
   0.08456848  0.316713  ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Bs, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79200000e+03   1.53705346e+01   1.58290946e+00   5.10000000e+02
    4.50000000e+01   5.95800000e+03   2.18444444e+01   5.18819939e-01
    5.07487485e+01]]
Scaled doc features:  [[ 8.27589509  0.21805437  0.68571344  9.05114781  1.50249762  9.17884564
  -0.07872743  0.16657539 -0.48853546]]
Probabilities: [ 0.06527498  0.12058769  0.09105432  0.18877525  0.06604477  0.06697897
  0.0845683   0.31671572]
Probability:  0.316715715937
Random chance:  0.125
[[ 0.06527498  0.12058769  0.09105432  0.18877525  0.06604477  0.06697897
   0.0845683   0.31671572]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science Foundation infrastructure project..
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.85200000e+03   1.54351531e+01   1.58249497e+00   5.15000000e+02
    4.50000000e+01   6.02900000e+03   2.20888889e+01   5.18108652e-01
    5.05357033e+01]]
Scaled doc features:  [[ 8.4812092   0.23131469  0.68352994  9.23476887  1.50249762  9.39653536
  -0.04416755  0.1515744  -0.49849809]]
Probabilities: [ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
  0.08456904  0.31670479]
Probability:  0.316704790938
Random chance:  0.125
[[ 0.06527563  0.12059021  0.09105615  0.18877932  0.06604505  0.06697981
   0.08456904  0.31670479]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science .
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82100000e+03   1.53500358e+01   1.57661290e+00   5.14000000e+02
    4.50000000e+01   5.99500000e+03   2.20444444e+01   5.18145161e-01
    5.10784373e+01]]
Scaled doc features:  [[ 8.37513024  0.21384783  0.65254384  9.19804466  1.50249762  9.29228958
  -0.05045117  0.15234438 -0.47311823]]
Probabilities: [ 0.06527541  0.12058938  0.09105554  0.18877798  0.06604496  0.06697953
  0.0845688   0.3167084 ]
Probability:  0.316708401219
Random chance:  0.125
[[ 0.06527541  0.12058938  0.09105554  0.18877798  0.06604496  0.06697953
   0.0845688   0.3167084 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, the director of the National Science ou.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.82300000e+03   1.53500358e+01   1.57762097e+00   5.15000000e+02
    4.50000000e+01   5.99700000e+03   2.20444444e+01   5.19153226e-01
    5.09931550e+01]]
Scaled doc features:  [[ 8.38197405  0.21384783  0.65785422  9.23476887  1.50249762  9.29842169
  -0.05045117  0.17360438 -0.47710628]]
Probabilities: [ 0.06527546  0.12058957  0.09105568  0.18877828  0.06604498  0.06697959
  0.08456885  0.31670758]
Probability:  0.316707578055
Random chance:  0.125
[[ 0.06527546  0.12058957  0.09105568  0.18877828  0.06604498  0.06697959
   0.08456885  0.31670758]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, t.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.79100000e+03   1.54545086e+01   1.57606491e+00   5.15000000e+02
    4.40000000e+01   5.95900000e+03   2.24090909e+01   5.22312373e-01
    5.07546814e+01]]
Scaled doc features:  [[  8.27247319e+00   2.35286606e-01   6.49657068e-01   9.23476887e+00
    1.42111739e+00   9.18191170e+00   1.10303887e-03   2.40230550e-01
   -4.88258021e-01]]
Probabilities: [ 0.06527521  0.12058858  0.09105496  0.18877668  0.06604487  0.06697926
  0.08456856  0.31671188]
Probability:  0.316711882495
Random chance:  0.125
[[ 0.06527521  0.12058858  0.09105496  0.18877668  0.06604487  0.06697926
   0.08456856  0.31671188]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
There are too many users on the internet.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.86600000e+03   1.54374093e+01   1.58391960e+00   5.17000000e+02
    4.50000000e+01   6.04400000e+03   2.21111111e+01   5.19597990e-01
    5.03926242e+01]]
Scaled doc features:  [[ 8.52911583  0.23177767  0.69103473  9.30821729  1.50249762  9.44252615
  -0.04102574  0.18298442 -0.5051889 ]]
Probabilities: [ 0.06527574  0.12059066  0.09105647  0.18878004  0.0660451   0.06697996
  0.08456917  0.31670286]
Probability:  0.316702858554
Random chance:  0.125
[[ 0.06527574  0.12059066  0.09105647  0.18878004  0.0660451   0.06697996
   0.08456917  0.31670286]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
T
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.83400000e+03   1.55810085e+01   1.58400810e+00   5.17000000e+02
    4.40000000e+01   6.00400000e+03   2.24545455e+01   5.23279352e-01
    5.00365513e+01]]
Scaled doc features:  [[  8.41961497e+00   2.61245566e-01   6.91500938e-01   9.30821729e+00
    1.42111739e+00   9.31988405e+00   7.52946334e-03   2.60624061e-01
   -5.21839936e-01]]
Probabilities: [ 0.06527557  0.12058998  0.09105598  0.18877895  0.06604503  0.06697973
  0.08456897  0.3167058 ]
Probability:  0.316705799505
Random chance:  0.125
[[ 0.06527557  0.12058998  0.09105598  0.18877895  0.06604503  0.06697973
   0.08456897  0.3167058 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of bits and bytes.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.97300000e+03   1.55665442e+01   1.58169291e+00   5.26000000e+02
    4.50000000e+01   6.17400000e+03   2.25777778e+01   5.17716535e-01
    5.01073351e+01]]
Scaled doc features:  [[ 8.89525932  0.25827736  0.6793048   9.6387352   1.50249762  9.84111296
   0.02495221  0.14330469 -0.51852988]]
Probabilities: [ 0.06527613  0.12059215  0.09105755  0.18878245  0.06604526  0.06698045
  0.08456961  0.3166964 ]
Probability:  0.316696398182
Random chance:  0.125
[[ 0.06527613  0.12059215  0.09105755  0.18878245  0.06604526  0.06698045
   0.08456961  0.3166964 ]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of 
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96100000e+03   1.57665053e+01   1.58185404e+00   5.25000000e+02
    4.40000000e+01   6.15900000e+03   2.30454545e+01   5.17751479e-01
    4.96190116e+01]]
Scaled doc features:  [[ 8.8541965   0.29931125  0.68015361  9.60201099  1.42111739  9.79512218
   0.09107298  0.14404166 -0.54136535]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669682]
Probability:  0.316696823341
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669682]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam of i
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96200000e+03   1.57665053e+01   1.58284024e+00   5.26000000e+02
    4.40000000e+01   6.16000000e+03   2.30454545e+01   5.18737673e-01
    4.95355796e+01]]
Scaled doc features:  [[ 8.8576184   0.29931125  0.68534877  9.6387352   1.42111739  9.79818823
   0.09107298  0.1648404  -0.54526687]]
Probabilities: [ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
  0.08456959  0.31669668]
Probability:  0.316696678266
Random chance:  0.125
[[ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
   0.08456959  0.31669668]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.57612648e+01   1.58399209e+00   5.25000000e+02
    4.40000000e+01   6.15500000e+03   2.30000000e+01   5.18774704e-01
    4.94842688e+01]]
Scaled doc features:  [[ 8.84735269  0.29823585  0.69141664  9.60201099  1.42111739  9.78285797
   0.08464656  0.16562138 -0.54766632]]
Probabilities: [ 0.0652761   0.12059204  0.09105747  0.18878227  0.06604525  0.06698042
  0.08456957  0.31669689]
Probability:  0.316696887111
Random chance:  0.125
[[ 0.0652761   0.12059204  0.09105747  0.18878227  0.06604525  0.06698042
   0.08456957  0.31669689]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam. o
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.96000000e+03   1.55592322e+01   1.58341560e+00   5.26000000e+02
    4.50000000e+01   6.15800000e+03   2.25111111e+01   5.19249753e-01
    5.00292627e+01]]
Scaled doc features:  [[ 8.8507746   0.25677687  0.68837971  9.6387352   1.50249762  9.79205612
   0.01552679  0.17564014 -0.52218078]]
Probabilities: [ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
  0.08456959  0.31669668]
Probability:  0.31669667823
Random chance:  0.125
[[ 0.06527611  0.12059209  0.0910575   0.18878235  0.06604526  0.06698043
   0.08456959  0.31669668]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.55568204e+01   1.58399209e+00   5.25000000e+02
    4.50000000e+01   6.15600000e+03   2.24888889e+01   5.18774704e-01
    5.00030466e+01]]
Scaled doc features:  [[ 8.84735269  0.25628194  0.69141664  9.60201099  1.50249762  9.78592402
   0.01238498  0.16562138 -0.52340672]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669683]
Probability:  0.316696832268
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669683]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam.

With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  4.95900000e+03   1.55592322e+01   1.58242843e+00   5.25000000e+02
    4.50000000e+01   6.15700000e+03   2.25111111e+01   5.18262586e-01
    5.01127770e+01]]
Scaled doc features:  [[ 8.84735269  0.25677687  0.68317942  9.60201099  1.50249762  9.78899007
   0.01552679  0.15482087 -0.5182754 ]]
Probabilities: [ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
  0.08456958  0.31669682]
Probability:  0.316696823368
Random chance:  0.125
[[ 0.0652761   0.12059205  0.09105748  0.18878229  0.06604525  0.06698042
   0.08456958  0.31669682]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big traffic jam.
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.05700000e+03   1.55677067e+01   1.58220503e+00   5.31000000e+02
    4.60000000e+01   6.27800000e+03   2.24782609e+01   5.13539652e-01
    5.01650198e+01]]
Scaled doc features:  [[  9.18269907   0.25851591   0.68200257   9.82235625   1.58387784
   10.15998241   0.01088238   0.05521455  -0.51583237]]
Probabilities: [ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698057
  0.0845697   0.31669494]
Probability:  0.316694940008
Random chance:  0.125
[[ 0.06527621  0.12059249  0.09105779  0.18878299  0.0660453   0.06698057
   0.0845697   0.31669494]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a big
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04700000e+03   1.57624806e+01   1.58236434e+00   5.31000000e+02
    4.50000000e+01   6.26500000e+03   2.29333333e+01   5.14534884e-01
    4.96896434e+01]]
Scaled doc features:  [[  9.14848005   0.29848535   0.68284181   9.82235625   1.50249762
   10.12012373   0.07522113   0.07620391  -0.53806239]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669504]
Probability:  0.316695036952
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669504]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
The internet is like a bigg
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.04800000e+03   1.57624806e+01   1.58139535e+00   5.31000000e+02
    4.50000000e+01   6.26600000e+03   2.29333333e+01   5.14534884e-01
    4.97716202e+01]]
Scaled doc features:  [[  9.15190196   0.29848535   0.67773726   9.82235625   1.50249762
   10.12318978   0.07522113   0.07620391  -0.53422891]]
Probabilities: [ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
  0.0845697   0.31669503]
Probability:  0.316695030067
Random chance:  0.125
[[ 0.06527621  0.12059247  0.09105778  0.18878296  0.0660453   0.06698056
   0.0845697   0.31669503]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>

This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02600000e+03   1.57111674e+01   1.58227848e+00   5.29000000e+02
    4.50000000e+01   6.23900000e+03   2.28222222e+01   5.15092502e-01
    4.98096850e+01]]
Scaled doc features:  [[  9.07662012   0.28795539   0.6823895    9.74890783   1.50249762
   10.04040637   0.0595121    0.08796405  -0.53244889]]
Probabilities: [ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698053
  0.08456968  0.31669534]
Probability:  0.316695341881
Random chance:  0.125
[[ 0.06527619  0.1205924   0.09105773  0.18878285  0.06604529  0.06698053
   0.08456968  0.31669534]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Internet
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.03400000e+03   1.57501158e+01   1.58519961e+00   5.29000000e+02
    4.50000000e+01   6.24700000e+03   2.28222222e+01   5.15092502e-01
    4.95625574e+01]]
Scaled doc features:  [[  9.10399533   0.29594796   0.6977777    9.74890783   1.50249762
   10.06493479   0.0595121    0.08796405  -0.54400531]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669526]
Probability:  0.316695262151
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669526]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
I
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.02700000e+03   1.57111674e+01   1.58325219e+00   5.30000000e+02
    4.50000000e+01   6.24000000e+03   2.28222222e+01   5.16066212e-01
    4.97273091e+01]]
Scaled doc features:  [[  9.08004202   0.28795539   0.6875189    9.78563204   1.50249762
   10.04347242   0.0595121    0.10849951  -0.53630103]]
Probabilities: [ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
  0.08456968  0.31669528]
Probability:  0.316695276258
Random chance:  0.125
[[ 0.06527619  0.12059241  0.09105774  0.18878287  0.06604529  0.06698054
   0.08456968  0.31669528]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Commuters stuck in traffic on the Leesburg Pike in Northern Virginia are just a few hundred yards away from an even bigger jam, one that stretches around the world.
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.16000000e+03   1.56573254e+01   1.58009479e+00   5.38000000e+02
    4.60000000e+01   6.40300000e+03   2.29347826e+01   5.09952607e-01
    4.98801767e+01]]
Scaled doc features:  [[  9.53515496   0.27690651   0.67088604  10.07942573   1.58387784
   10.54323896   0.07542603  -0.02043595  -0.52915248]]
Probabilities: [ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
  0.08456974  0.31669436]
Probability:  0.316694361601
Random chance:  0.125
[[ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
   0.08456974  0.31669436]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Commuters stuck in traffic on the Leesburg Pike in Northern Virginia are just a few hundred yards away from an even bigger jam, one that stretches around the world.
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.16000000e+03   1.58611901e+01   1.58009479e+00   5.38000000e+02
    4.50000000e+01   6.40200000e+03   2.34444444e+01   5.09952607e-01
    4.93628699e+01]]
Scaled doc features:  [[  9.53515496   0.31874145   0.67088604  10.07942573   1.50249762
   10.54017291   0.14748271  -0.02043595  -0.5533433 ]]
Probabilities: [ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
  0.08456974  0.31669437]
Probability:  0.316694365801
Random chance:  0.125
[[ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
   0.08456974  0.31669437]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Commuters stuck in traffic on the Leesburg Pike in Northern Virginia are just a few hundred yards away from an even bigger jam, one that stretches around the world.
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.16000000e+03   1.56573254e+01   1.58009479e+00   5.38000000e+02
    4.60000000e+01   6.40300000e+03   2.29347826e+01   5.09952607e-01
    4.98801767e+01]]
Scaled doc features:  [[  9.53515496   0.27690651   0.67088604  10.07942573   1.58387784
   10.54323896   0.07542603  -0.02043595  -0.52915248]]
Probabilities: [ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
  0.08456974  0.31669436]
Probability:  0.316694361601
Random chance:  0.125
[[ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
   0.08456974  0.31669436]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
Commuters stuck in traffic on the Leesburg Pike in Northern Virginia are just a few hundred yards away from an even bigger jam, one that stretches around the world.
This is a traffic jam of bits and bytes, digital data travelling through one of the main arteries of the global Internet.
With thousands of new users signing on every day, the Internet's data routes are brimming with traffic and some predict the imminent demise of the net due to overcrowding.
"Demand for the Internet is outstripping supply," said Mark Luker, programme director of the National Science Foundation's network infrastructure project.
But work is under way to provide more capacity through technical solutions and ways are being explored to ration the Internet by price, through fees based on the speed and quantity of data.
The facility in Northern Virginia is a computer interchange, called Mae East, where 46 major and minor Internet service providers come together to exchange data travelling across the Internet.
BUSIEST PUBLIC EXCHANGE POINT ON THE INTERNET, FIRM SAYS
MFS Communications Inc, which runs Mae East and its California counterpart Mae West, describes it as "the busiest public exchange point on the Internet." Peak loads run as high as 53 million bytes per second.
Until April 1995, the National Science Foundation (NSF) managed and financed the common highway underpinning most of the Internet, called a network backbone.
Before the network was phased out in favour of privately managed networks, traffic increased steadily from 1.3 trillion bytes a month in March 1991 to a high of 17.8 trillion bytes in late 1994, the equivalent of transfering the entire contents of the Library of Congress every four months.
The private system sends data across a host of different company networks and crowded interchange points. As the load continues to increase, MFS and the Internet providers can add more fibre optic lines and switching equipment, but that is expensive and still may fail to keep up with rising usage.
Some providers are setting up smaller, one-to-one interchanges to relieve the burden on Mae East. MCI Communications, which has seen a 5,000 percent increase in Internet traffic since the beginning of 1995, set up 22 circuits for one-to-one exchanges.
For now, the best solution is to keep adding hardware, said Luker. "But in the long run, you can solve problems by adding knowledge to the system instead of brute force," he said.
The NSF recently announced 13 grants for the development of high speed networking technologies and software programmes.
IDEAS TO REDUCE CONGESTION
One project hopes to reduce congestion by developing a system, called caching, to hold copies of popular sites at duplicate locations around the world.
If an Internet user in Japan tries to download a web page in New York, the data must travel all the way around the world, even if another user in Japan has just downloaded the same page. With a cache, the second user could grab the page from a closer computer that the data had already gone through.
"A cache automatically duplicates the pages that are used most often," Luker explained. The NSF had sponsored a study at the National Laboratory for Advanced Networking Research of a cache that "is showing great improvements in traffic."
A procedure called multicasting could also reduce repetitive data transfers, said Robert Hagens, MCI's director of Internet engineering. Multicasting sends a stream of data such as sound or video across the net that can be accessed by many users.
"If you look at the radio stations beginning to appear on the Internet, they commonly require each listener to open a connection to the station so you've got a lot of duplication," Hagens said. "With multicast, you put that data into a stream of packets that only get duplicated when they have to."
New ways of charging for Internet usage may alter the traffic patterns, as well. At the moment, most users pay a flat rate for Internet service regardless of how much capacity they use. Sending e-mail is considerably less taxing than sending live videos, but users pay the same for both.
"The Internet is a mature technology but an immature economy," said Hal Varian, dean of the University of California's School of Information Management and Systems. The current pricing model does not provide incentives to Internet providers to offer high quality service, he said.
People who need high priority channels for sending live video might have to pay more, Varian said, but basic tasks like sending e-mail will remain essentially free.
Varian also predicted Internet service providers will begin paying each other based on the amount of traffic they exchange, much as phone carriers make settlement payments to foreign telecommunications companies for completing international calls.
Much of the slowdown experienced by individuals trying to surf the World Wide Web is caused by limits at each end of a connection rather than by delays moving across the network.
Web pages reside on computer servers that can be overwhelmed when too many requests arrive at the same time. And many individuals access the Internet through relatively slow modem links.
Such delays could be eased by a proposal from Sun Microsystems to create a new Internet standard for the way computers access each others' files.
Called Web Network File System, the protocol reduces the burden on the computer holding the web pages, speeding the transfer of files and allowing three times as many Internet surfers to gain access at one time, according to Sanjay Sinha, head of Sun's Solaris Server project.
"In real life, most web servers are heavily loaded," Sinha said. Sun's web file system "doesn't require that load."
User delays are real, but by some measures the Internet's performance has actually improved over the last few years.
Matrix Information and Directory Services of Austin, Texas, compiles a weather report which it publishes on the Internet.
The report, updated every four hours, shows how long it takes a small message to travel from Matrix's headquarters to 4,500 major computers around the world and back.
Traffic clogging the Internet slows the round trips. The reports show huge fluctuations in Internet traffic, with "rush hour" occuring weekdays as business users log on.
But the average delay declined by 30 percent between January 1994 and January 1996, according a report by Matrix.

Current doc features:  [[  5.16000000e+03   1.56573254e+01   1.58009479e+00   5.38000000e+02
    4.60000000e+01   6.40300000e+03   2.29347826e+01   5.09952607e-01
    4.98801767e+01]]
Scaled doc features:  [[  9.53515496   0.27690651   0.67088604  10.07942573   1.58387784
   10.54323896   0.07542603  -0.02043595  -0.52915248]]
Probabilities: [ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
  0.08456974  0.31669436]
Probability:  0.316694361601
Random chance:  0.125
[[ 0.06527625  0.12059262  0.09105789  0.18878321  0.06604531  0.06698061
   0.08456974  0.31669436]]
Current prediction:  user
Prediction type:  <class 'numpy.str_'>
